#!/usr/bin/env python3
"""
Complete RAG Tutorial: End-to-End Integration
å®Œå…¨RAGãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ï¼šã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰çµ±åˆ

This comprehensive example demonstrates the complete RAG workflow using refinire-rag:
Part 1: Corpus Creation (Document loading, processing, indexing)
Part 2: Query Engine (Search, retrieval, answer generation)  
Part 3: Evaluation (QA generation, performance assessment, reporting)

ã“ã®åŒ…æ‹¬çš„ãªä¾‹ã§ã¯ã€refinire-ragã‚’ä½¿ç”¨ã—ãŸå®Œå…¨ãªRAGãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿæ¼”ã—ã¾ã™ï¼š
Part 1: ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆï¼ˆæ–‡æ›¸ãƒ­ãƒ¼ãƒ‰ã€å‡¦ç†ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
Part 2: Query Engineï¼ˆæ¤œç´¢ã€å–å¾—ã€å›ç­”ç”Ÿæˆï¼‰
Part 3: è©•ä¾¡ï¼ˆQAç”Ÿæˆã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã€ãƒ¬ãƒãƒ¼ãƒˆï¼‰
"""

import sys
import tempfile
import time
import json
from pathlib import Path
from typing import List, Dict, Any, Optional

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

# Core imports
from refinire_rag.application.corpus_manager_new import CorpusManager
from refinire_rag.application.query_engine import QueryEngine, QueryEngineConfig
from refinire_rag.application.quality_lab import QualityLab, QualityLabConfig
from refinire_rag.storage.sqlite_store import SQLiteDocumentStore
from refinire_rag.storage.in_memory_vector_store import InMemoryVectorStore
from refinire_rag.retrieval import SimpleRetriever, SimpleReranker, SimpleReader
from refinire_rag.models.document import Document


class CompleteRAGTutorial:
    """
    Complete RAG tutorial demonstration class
    å®Œå…¨RAGãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, work_dir: Path):
        """
        Initialize tutorial with working directory
        ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’åˆæœŸåŒ–
        """
        self.work_dir = work_dir
        self.knowledge_base_dir = work_dir / "knowledge_base"
        self.reports_dir = work_dir / "reports"
        self.data_dir = work_dir / "data"
        
        # Create directories / ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        for dir_path in [self.knowledge_base_dir, self.reports_dir, self.data_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # Initialize components / ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–
        self.doc_store: Optional[SQLiteDocumentStore] = None
        self.vector_store: Optional[InMemoryVectorStore] = None
        self.corpus_manager: Optional[CorpusManager] = None
        self.query_engine: Optional[QueryEngine] = None
        self.quality_lab: Optional[QualityLab] = None
        
        # Store results / çµæœã‚’ä¿å­˜
        self.corpus_stats = None
        self.evaluation_results = None
        self.performance_metrics = {}
    
    def create_knowledge_base(self) -> List[str]:
        """
        Create comprehensive knowledge base for demonstration
        ãƒ‡ãƒ¢ç”¨ã®åŒ…æ‹¬çš„çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ
        """
        
        print("ğŸ“š Creating comprehensive knowledge base...")
        print("ğŸ“š åŒ…æ‹¬çš„çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆä¸­...")
        
        # AI Fundamentals / AIåŸºç¤
        ai_fundamentals = self.knowledge_base_dir / "01_ai_fundamentals.txt"
        ai_fundamentals.write_text("""
# äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰åŸºç¤

## æ¦‚è¦
äººå·¥çŸ¥èƒ½ï¼ˆArtificial Intelligence, AIï¼‰ã¯ã€äººé–“ã®çŸ¥çš„èƒ½åŠ›ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§å†ç¾ã™ã‚‹æŠ€è¡“ã®ç·ç§°ã§ã™ã€‚
1950å¹´ä»£ã®ã‚¢ãƒ©ãƒ³ãƒ»ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆã®ææ¡ˆä»¥æ¥ã€é•·ã„æ­´å²ã‚’æŒã¤ç ”ç©¶åˆ†é‡ã§ã™ã€‚

## AIã®åˆ†é¡

### 1. å¼±ã„AIï¼ˆNarrow AIï¼‰
- ç‰¹å®šã®é ˜åŸŸã‚„ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸAI
- ç¾åœ¨å®Ÿç”¨åŒ–ã•ã‚Œã¦ã„ã‚‹AIã®å¤§éƒ¨åˆ†
- ä¾‹ï¼šéŸ³å£°èªè­˜ã€ç”»åƒèªè­˜ã€ç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ 

### 2. å¼·ã„AIï¼ˆGeneral AIï¼‰
- äººé–“ã¨åŒç­‰ã®æ±ç”¨çš„ãªçŸ¥èƒ½ã‚’æŒã¤AI
- ã¾ã å®Ÿç¾ã•ã‚Œã¦ã„ãªã„ç†è«–çš„æ¦‚å¿µ
- AGIï¼ˆArtificial General Intelligenceï¼‰ã¨ã‚‚å‘¼ã°ã‚Œã‚‹

### 3. è¶…AIï¼ˆSuper AIï¼‰
- äººé–“ã®çŸ¥èƒ½ã‚’è¶…è¶Šã—ãŸAI
- ç†è«–çš„ãƒ»ä»®æƒ³çš„ãªæ¦‚å¿µ

## ä¸»è¦æŠ€è¡“é ˜åŸŸ

### æ©Ÿæ¢°å­¦ç¿’ï¼ˆMachine Learningï¼‰
ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«å­¦ç¿’ã—ã€äºˆæ¸¬ã‚„åˆ¤æ–­ã‚’è¡Œã†æŠ€è¡“

### æ·±å±¤å­¦ç¿’ï¼ˆDeep Learningï¼‰
å¤šå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•

### è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰
äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ç†è§£ãƒ»ç”Ÿæˆã™ã‚‹æŠ€è¡“

### ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ï¼ˆComputer Visionï¼‰
ç”»åƒã‚„å‹•ç”»ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºãƒ»ç†è§£ã™ã‚‹æŠ€è¡“

### ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ï¼ˆRoboticsï¼‰
ç‰©ç†çš„ãªç’°å¢ƒã§è‡ªå¾‹çš„ã«å‹•ä½œã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ 

## å¿œç”¨åˆ†é‡
- åŒ»ç™‚ï¼šè¨ºæ–­æ”¯æ´ã€å‰µè–¬ã€æ‰‹è¡“æ”¯æ´
- é‡‘èï¼šä¸æ­£æ¤œçŸ¥ã€ãƒªã‚¹ã‚¯è©•ä¾¡ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å–å¼•
- äº¤é€šï¼šè‡ªå‹•é‹è»¢ã€äº¤é€šæœ€é©åŒ–
- è£½é€ ï¼šå“è³ªç®¡ç†ã€äºˆçŸ¥ä¿å…¨ã€å·¥ç¨‹æœ€é©åŒ–
- ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆï¼šæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã€ã‚²ãƒ¼ãƒ AI

## ç¤¾ä¼šçš„å½±éŸ¿
- é›‡ç”¨ã¸ã®å½±éŸ¿ï¼šè‡ªå‹•åŒ–ã«ã‚ˆã‚‹è·æ¥­ã®å¤‰åŒ–
- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ï¼šå€‹äººãƒ‡ãƒ¼ã‚¿ã®ä¿è­·
- å…¬å¹³æ€§ï¼šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒã‚¤ã‚¢ã‚¹ã®å•é¡Œ
- å®‰å…¨æ€§ï¼šAIå®‰å…¨æ€§ã®ç¢ºä¿

AIã¯ä»Šå¾Œã‚‚æ€¥é€Ÿã«ç™ºå±•ã—ã€äººé–“ç¤¾ä¼šã«å¤§ããªå¤‰é©ã‚’ã‚‚ãŸã‚‰ã™ã¨äºˆæƒ³ã•ã‚Œã¦ã„ã¾ã™ã€‚
""", encoding='utf-8')
        
        # Machine Learning Details / æ©Ÿæ¢°å­¦ç¿’è©³ç´°
        ml_details = self.knowledge_base_dir / "02_machine_learning.txt"
        ml_details.write_text("""
# æ©Ÿæ¢°å­¦ç¿’ï¼ˆMachine Learningï¼‰è©³ç´°

## å®šç¾©
æ©Ÿæ¢°å­¦ç¿’ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«å­¦ç¿’ã—ã€
æ˜ç¤ºçš„ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã•ã‚Œã‚‹ã“ã¨ãªãäºˆæ¸¬ã‚„åˆ¤æ–­ã‚’è¡Œã†æŠ€è¡“ã§ã™ã€‚

## å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ 

### 1. æ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆSupervised Learningï¼‰
æ­£è§£ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ©ãƒ™ãƒ«ï¼‰ã‚’ç”¨ã„ã¦å­¦ç¿’ã™ã‚‹æ‰‹æ³•

#### åˆ†é¡ï¼ˆClassificationï¼‰
- ç›®çš„ï¼šãƒ‡ãƒ¼ã‚¿ã‚’äº‹å‰å®šç¾©ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã«åˆ†é¡
- ä¾‹ï¼šãƒ¡ãƒ¼ãƒ«åˆ†é¡ã€ç”»åƒèªè­˜ã€è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ 
- è©•ä¾¡æŒ‡æ¨™ï¼šç²¾åº¦ã€é©åˆç‡ã€å†ç¾ç‡ã€F1ã‚¹ã‚³ã‚¢
- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
  * ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
  *æ±ºå®šæœ¨ï¼ˆDecision Treeï¼‰
  * ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼ˆRandom Forestï¼‰
  * ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ï¼ˆSVMï¼‰
  * ãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚ºï¼ˆNaive Bayesï¼‰

#### å›å¸°ï¼ˆRegressionï¼‰
- ç›®çš„ï¼šé€£ç¶šå€¤ã®äºˆæ¸¬
- ä¾‹ï¼šæ ªä¾¡äºˆæ¸¬ã€å£²ä¸Šäºˆæ¸¬ã€æ°—æ¸©äºˆæ¸¬
- è©•ä¾¡æŒ‡æ¨™ï¼šMAEã€MSEã€RMSEã€RÂ²
- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
  * ç·šå½¢å›å¸°ï¼ˆLinear Regressionï¼‰
  * å¤šé …å¼å›å¸°ï¼ˆPolynomial Regressionï¼‰
  * ãƒªãƒƒã‚¸å›å¸°ï¼ˆRidge Regressionï¼‰
  * ãƒ©ãƒƒã‚½å›å¸°ï¼ˆLasso Regressionï¼‰

### 2. æ•™å¸«ãªã—å­¦ç¿’ï¼ˆUnsupervised Learningï¼‰
æ­£è§£ãƒ‡ãƒ¼ã‚¿ãªã—ã§ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹

#### ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
- ç›®çš„ï¼šé¡ä¼¼ãƒ‡ãƒ¼ã‚¿ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
- ä¾‹ï¼šé¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€éºä¼å­åˆ†æ
- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
  * K-means
  * éšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
  * DBSCAN
  * ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆGMMï¼‰

#### æ¬¡å…ƒå‰Šæ¸›
- ç›®çš„ï¼šé«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ãƒ»åœ§ç¸®
- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
  * ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰
  * ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰
  * t-SNE
  * UMAP

#### ç•°å¸¸æ¤œçŸ¥
- ç›®çš„ï¼šæ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å¤–ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã®ç™ºè¦‹
- ä¾‹ï¼šä¸æ­£æ¤œçŸ¥ã€æ•…éšœè¨ºæ–­
- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
  * Isolation Forest
  * One-Class SVM
  * Local Outlier Factor (LOF)

### 3. å¼·åŒ–å­¦ç¿’ï¼ˆReinforcement Learningï¼‰
ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦æœ€é©ãªè¡Œå‹•æˆ¦ç•¥ã‚’å­¦ç¿’

#### åŸºæœ¬æ¦‚å¿µ
- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼šå­¦ç¿’ãƒ»è¡Œå‹•ä¸»ä½“
- ç’°å¢ƒï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè¡Œå‹•ã™ã‚‹å ´
- çŠ¶æ…‹ï¼šç’°å¢ƒã®ç¾åœ¨ã®çŠ¶æ³
- è¡Œå‹•ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒé¸æŠã§ãã‚‹å‹•ä½œ
- å ±é…¬ï¼šè¡Œå‹•ã«å¯¾ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
- æ–¹ç­–ï¼šçŠ¶æ…‹ã‹ã‚‰è¡Œå‹•ã¸ã®å†™åƒ

#### ä¸»è¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- Qå­¦ç¿’ï¼ˆQ-Learningï¼‰
- SARSA
- Deep Q Network (DQN)
- Policy Gradient
- Actor-Critic
- Proximal Policy Optimization (PPO)

#### å¿œç”¨ä¾‹
- ã‚²ãƒ¼ãƒ AIï¼šå›²ç¢ã€ãƒã‚§ã‚¹ã€ãƒ“ãƒ‡ã‚ªã‚²ãƒ¼ãƒ 
- ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ï¼šæ­©è¡Œã€æ“ä½œ
- è‡ªå‹•é‹è»¢ï¼šçµŒè·¯è¨ˆç”»ã€é‹è»¢æˆ¦ç•¥
- é‡‘èï¼šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å–å¼•

## æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### 1. ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»ç†è§£
- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ç‰¹å®š
- ãƒ‡ãƒ¼ã‚¿å“è³ªã®è©•ä¾¡
- æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰

### 2. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
- æ¬ æå€¤å‡¦ç†
- å¤–ã‚Œå€¤å‡¦ç†
- ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ»æ­£è¦åŒ–
- ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

### 3. ãƒ¢ãƒ‡ãƒ«é¸æŠãƒ»æ§‹ç¯‰
- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸æŠ
- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
- äº¤å·®æ¤œè¨¼

### 4. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
- æ€§èƒ½æŒ‡æ¨™ã®è¨ˆç®—
- éå­¦ç¿’ãƒ»æœªå­¦ç¿’ã®æ¤œè¨¼
- ãƒ¢ãƒ‡ãƒ«è§£é‡ˆ

### 5. ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»é‹ç”¨
- ãƒ¢ãƒ‡ãƒ«ã®æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹
- ç¶™ç¶šçš„ç›£è¦–ãƒ»æ›´æ–°
- A/Bãƒ†ã‚¹ãƒˆ

## èª²é¡Œã¨é™ç•Œ
- ãƒ‡ãƒ¼ã‚¿å“è³ªã¸ã®ä¾å­˜
- è§£é‡ˆå¯èƒ½æ€§ã®ä¸è¶³
- ãƒã‚¤ã‚¢ã‚¹ã¨å…¬å¹³æ€§ã®å•é¡Œ
- è¨ˆç®—ã‚³ã‚¹ãƒˆã¨ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶
- æ±åŒ–æ€§èƒ½ã®é™ç•Œ

æ©Ÿæ¢°å­¦ç¿’ã¯ç¾ä»£AIã®ä¸­æ ¸æŠ€è¡“ã¨ã—ã¦ã€æ§˜ã€…ãªåˆ†é‡ã§é©æ–°çš„ãªè§£æ±ºç­–ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚
""", encoding='utf-8')
        
        # Deep Learning / æ·±å±¤å­¦ç¿’
        deep_learning = self.knowledge_base_dir / "03_deep_learning.txt"
        deep_learning.write_text("""
# æ·±å±¤å­¦ç¿’ï¼ˆDeep Learningï¼‰

## æ¦‚è¦
æ·±å±¤å­¦ç¿’ã¯ã€äººé–“ã®è„³ã®ç¥çµŒå›è·¯ã‚’æ¨¡å€£ã—ãŸå¤šå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ã‚‹
æ©Ÿæ¢°å­¦ç¿’ã®ä¸€åˆ†é‡ã§ã™ã€‚2010å¹´ä»£ä»¥é™ã®AIãƒ–ãƒ¼ãƒ ã‚’ç‰½å¼•ã™ã‚‹ä¸­æ ¸æŠ€è¡“ã§ã™ã€‚

## ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åŸºç¤

### åŸºæœ¬æ§‹é€ 
- ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ï¼ˆãƒãƒ¼ãƒ‰ï¼‰ï¼šæƒ…å ±å‡¦ç†ã®åŸºæœ¬å˜ä½
- é‡ã¿ï¼ˆWeightï¼‰ï¼šãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é–“ã®æ¥ç¶šå¼·åº¦
- ãƒã‚¤ã‚¢ã‚¹ï¼ˆBiasï¼‰ï¼šæ´»æ€§åŒ–ã®é–¾å€¤èª¿æ•´
- æ´»æ€§åŒ–é–¢æ•°ï¼šéç·šå½¢å¤‰æ›ï¼ˆReLUã€Sigmoidã€Tanhç­‰ï¼‰

### å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹
1. é †ä¼æ’­ï¼ˆForward Propagationï¼‰ï¼šå…¥åŠ›ã‹ã‚‰å‡ºåŠ›ã¸ã®è¨ˆç®—
2. æå¤±è¨ˆç®—ï¼šäºˆæ¸¬å€¤ã¨æ­£è§£å€¤ã®èª¤å·®æ¸¬å®š
3. é€†ä¼æ’­ï¼ˆBackpropagationï¼‰ï¼šèª¤å·®ã®é€†å‘ãä¼æ’­
4. å‹¾é…é™ä¸‹æ³•ï¼šé‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®æ›´æ–°

## ä¸»è¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### 1. ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆCNNï¼‰
ç”»åƒå‡¦ç†ã«ç‰¹åŒ–ã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#### æ§‹æˆè¦ç´ 
- ç•³ã¿è¾¼ã¿å±¤ï¼šå±€æ‰€çš„ç‰¹å¾´ã®æŠ½å‡º
- ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ï¼šãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
- å…¨çµåˆå±¤ï¼šåˆ†é¡ãƒ»å›å¸°

#### ä»£è¡¨çš„ãƒ¢ãƒ‡ãƒ«
- LeNetï¼ˆ1998ï¼‰ï¼šæ‰‹æ›¸ãæ•°å­—èªè­˜
- AlexNetï¼ˆ2012ï¼‰ï¼šImageNeté©å‘½
- VGGï¼ˆ2014ï¼‰ï¼šæ·±ã„ç•³ã¿è¾¼ã¿å±¤
- ResNetï¼ˆ2015ï¼‰ï¼šæ®‹å·®æ¥ç¶š
- EfficientNetï¼ˆ2019ï¼‰ï¼šåŠ¹ç‡çš„è¨­è¨ˆ

#### å¿œç”¨åˆ†é‡
- ç”»åƒåˆ†é¡ï¼šImageNetã€CIFAR-10
- ç‰©ä½“æ¤œå‡ºï¼šYOLOã€R-CNNç³»åˆ—
- ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
- åŒ»ç™‚ç”»åƒè§£æï¼šXç·šã€CTã€MRI

### 2. å†å¸°ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆRNNï¼‰
æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«é©ã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#### åŸºæœ¬æ§‹é€ 
- éš ã‚ŒçŠ¶æ…‹ï¼šå‰ã®æ™‚åˆ»ã®æƒ…å ±ã‚’ä¿æŒ
- å…¥åŠ›ã‚²ãƒ¼ãƒˆã€å¿˜å´ã‚²ãƒ¼ãƒˆã€å‡ºåŠ›ã‚²ãƒ¼ãƒˆ

#### æ”¹è‰¯ç‰ˆ
- LSTMï¼ˆLong Short-Term Memoryï¼‰
  * é•·æœŸä¾å­˜é–¢ä¿‚ã®å­¦ç¿’
  * å‹¾é…æ¶ˆå¤±å•é¡Œã®è§£æ±º
- GRUï¼ˆGated Recurrent Unitï¼‰
  * LSTMã®ç°¡ç´ åŒ–ç‰ˆ
  * è¨ˆç®—åŠ¹ç‡ã®å‘ä¸Š

#### å¿œç”¨åˆ†é‡
- è‡ªç„¶è¨€èªå‡¦ç†ï¼šç¿»è¨³ã€è¦ç´„ã€æ„Ÿæƒ…åˆ†æ
- éŸ³å£°èªè­˜ãƒ»åˆæˆ
- æ™‚ç³»åˆ—äºˆæ¸¬ï¼šæ ªä¾¡ã€å¤©æ°—äºˆå ±
- éŸ³æ¥½ç”Ÿæˆ

### 3. ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆTransformerï¼‰
æ³¨æ„æ©Ÿæ§‹ï¼ˆAttentionï¼‰ã‚’æ ¸ã¨ã—ãŸé©æ–°çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#### ç‰¹å¾´
- Self-Attentionï¼šç³»åˆ—å†…ã®é–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‹
- Multi-Head Attentionï¼šè¤‡æ•°ã®æ³¨æ„ã®çµ„ã¿åˆã‚ã›
- ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼šé †åºæƒ…å ±ã®ä»˜ä¸
- ä¸¦åˆ—å‡¦ç†ï¼šRNNã‚ˆã‚Šé«˜é€Ÿ

#### ä»£è¡¨çš„ãƒ¢ãƒ‡ãƒ«
- BERTï¼ˆ2018ï¼‰ï¼šåŒæ–¹å‘ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
- GPTç³»åˆ—ï¼ˆ2018-2023ï¼‰ï¼šç”Ÿæˆå‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼
- T5ï¼ˆ2019ï¼‰ï¼šText-to-Textå¤‰æ›
- Vision Transformerï¼ˆ2020ï¼‰ï¼šç”»åƒã¸ã®Transformeré©ç”¨

#### å¿œç”¨åˆ†é‡
- æ©Ÿæ¢°ç¿»è¨³ï¼šGoogle Translate
- è¨€èªãƒ¢ãƒ‡ãƒ«ï¼šChatGPTã€Claude
- æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ï¼šBERTæ­è¼‰Googleæ¤œç´¢
- ã‚³ãƒ¼ãƒ‰ç”Ÿæˆï¼šGitHub Copilot

## ç”Ÿæˆãƒ¢ãƒ‡ãƒ«

### 1. ç”Ÿæˆçš„æ•µå¯¾ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆGANï¼‰
- Generatorï¼šå½ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- Discriminatorï¼šçœŸå½åˆ¤å®š
- æ•µå¯¾çš„å­¦ç¿’ï¼šäº’ã„ã®æ€§èƒ½å‘ä¸Š

#### å¿œç”¨
- ç”»åƒç”Ÿæˆï¼šStyleGANã€BigGAN
- è¶…è§£åƒï¼šSRGANã€ESRGAN
- ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼šå°‘æ•°ãƒ‡ãƒ¼ã‚¿ã®å¢—å¼·

### 2. å¤‰åˆ†ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆVAEï¼‰
- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼šãƒ‡ãƒ¼ã‚¿ã®æ½œåœ¨è¡¨ç¾å­¦ç¿’
- ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ï¼šæ½œåœ¨è¡¨ç¾ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ
- ç¢ºç‡çš„ç”Ÿæˆ

### 3. æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ï¼ˆDiffusion Modelsï¼‰
- ãƒã‚¤ã‚ºé™¤å»ãƒ—ãƒ­ã‚»ã‚¹ã®å­¦ç¿’
- é«˜å“è³ªç”»åƒç”Ÿæˆ
- ä¾‹ï¼šDALL-E 2ã€Stable Diffusionã€Midjourney

## å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰

### ç™ºå±•ã®æ­´å²
- GPT-1ï¼ˆ2018ï¼‰ï¼š1.17å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- BERTï¼ˆ2018ï¼‰ï¼š3.4å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- GPT-2ï¼ˆ2019ï¼‰ï¼š15å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- GPT-3ï¼ˆ2020ï¼‰ï¼š1750å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- PaLMï¼ˆ2022ï¼‰ï¼š5400å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- GPT-4ï¼ˆ2023ï¼‰ï¼šæ¨å®š1å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

### èƒ½åŠ›
- æ–‡ç« ç”Ÿæˆãƒ»è¦ç´„
- è³ªå•å¿œç­”
- ç¿»è¨³
- ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
- æ¨è«–ãƒ»è¨ˆç®—

## æ·±å±¤å­¦ç¿’ã®èª²é¡Œ

### æŠ€è¡“çš„èª²é¡Œ
- å¤§é‡ãƒ‡ãƒ¼ã‚¿ãƒ»è¨ˆç®—è³‡æºã®å¿…è¦æ€§
- ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹å•é¡Œ
- éå­¦ç¿’ãƒ»æ±åŒ–æ€§èƒ½
- æ•µå¯¾çš„æ”»æ’ƒã¸ã®è„†å¼±æ€§
- ã‚«ã‚¿ã‚¹ãƒˆãƒ­ãƒ•ã‚£ãƒƒã‚¯å¿˜å´

### ç¤¾ä¼šçš„èª²é¡Œ
- ãƒã‚¤ã‚¢ã‚¹ã¨å·®åˆ¥
- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¾µå®³
- é›‡ç”¨ã¸ã®å½±éŸ¿
- ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¶ˆè²»
- æƒ…å ±ã®ä¿¡é ¼æ€§

## æœ€æ–°å‹•å‘
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIï¼šãƒ†ã‚­ã‚¹ãƒˆãƒ»ç”»åƒãƒ»éŸ³å£°ã®çµ±åˆ
- å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆå­¦ç¿’ï¼šå°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’
- é€£åˆå­¦ç¿’ï¼šãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·å­¦ç¿’
- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¢ç´¢ï¼ˆNASï¼‰
- é‡å­æ©Ÿæ¢°å­¦ç¿’

æ·±å±¤å­¦ç¿’ã¯ç¾åœ¨ã‚‚æ€¥é€Ÿã«ç™ºå±•ã‚’ç¶šã‘ã€
AGIï¼ˆæ±ç”¨äººå·¥çŸ¥èƒ½ï¼‰ã®å®Ÿç¾ã«å‘ã‘ãŸé‡è¦ãªæŠ€è¡“ã¨ã—ã¦æ³¨ç›®ã•ã‚Œã¦ã„ã¾ã™ã€‚
""", encoding='utf-8')
        
        # NLP Applications / NLPå¿œç”¨
        nlp_applications = self.knowledge_base_dir / "04_nlp_applications.txt"
        nlp_applications.write_text("""
# è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰å¿œç”¨

## æ¦‚è¦
è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNatural Language Processing, NLPï¼‰ã¯ã€
äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ç†è§£ãƒ»ç”Ÿæˆãƒ»æ“ä½œã™ã‚‹æŠ€è¡“åˆ†é‡ã§ã™ã€‚

## åŸºæœ¬çš„ãªNLPã‚¿ã‚¹ã‚¯

### 1. å‰å‡¦ç†ï¼ˆPreprocessingï¼‰
#### ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–
- å¤§æ–‡å­—ãƒ»å°æ–‡å­—çµ±ä¸€
- æ•°å­—ãƒ»è¨˜å·ã®å‡¦ç†
- æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµ±ä¸€

#### ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆTokenizationï¼‰
- æ–‡åˆ†å‰²ï¼šæ–‡ç« ã‚’æ–‡ã«åˆ†å‰²
- å˜èªåˆ†å‰²ï¼šæ–‡ã‚’å˜èªã«åˆ†å‰²
- ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²ï¼šBPEã€SentencePiece

#### å½¢æ…‹ç´ è§£æ
- å“è©ã‚¿ã‚°ä»˜ã‘ï¼ˆPOS Taggingï¼‰
- èªå¹¹æŠ½å‡ºï¼ˆStemmingï¼‰
- èªå½¢æ­£è¦åŒ–ï¼ˆLemmatizationï¼‰

### 2. è¨€èªç†è§£ã‚¿ã‚¹ã‚¯

#### å›ºæœ‰è¡¨ç¾èªè­˜ï¼ˆNERï¼‰
- äººåã€åœ°åã€çµ„ç¹”åã®æŠ½å‡º
- æ—¥ä»˜ã€æ™‚é–“ã€é‡‘é¡ã®è­˜åˆ¥
- å¿œç”¨ï¼šæƒ…å ±æŠ½å‡ºã€è³ªå•å¿œç­”

#### æ§‹æ–‡è§£æï¼ˆParsingï¼‰
- ä¾å­˜æ§‹é€ è§£æï¼šèªã®ä¾å­˜é–¢ä¿‚
- å¥æ§‹é€ è§£æï¼šæ–‡æ³•æ§‹é€ ã®éšå±¤åŒ–
- å¿œç”¨ï¼šæ©Ÿæ¢°ç¿»è¨³ã€æƒ…å ±æŠ½å‡º

#### æ„å‘³è§£æ
- èªç¾©æ›–æ˜§æ€§è§£æ¶ˆï¼ˆWSDï¼‰
- æ„å‘³å½¹å‰²ãƒ©ãƒ™ãƒªãƒ³ã‚°ï¼ˆSRLï¼‰
- å«æ„é–¢ä¿‚èªè­˜ï¼ˆRTEï¼‰

## ä¸»è¦ãªNLPã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

### 1. æ©Ÿæ¢°ç¿»è¨³ï¼ˆMachine Translationï¼‰

#### ç™ºå±•ã®æ­´å²
- è¦å‰‡ãƒ™ãƒ¼ã‚¹ç¿»è¨³ï¼ˆ1950å¹´ä»£-1980å¹´ä»£ï¼‰
  * è¨€èªå­¦çš„è¦å‰‡ã®æ‰‹å‹•ä½œæˆ
  * é™å®šçš„ãªç²¾åº¦ã¨ç¯„å›²
  
- çµ±è¨ˆçš„æ©Ÿæ¢°ç¿»è¨³ï¼ˆ1990å¹´ä»£-2010å¹´ä»£ï¼‰
  * å¤§è¦æ¨¡ä¸¦åˆ—ã‚³ãƒ¼ãƒ‘ã‚¹ã®æ´»ç”¨
  * ãƒ•ãƒ¬ãƒ¼ã‚ºãƒ™ãƒ¼ã‚¹ã€éšå±¤ãƒ•ãƒ¬ãƒ¼ã‚ºãƒ™ãƒ¼ã‚¹
  
- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ©Ÿæ¢°ç¿»è¨³ï¼ˆ2010å¹´ä»£-ç¾åœ¨ï¼‰
  * Encoder-Decoder ãƒ¢ãƒ‡ãƒ«
  * Attentionæ©Ÿæ§‹ã®å°å…¥
  * Transformer ã«ã‚ˆã‚‹é©æ–°

#### ç¾ä»£ã®æ‰‹æ³•
- Google Translateï¼šå¤šè¨€èªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ç¿»è¨³
- DeepLï¼šé«˜å“è³ªãªç¿»è¨³ã‚µãƒ¼ãƒ“ã‚¹
- mBERTã€XLM-Rï¼šå¤šè¨€èªäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«
- ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç¿»è¨³ï¼šç›´æ¥ç¿»è¨³ãƒšã‚¢ãªã—

#### è©•ä¾¡æŒ‡æ¨™
- BLEU Scoreï¼šn-gramãƒ™ãƒ¼ã‚¹è‡ªå‹•è©•ä¾¡
- METEORï¼šèªå¹¹ã‚„åŒç¾©èªã‚’è€ƒæ…®
- chrFï¼šæ–‡å­—ãƒ¬ãƒ™ãƒ«è©•ä¾¡
- äººæ‰‹è©•ä¾¡ï¼šæµæš¢ã•ã€æ­£ç¢ºæ€§ã€é©åˆ‡æ€§

### 2. æ„Ÿæƒ…åˆ†æï¼ˆSentiment Analysisï¼‰

#### åˆ†æãƒ¬ãƒ™ãƒ«
- æ–‡æ›¸ãƒ¬ãƒ™ãƒ«ï¼šæ–‡æ›¸å…¨ä½“ã®æ„Ÿæƒ…æ¥µæ€§
- æ–‡ãƒ¬ãƒ™ãƒ«ï¼šå„æ–‡ã®æ„Ÿæƒ…
- ã‚¢ã‚¹ãƒšã‚¯ãƒˆãƒ¬ãƒ™ãƒ«ï¼šç‰¹å®šè¦³ç‚¹ã®æ„Ÿæƒ…
- æ„Ÿæƒ…ã®å¼·åº¦ï¼šãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–ã®ç¨‹åº¦

#### æ‰‹æ³•
- è¾æ›¸ãƒ™ãƒ¼ã‚¹ï¼šæ„Ÿæƒ…è¾æ›¸ã®æ´»ç”¨
- æ©Ÿæ¢°å­¦ç¿’ï¼šç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
- æ·±å±¤å­¦ç¿’ï¼šLSTMã€BERTç­‰

#### å¿œç”¨åˆ†é‡
- ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢åˆ†æ
- è£½å“ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æ
- é¡§å®¢æº€è¶³åº¦èª¿æŸ»
- ãƒ–ãƒ©ãƒ³ãƒ‰ç›£è¦–
- æ ªå¼å¸‚å ´äºˆæ¸¬

### 3. è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ï¼ˆQAï¼‰

#### ã‚·ã‚¹ãƒ†ãƒ åˆ†é¡
- æŠ½å‡ºå‹QAï¼šæ–‡æ›¸ã‹ã‚‰è©²å½“ç®‡æ‰€æŠ½å‡º
- ç”Ÿæˆå‹QAï¼šå›ç­”æ–‡ã®ç”Ÿæˆ
- çŸ¥è­˜ãƒ™ãƒ¼ã‚¹QAï¼šæ§‹é€ åŒ–çŸ¥è­˜æ´»ç”¨
- ä¼šè©±å‹QAï¼šå¯¾è©±çš„è³ªå•å¿œç­”

#### ä»£è¡¨çš„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- SQuADï¼šèª­è§£ç†è§£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
- Natural Questionsï¼šå®Ÿä¸–ç•Œè³ªå•
- MS MARCOï¼šå¤§è¦æ¨¡æ¤œç´¢QA
- JAQKETï¼šæ—¥æœ¬èªQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

#### æŠ€è¡“è¦ç´ 
- æ–‡æ›¸æ¤œç´¢ï¼ˆRetrievalï¼‰
- èª­è§£ç†è§£ï¼ˆReading Comprehensionï¼‰
- å›ç­”ç”Ÿæˆï¼ˆAnswer Generationï¼‰
- è¤‡æ•°ãƒ›ãƒƒãƒ—æ¨è«–

### 4. æ–‡æ›¸è¦ç´„ï¼ˆText Summarizationï¼‰

#### è¦ç´„æ‰‹æ³•
- æŠ½å‡ºå‹è¦ç´„ï¼šé‡è¦æ–‡ã®é¸æŠãƒ»çµåˆ
- ç”Ÿæˆå‹è¦ç´„ï¼šæ–°ã—ã„æ–‡ã®ç”Ÿæˆ
- ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼šä¸¡æ‰‹æ³•ã®çµ„ã¿åˆã‚ã›

#### è¦ç´„ã®ç¨®é¡
- å˜ä¸€æ–‡æ›¸è¦ç´„ï¼š1ã¤ã®æ–‡æ›¸ã‹ã‚‰è¦ç´„
- è¤‡æ•°æ–‡æ›¸è¦ç´„ï¼šè¤‡æ•°æ–‡æ›¸ã®çµ±åˆè¦ç´„
- æ›´æ–°è¦ç´„ï¼šæ–°æƒ…å ±ã®è¿½åŠ è¦ç´„
- ã‚¯ã‚¨ãƒªæŒ‡å‘è¦ç´„ï¼šç‰¹å®šè¦³ç‚¹ã®è¦ç´„

#### è©•ä¾¡æŒ‡æ¨™
- ROUGEï¼šè¦ç´„å“è³ªã®è‡ªå‹•è©•ä¾¡
- Pyramidï¼šé‡è¦åº¦é‡ã¿ä»˜ãè©•ä¾¡
- äººæ‰‹è©•ä¾¡ï¼šæƒ…å ±æ€§ã€èª­ã¿ã‚„ã™ã•

### 5. å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ï¼ˆDialogue Systemsï¼‰

#### ã‚·ã‚¹ãƒ†ãƒ åˆ†é¡
- ã‚¿ã‚¹ã‚¯æŒ‡å‘ï¼šç‰¹å®šæ¥­å‹™ã®é‚è¡Œ
  * ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³äºˆç´„ã€èˆªç©ºåˆ¸äºˆç´„
  * ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆ
  
- é›‘è«‡å‹ï¼šè‡ªç„¶ãªä¼šè©±
  * å¨¯æ¥½ãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
  * æ„Ÿæƒ…çš„ã‚µãƒãƒ¼ãƒˆ
  
- è³ªå•å¿œç­”å‹ï¼šæƒ…å ±æä¾›
  * çŸ¥è­˜æ¤œç´¢ãƒ»æä¾›
  * æ•™è‚²æ”¯æ´

#### æŠ€è¡“ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
- è‡ªç„¶è¨€èªç†è§£ï¼ˆNLUï¼‰
  * æ„å›³ç†è§£ï¼ˆIntent Recognitionï¼‰
  * ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºï¼ˆEntity Extractionï¼‰
  
- å¯¾è©±ç®¡ç†ï¼ˆDialogue Managementï¼‰
  * å¯¾è©±çŠ¶æ…‹è¿½è·¡
  * æ¬¡è¡Œå‹•æ±ºå®š
  
- è‡ªç„¶è¨€èªç”Ÿæˆï¼ˆNLGï¼‰
  * å¿œç­”æ–‡ç”Ÿæˆ
  * è‡ªç„¶æ€§ã®ç¢ºä¿

### 6. æƒ…å ±æŠ½å‡ºï¼ˆInformation Extractionï¼‰

#### æŠ½å‡ºå¯¾è±¡
- å›ºæœ‰è¡¨ç¾ï¼šäººåã€åœ°åã€çµ„ç¹”å
- é–¢ä¿‚æŠ½å‡ºï¼šã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ã®é–¢ä¿‚
- ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡ºï¼šå‡ºæ¥äº‹ã®æŠ½å‡º
- çŸ¥è­˜ã‚°ãƒ©ãƒ•æ§‹ç¯‰

#### å¿œç”¨
- ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æ
- ç§‘å­¦æ–‡çŒ®ã‹ã‚‰ã®çŸ¥è­˜æŠ½å‡º
- ä¼æ¥­æƒ…å ±ã®æ•´ç†
- æ³•çš„æ–‡æ›¸ã®è§£æ

## æœ€æ–°ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«

### GPTã‚·ãƒªãƒ¼ã‚ºã®ç™ºå±•
- GPT-1ï¼ˆ2018ï¼‰ï¼šç”Ÿæˆå‹äº‹å‰å­¦ç¿’
- GPT-2ï¼ˆ2019ï¼‰ï¼šã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
- GPT-3ï¼ˆ2020ï¼‰ï¼šFew-shotå­¦ç¿’
- InstructGPTï¼šäººé–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’
- ChatGPTï¼ˆ2022ï¼‰ï¼šå¯¾è©±ç‰¹åŒ–
- GPT-4ï¼ˆ2023ï¼‰ï¼šãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œ

### æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«
- äº¬éƒ½å¤§å­¦BERT
- æ±åŒ—å¤§å­¦BERT
- rinna GPT
- Japanese T5
- Stockmark GPT
- CyberAgent LLM

### å¤šè¨€èªãƒ¢ãƒ‡ãƒ«
- mBERTï¼š104è¨€èªå¯¾å¿œ
- XLM-Rï¼š100è¨€èªå¯¾å¿œ
- mT5ï¼š101è¨€èªå¯¾å¿œ
- BLOOMï¼š176è¨€èªå¯¾å¿œ

## NLPè©•ä¾¡ã®èª²é¡Œ

### è‡ªå‹•è©•ä¾¡ã®é™ç•Œ
- èªå½™çš„é¡ä¼¼æ€§ã¸ã®åé‡
- æ„å‘³çš„ä¸€è‡´ã®ä¸å®Œå…¨ãªæ•æ‰
- å‰µé€ æ€§ãƒ»å¤šæ§˜æ€§ã®è©•ä¾¡å›°é›£

### äººæ‰‹è©•ä¾¡ã®èª²é¡Œ
- ä¸»è¦³æ€§ã¨ã‚¢ãƒãƒ†ãƒ¼ã‚¿é–“ä¸€è‡´
- ã‚³ã‚¹ãƒˆã¨æ™‚é–“ã®åˆ¶ç´„
- ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®å•é¡Œ

## ç¾åœ¨ã®èª²é¡Œã¨ä»Šå¾Œã®å±•æœ›

### æŠ€è¡“çš„èª²é¡Œ
- è¨€èªã®æ›–æ˜§æ€§ãƒ»å¤šç¾©æ€§
- æ–‡è„ˆç†è§£ã®é™ç•Œ
- å¸¸è­˜æ¨è«–ã®ä¸è¶³
- å¤šè¨€èªãƒ»æ–¹è¨€ã¸ã®å¯¾å¿œ

### ç¤¾ä¼šçš„èª²é¡Œ
- ãƒã‚¤ã‚¢ã‚¹ã¨å…¬å¹³æ€§
- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·
- å½æƒ…å ±å¯¾ç­–
- è‘—ä½œæ¨©ãƒ»çŸ¥çš„è²¡ç”£æ¨©

### ä»Šå¾Œã®å±•æœ›
- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«NLPï¼šãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒãƒ»éŸ³å£°
- åŠ¹ç‡çš„ãƒ¢ãƒ‡ãƒ«ï¼šè»½é‡åŒ–ãƒ»é«˜é€ŸåŒ–
- ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œï¼šå°‚é–€åˆ†é‡ç‰¹åŒ–
- èª¬æ˜å¯èƒ½AIï¼šæ„æ€æ±ºå®šã®é€æ˜æ€§
- äººé–“ã¨AIã®å”åƒï¼šHuman-in-the-loop

NLPã¯ç¾åœ¨ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç™»å ´ã«ã‚ˆã‚Šé©å‘½çš„ãªé€²æ­©ã‚’é‚ã’ã¦ãŠã‚Šã€
äººé–“ã¨ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®è‡ªç„¶ãªå¯¾è©±ã‚’å®Ÿç¾ã™ã‚‹é‡è¦æŠ€è¡“ã¨ã—ã¦ç™ºå±•ã—ç¶šã‘ã¦ã„ã¾ã™ã€‚
""", encoding='utf-8')
        
        # Computer Vision / ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³
        computer_vision = self.knowledge_base_dir / "05_computer_vision.txt"
        computer_vision.write_text("""
# ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ï¼ˆComputer Visionï¼‰

## æ¦‚è¦
ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒäººé–“ã®è¦–è¦šã‚·ã‚¹ãƒ†ãƒ ã®ã‚ˆã†ã«
ç”»åƒã‚„å‹•ç”»ã‹ã‚‰æƒ…å ±ã‚’ç†è§£ãƒ»è§£é‡ˆã™ã‚‹æŠ€è¡“åˆ†é‡ã§ã™ã€‚

## åŸºæœ¬çš„ãªç”»åƒå‡¦ç†

### 1. å‰å‡¦ç†ï¼ˆPreprocessingï¼‰
#### ç”»åƒã®åŸºæœ¬æ“ä½œ
- ãƒªã‚µã‚¤ã‚ºï¼ˆResizeï¼‰ï¼šç”»åƒã‚µã‚¤ã‚ºã®å¤‰æ›´
- ã‚¯ãƒ­ãƒƒãƒ”ãƒ³ã‚°ï¼ˆCroppingï¼‰ï¼šç”»åƒã®åˆ‡ã‚Šå‡ºã—
- å›è»¢ãƒ»åè»¢ï¼šãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
- æ­£è¦åŒ–ï¼šãƒ”ã‚¯ã‚»ãƒ«å€¤ã®æ¨™æº–åŒ–

#### ãƒã‚¤ã‚ºé™¤å»
- ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ï¼šæ»‘ã‚‰ã‹ãªãƒã‚¤ã‚ºé™¤å»
- ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ï¼šçªç™ºçš„ãƒã‚¤ã‚ºé™¤å»
- ãƒã‚¤ãƒ©ãƒ†ãƒ©ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ï¼šã‚¨ãƒƒã‚¸ä¿æŒãƒã‚¤ã‚ºé™¤å»

#### ã‚¨ãƒƒã‚¸æ¤œå‡º
- Sobel ãƒ•ã‚£ãƒ«ã‚¿ï¼šå‹¾é…ãƒ™ãƒ¼ã‚¹
- Canny ã‚¨ãƒƒã‚¸æ¤œå‡ºï¼šå¤šæ®µéšå‡¦ç†
- Laplacianï¼šäºŒæ¬¡å¾®åˆ†ãƒ™ãƒ¼ã‚¹

### 2. ç‰¹å¾´æŠ½å‡ºï¼ˆFeature Extractionï¼‰
#### å¾“æ¥æ‰‹æ³•
- SIFTï¼ˆScale-Invariant Feature Transformï¼‰
- SURFï¼ˆSpeeded Up Robust Featuresï¼‰
- ORBï¼ˆOriented FAST and Rotated BRIEFï¼‰
- HOGï¼ˆHistogram of Oriented Gradientsï¼‰

#### æ·±å±¤å­¦ç¿’ãƒ™ãƒ¼ã‚¹
- CNNç‰¹å¾´é‡ï¼šè‡ªå‹•çš„ç‰¹å¾´å­¦ç¿’
- Transfer Learningï¼šäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ´»ç”¨
- Feature Pyramid Networksï¼ˆFPNï¼‰

## ä¸»è¦ãªã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯

### 1. ç”»åƒåˆ†é¡ï¼ˆImage Classificationï¼‰

#### å®šç¾©ã¨ç›®çš„
- ç”»åƒå…¨ä½“ã‚’äº‹å‰å®šç¾©ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã«åˆ†é¡
- å˜ä¸€ãƒ©ãƒ™ãƒ«ãƒ»ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«åˆ†é¡

#### ä»£è¡¨çš„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- MNISTï¼šæ‰‹æ›¸ãæ•°å­—ï¼ˆ28Ã—28ãƒ”ã‚¯ã‚»ãƒ«ï¼‰
- CIFAR-10/100ï¼šå°ç‰©ä½“10/100ã‚¯ãƒ©ã‚¹
- ImageNetï¼š1000ã‚¯ãƒ©ã‚¹ã€120ä¸‡æš
- Places365ï¼šå ´æ‰€ãƒ»ã‚·ãƒ¼ãƒ³èªè­˜

#### ä¸»è¦ãƒ¢ãƒ‡ãƒ«
- LeNet-5ï¼ˆ1998ï¼‰ï¼šåˆæœŸCNN
- AlexNetï¼ˆ2012ï¼‰ï¼šImageNetå‹åˆ©
- VGGï¼ˆ2014ï¼‰ï¼šæ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
- GoogLeNetï¼ˆ2014ï¼‰ï¼šInceptionæ§‹é€ 
- ResNetï¼ˆ2015ï¼‰ï¼šæ®‹å·®æ¥ç¶š
- DenseNetï¼ˆ2017ï¼‰ï¼šå¯†æ¥ç¶š
- EfficientNetï¼ˆ2019ï¼‰ï¼šåŠ¹ç‡çš„è¨­è¨ˆ
- Vision Transformerï¼ˆ2020ï¼‰ï¼šAttentionæ©Ÿæ§‹

#### å¿œç”¨åˆ†é‡
- åŒ»ç™‚ç”»åƒè¨ºæ–­ï¼šXç·šã€CTã€MRIè§£æ
- è£½é€ æ¥­å“è³ªç®¡ç†ï¼šæ¬ é™¥æ¤œå‡º
- è¾²æ¥­ï¼šä½œç‰©ã®ç—…æ°—è¨ºæ–­
- å°å£²ï¼šå•†å“èªè­˜ãƒ»åœ¨åº«ç®¡ç†

### 2. ç‰©ä½“æ¤œå‡ºï¼ˆObject Detectionï¼‰

#### å®šç¾©ã¨ç›®çš„
- ç”»åƒå†…ã®ç‰©ä½“ä½ç½®ï¼ˆBounding Boxï¼‰ã¨
  ã‚¯ãƒ©ã‚¹ã®åŒæ™‚äºˆæ¸¬

#### æ‰‹æ³•ã®åˆ†é¡
- Two-Stageï¼šå€™è£œé ˜åŸŸâ†’åˆ†é¡
  * R-CNNç³»åˆ—ï¼šR-CNNã€Fast R-CNNã€Faster R-CNN
  * Feature Pyramid Networksï¼ˆFPNï¼‰
  
- One-Stageï¼šç›´æ¥çš„æ¤œå‡º
  * YOLOç³»åˆ—ï¼šYOLOv1-v8
  * SSDï¼ˆSingle Shot MultiBox Detectorï¼‰
  * RetinaNetï¼šFocal Loss

#### è©•ä¾¡æŒ‡æ¨™
- mAPï¼ˆmean Average Precisionï¼‰
- IoUï¼ˆIntersection over Unionï¼‰
- ç²¾åº¦ãƒ»å†ç¾ç‡æ›²ç·š

#### å¿œç”¨åˆ†é‡
- è‡ªå‹•é‹è»¢ï¼šæ­©è¡Œè€…ãƒ»è»Šä¸¡æ¤œå‡º
- ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ï¼šç•°å¸¸è¡Œå‹•æ¤œçŸ¥
- ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ï¼šç‰©ä½“èªè­˜ãƒ»æŠŠæŒ
- å°å£²ï¼šå•†å“æ£šåˆ†æ

### 3. ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

#### å®šç¾©ã¨ç›®çš„
- ç”»åƒã®å„ãƒ”ã‚¯ã‚»ãƒ«ã«ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ä»˜ä¸
- ç‰©ä½“ã®è©³ç´°ãªé ˜åŸŸåˆ†å‰²

#### ä¸»è¦ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- FCNï¼ˆFully Convolutional Networksï¼‰
- U-Netï¼šåŒ»ç™‚ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
- DeepLabç³»åˆ—ï¼šAtrous Convolution
- PSPNetï¼ˆPyramid Scene Parsingï¼‰
- HRNetï¼ˆHigh-Resolution Networkï¼‰

#### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- PASCAL VOCï¼š20ã‚¯ãƒ©ã‚¹
- COCOï¼š80ã‚¯ãƒ©ã‚¹
- Cityscapesï¼šéƒ½å¸‚é¢¨æ™¯
- ADE20Kï¼š150ã‚¯ãƒ©ã‚¹

#### å¿œç”¨åˆ†é‡
- åŒ»ç™‚ï¼šè…«ç˜ãƒ»è‡“å™¨ã®é ˜åŸŸç‰¹å®š
- è‡ªå‹•é‹è»¢ï¼šé“è·¯ãƒ»æ­©é“èªè­˜
- è¡›æ˜Ÿç”»åƒï¼šåœŸåœ°åˆ©ç”¨åˆ†æ
- å·¥æ¥­ï¼šéƒ¨å“ã®ç²¾å¯†æ¸¬å®š

### 4. ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

#### å®šç¾©ã¨ç›®çš„
- ç‰©ä½“æ¤œå‡ºã¨ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®èåˆ
- åŒä¸€ã‚¯ãƒ©ã‚¹å†…ã®å€‹ä½“è­˜åˆ¥

#### ä¸»è¦ãƒ¢ãƒ‡ãƒ«
- Mask R-CNNï¼šFaster R-CNN + ãƒã‚¹ã‚¯äºˆæ¸¬
- YOLACTï¼šãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†
- SOLOv2ï¼šåˆ†å‰²ã«ã‚ˆã‚‹æ¤œå‡º

### 5. é¡”èªè­˜ãƒ»é¡”æ¤œå‡º

#### é¡”æ¤œå‡º
- Viola-Jonesï¼šHaarç‰¹å¾´é‡
- MTCNNï¼šMulti-task CNN
- RetinaFaceï¼šè©³ç´°ãªãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æ¤œå‡º

#### é¡”èªè­˜
- Eigenfacesï¼šä¸»æˆåˆ†åˆ†æ
- FaceNetï¼šTriplet Loss
- ArcFaceï¼šè§’åº¦ãƒãƒ¼ã‚¸ãƒ³æå¤±
- DeepFaceï¼šFacebooké–‹ç™º

#### å¿œç”¨åˆ†é‡
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼šå…¥é€€å®¤ç®¡ç†
- æ±ºæ¸ˆã‚·ã‚¹ãƒ†ãƒ ï¼šç”Ÿä½“èªè¨¼
- å†™çœŸç®¡ç†ï¼šè‡ªå‹•ã‚¿ã‚°ä»˜ã‘
- ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆï¼šARãƒ•ã‚£ãƒ«ã‚¿

### 6. å‹•ç”»è§£æï¼ˆVideo Analysisï¼‰

#### è¡Œå‹•èªè­˜ï¼ˆAction Recognitionï¼‰
- 3D CNNï¼šæ™‚ç©ºé–“ç‰¹å¾´å­¦ç¿’
- Two-Streamï¼šRGB+å…‰å­¦ãƒ•ãƒ­ãƒ¼
- Transformerï¼šæ™‚ç³»åˆ—æ³¨æ„æ©Ÿæ§‹

#### ç‰©ä½“è¿½è·¡ï¼ˆObject Trackingï¼‰
- å˜ä¸€ç‰©ä½“è¿½è·¡ï¼ˆSOTï¼‰ï¼šSORTã€DeepSORT
- å¤šç‰©ä½“è¿½è·¡ï¼ˆMOTï¼‰ï¼šFairMOTã€ByteTrack

#### å‹•ç”»ç•°å¸¸æ¤œçŸ¥
- æ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã®é€¸è„±æ¤œå‡º
- ç›£è¦–ã‚«ãƒ¡ãƒ©ã§ã®ç•°å¸¸è¡Œå‹•æ¤œçŸ¥

## 3D ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³

### 1. æ·±åº¦æ¨å®šï¼ˆDepth Estimationï¼‰
- ã‚¹ãƒ†ãƒ¬ã‚ªãƒ“ã‚¸ãƒ§ãƒ³ï¼šè¤‡æ•°ã‚«ãƒ¡ãƒ©
- å˜çœ¼æ·±åº¦æ¨å®šï¼šå˜ä¸€ç”»åƒã‹ã‚‰
- LiDARï¼šãƒ¬ãƒ¼ã‚¶ãƒ¼æ¸¬è·

### 2. 3Dç‰©ä½“æ¤œå‡º
- Point Cloudå‡¦ç†ï¼šPointNetã€PointNet++
- RGB-Dï¼šè‰²æƒ…å ±+æ·±åº¦æƒ…å ±
- è‡ªå‹•é‹è»¢ã§ã®3Dç‰©ä½“èªè­˜

### 3. SLAMï¼ˆSimultaneous Localization and Mappingï¼‰
- è¦–è¦šSLAMï¼šVisual SLAM
- ç’°å¢ƒåœ°å›³æ§‹ç¯‰ã¨è‡ªå·±ä½ç½®æ¨å®š
- AR/VRã§ã®ç©ºé–“èªè­˜

## ç”Ÿæˆç³»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³

### 1. ç”»åƒç”Ÿæˆ
- GANï¼ˆGenerative Adversarial Networksï¼‰
  * StyleGANï¼šé«˜å“è³ªé¡”ç”»åƒç”Ÿæˆ
  * BigGANï¼šé«˜è§£åƒåº¦ç”»åƒç”Ÿæˆ
  
- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ï¼ˆDiffusion Modelsï¼‰
  * DALL-E 2ï¼šãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒç”Ÿæˆ
  * Stable Diffusionï¼šã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹å®Ÿè£…
  * Midjourneyï¼šã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒ†ã‚£ãƒƒã‚¯ç”Ÿæˆ

### 2. ç”»åƒç·¨é›†
- è¶…è§£åƒï¼ˆSuper Resolutionï¼‰ï¼šSRGANã€ESRGAN
- ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ï¼šãƒã‚¤ã‚ºé™¤å»
- ã‚¤ãƒ³ãƒšã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚°ï¼šæ¬ æéƒ¨åˆ†è£œå®Œ
- ã‚¹ã‚¿ã‚¤ãƒ«å¤‰æ›ï¼šNeural Style Transfer

### 3. ç”»åƒã‹ã‚‰ç”»åƒã¸ã®å¤‰æ›
- Pix2Pixï¼šãƒšã‚¢ç”»åƒå¤‰æ›
- CycleGANï¼šéãƒšã‚¢ç”»åƒå¤‰æ›
- StarGANï¼šå¤šãƒ‰ãƒ¡ã‚¤ãƒ³å¤‰æ›

## è©•ä¾¡æŒ‡æ¨™ã¨èª²é¡Œ

### è©•ä¾¡æŒ‡æ¨™
- åˆ†é¡ï¼šAccuracyã€Top-k Accuracy
- æ¤œå‡ºï¼šmAPã€IoU
- ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼šIoUã€Diceä¿‚æ•°
- ç”Ÿæˆï¼šFIDã€ISã€LPIPS

### æŠ€è¡“çš„èª²é¡Œ
- ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚®ãƒ£ãƒƒãƒ—ï¼šå­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å·®
- å°ã•ãªç‰©ä½“ã®æ¤œå‡ºå›°é›£
- ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆé®è”½ï¼‰ã¸ã®å¯¾å¿œ
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã®è¦æ±‚

### ç¤¾ä¼šçš„èª²é¡Œ
- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·
- ãƒã‚¤ã‚¢ã‚¹ã¨å…¬å¹³æ€§
- æ‚ªç”¨é˜²æ­¢ï¼šãƒ‡ã‚£ãƒ¼ãƒ—ãƒ•ã‚§ã‚¤ã‚¯
- ç›£è¦–ç¤¾ä¼šã¸ã®æ‡¸å¿µ

## æœ€æ–°å‹•å‘ã¨ä»Šå¾Œã®å±•æœ›

### ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AI
- CLIPï¼šç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®çµ±åˆç†è§£
- DALL-Eï¼šãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒç”Ÿæˆ
- GPT-4Vï¼šè¦–è¦šè³ªå•å¿œç­”

### åŠ¹ç‡åŒ–æŠ€è¡“
- ãƒ¢ãƒ‡ãƒ«è»½é‡åŒ–ï¼šçŸ¥è­˜è’¸ç•™ã€é‡å­åŒ–
- Neural Architecture Searchï¼ˆNASï¼‰
- ã‚¨ãƒƒã‚¸ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å¯¾å¿œ

### æ–°ã—ã„å¿œç”¨åˆ†é‡
- ãƒ¡ã‚¿ãƒãƒ¼ã‚¹ï¼š3Dç©ºé–“ç†è§£
- åŒ»ç™‚AIï¼šç”»åƒè¨ºæ–­æ”¯æ´
- è¾²æ¥­AIï¼šä½œç‰©ç›£è¦–ãƒ»åç©«äºˆæ¸¬
- ç’°å¢ƒç›£è¦–ï¼šè¡›æ˜Ÿç”»åƒè§£æ

ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã¯æ·±å±¤å­¦ç¿’ã®ç™ºå±•ã¨ã¨ã‚‚ã«æ€¥é€Ÿã«é€²æ­©ã—ã€
äººé–“ã®è¦–è¦šã‚’è¶…ãˆã‚‹æ€§èƒ½ã‚’å¤šãã®ã‚¿ã‚¹ã‚¯ã§å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚
ä»Šå¾Œã‚‚æ§˜ã€…ãªåˆ†é‡ã§ã®å¿œç”¨æ‹¡å¤§ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚
""", encoding='utf-8')
        
        file_paths = [str(f) for f in self.knowledge_base_dir.glob("*.txt")]
        print(f"âœ… Created knowledge base with {len(file_paths)} documents")
        return file_paths
    
    def part1_corpus_creation(self, file_paths: List[str]) -> Dict[str, Any]:
        """
        Part 1: Comprehensive corpus creation demonstration
        Part 1: åŒ…æ‹¬çš„ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        """
        
        print("\n" + "="*70)
        print("ğŸš€ PART 1: CORPUS CREATION / ãƒ‘ãƒ¼ãƒˆ1: ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆ")
        print("="*70)
        
        # Initialize storage / ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’åˆæœŸåŒ–
        print("\nğŸ“Š Initializing storage components...")
        print("ğŸ“Š ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–ä¸­...")
        
        db_path = self.data_dir / "tutorial_corpus.db"
        self.doc_store = SQLiteDocumentStore(str(db_path))
        self.vector_store = InMemoryVectorStore()
        
        print(f"âœ… Storage initialized:")
        print(f"   Document store: {db_path}")
        print(f"   Vector store: In-memory")
        
        # Demonstrate different corpus building approaches / ç•°ãªã‚‹ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ãƒ‡ãƒ¢
        approaches = [
            ("Simple RAG", "simple_rag"),
            ("Semantic RAG", "semantic_rag"), 
            ("Knowledge RAG", "knowledge_rag")
        ]
        
        results = {}
        
        for approach_name, approach_type in approaches:
            print(f"\nğŸ“Œ Building corpus with {approach_name}...")
            print(f"ğŸ“Œ {approach_name}ã§ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰ä¸­...")
            
            # Create fresh stores for each approach / å„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒç”¨ã«æ–°ã—ã„ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
            temp_doc_store = SQLiteDocumentStore(":memory:")
            temp_vector_store = InMemoryVectorStore()
            
            # Create corpus manager / ã‚³ãƒ¼ãƒ‘ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’ä½œæˆ
            if approach_type == "simple_rag":
                manager = CorpusManager.create_simple_rag(temp_doc_store, temp_vector_store)
            elif approach_type == "semantic_rag":
                manager = CorpusManager.create_semantic_rag(temp_doc_store, temp_vector_store)
            else:  # knowledge_rag
                manager = CorpusManager.create_knowledge_rag(temp_doc_store, temp_vector_store)
            
            # Build corpus / ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
            start_time = time.time()
            stats = manager.build_corpus(file_paths)
            build_time = time.time() - start_time
            
            results[approach_type] = {
                'approach_name': approach_name,
                'files_processed': stats.total_files_processed,
                'documents_created': stats.total_documents_created,
                'chunks_created': stats.total_chunks_created,
                'processing_time': stats.total_processing_time,
                'build_time': build_time,
                'stages_executed': stats.pipeline_stages_executed
            }
            
            print(f"âœ… {approach_name} completed:")
            print(f"   Files processed: {stats.total_files_processed}")
            print(f"   Documents created: {stats.total_documents_created}")
            print(f"   Chunks created: {stats.total_chunks_created}")
            print(f"   Processing time: {stats.total_processing_time:.3f}s")
            print(f"   Pipeline stages: {stats.pipeline_stages_executed}")
        
        # Use semantic RAG for remaining parts / æ®‹ã‚Šã®éƒ¨åˆ†ã«ã¯ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯RAGã‚’ä½¿ç”¨
        print(f"\nğŸ¯ Setting up final corpus with Semantic RAG...")
        print(f"ğŸ¯ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯RAGã§æœ€çµ‚ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
        
        self.corpus_manager = CorpusManager.create_semantic_rag(self.doc_store, self.vector_store)
        self.corpus_stats = self.corpus_manager.build_corpus(file_paths)
        
        print(f"âœ… Final corpus setup completed:")
        print(f"   Total chunks: {self.corpus_stats.total_chunks_created}")
        print(f"   Vector store size: {len(self.vector_store._vectors)}")
        
        # Save corpus comparison results / ã‚³ãƒ¼ãƒ‘ã‚¹æ¯”è¼ƒçµæœã‚’ä¿å­˜
        comparison_file = self.reports_dir / "corpus_comparison.json"
        with open(comparison_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Corpus comparison saved to: {comparison_file}")
        
        return results
    
    def part2_query_engine(self) -> Dict[str, Any]:
        """
        Part 2: Query engine demonstration and testing
        Part 2: ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ‡ãƒ¢ã¨ãƒ†ã‚¹ãƒˆ
        """
        
        print("\n" + "="*70)
        print("ğŸ” PART 2: QUERY ENGINE / ãƒ‘ãƒ¼ãƒˆ2: ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³")
        print("="*70)
        
        # Initialize QueryEngine / QueryEngineã‚’åˆæœŸåŒ–
        print("\nğŸ¤– Initializing QueryEngine...")
        print("ğŸ¤– QueryEngineã‚’åˆæœŸåŒ–ä¸­...")
        
        self.query_engine = QueryEngine(
            document_store=self.doc_store,
            vector_store=self.vector_store,
            retriever=SimpleRetriever(self.vector_store),
            reranker=SimpleReranker(),
            reader=SimpleReader(),
            config=QueryEngineConfig(
                enable_query_normalization=True,
                include_sources=True,
                include_confidence=True,
                max_response_time=30.0
            )
        )
        
        print(f"âœ… QueryEngine initialized successfully")
        
        # Test queries / ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
        test_queries = [
            "äººå·¥çŸ¥èƒ½ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "æ©Ÿæ¢°å­¦ç¿’ã®ä¸»è¦ãªç¨®é¡ã‚’æ•™ãˆã¦ãã ã•ã„",
            "æ·±å±¤å­¦ç¿’ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã¯ã©ã®ã‚ˆã†ãªã‚‚ã®ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
            "è‡ªç„¶è¨€èªå‡¦ç†ã®å¿œç”¨åˆ†é‡ã¯ï¼Ÿ",
            "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã§ã§ãã‚‹ã“ã¨ã‚’æ•™ãˆã¦",
            "GANã¨VAEã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "å¼·åŒ–å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã‚’èª¬æ˜ã—ã¦",
            "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ç‰¹å¾´ã¯ï¼Ÿ"
        ]
        
        print(f"\nğŸ’¬ Testing QueryEngine with {len(test_queries)} queries...")
        print(f"ğŸ’¬ {len(test_queries)}ã‚¯ã‚¨ãƒªã§QueryEngineã‚’ãƒ†ã‚¹ãƒˆä¸­...")
        
        query_results = []
        total_start_time = time.time()
        
        for i, query in enumerate(test_queries, 1):
            print(f"\nğŸ“ Query {i}: {query}")
            
            try:
                start_time = time.time()
                result = self.query_engine.answer(query)
                end_time = time.time()
                
                query_time = end_time - start_time
                
                query_result = {
                    'query': query,
                    'answer': result.answer,
                    'confidence': result.confidence,
                    'source_count': len(result.sources),
                    'processing_time': query_time,
                    'success': True
                }
                
                print(f"ğŸ¤– Answer: {result.answer[:100]}...")
                print(f"ğŸ“Š Confidence: {result.confidence:.3f}")
                print(f"ğŸ“š Sources: {len(result.sources)}")
                print(f"â±ï¸  Time: {query_time:.3f}s")
                
            except Exception as e:
                query_result = {
                    'query': query,
                    'answer': None,
                    'confidence': 0.0,
                    'source_count': 0,
                    'processing_time': 0.0,
                    'success': False,
                    'error': str(e)
                }
                
                print(f"âŒ Query failed: {e}")
            
            query_results.append(query_result)
        
        total_time = time.time() - total_start_time
        
        # Calculate performance metrics / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
        successful_queries = [r for r in query_results if r['success']]
        
        if successful_queries:
            avg_time = sum(r['processing_time'] for r in successful_queries) / len(successful_queries)
            avg_confidence = sum(r['confidence'] for r in successful_queries) / len(successful_queries)
            avg_sources = sum(r['source_count'] for r in successful_queries) / len(successful_queries)
            
            performance_metrics = {
                'total_queries': len(test_queries),
                'successful_queries': len(successful_queries),
                'success_rate': len(successful_queries) / len(test_queries) * 100,
                'average_response_time': avg_time,
                'average_confidence': avg_confidence,
                'average_sources': avg_sources,
                'total_time': total_time,
                'throughput': len(test_queries) / total_time
            }
            
            print(f"\nğŸ“ˆ QueryEngine Performance Summary:")
            print(f"   Success rate: {performance_metrics['success_rate']:.1f}%")
            print(f"   Average response time: {avg_time:.3f}s")
            print(f"   Average confidence: {avg_confidence:.3f}")
            print(f"   Average sources per query: {avg_sources:.1f}")
            print(f"   Throughput: {performance_metrics['throughput']:.2f} queries/sec")
            
            self.performance_metrics['query_engine'] = performance_metrics
        
        # Save query results / ã‚¯ã‚¨ãƒªçµæœã‚’ä¿å­˜
        query_results_file = self.reports_dir / "query_results.json"
        with open(query_results_file, 'w', encoding='utf-8') as f:
            json.dump(query_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Query results saved to: {query_results_file}")
        
        return {
            'query_results': query_results,
            'performance_metrics': performance_metrics if successful_queries else None
        }
    
    def part3_evaluation(self) -> Dict[str, Any]:
        """
        Part 3: Comprehensive evaluation with QualityLab
        Part 3: QualityLabã«ã‚ˆã‚‹åŒ…æ‹¬çš„è©•ä¾¡
        """
        
        print("\n" + "="*70)
        print("ğŸ”¬ PART 3: EVALUATION / ãƒ‘ãƒ¼ãƒˆ3: è©•ä¾¡")
        print("="*70)
        
        # Initialize QualityLab / QualityLabã‚’åˆæœŸåŒ–
        print("\nğŸ§ª Initializing QualityLab...")
        print("ğŸ§ª QualityLabã‚’åˆæœŸåŒ–ä¸­...")
        
        quality_lab_config = QualityLabConfig(
            qa_pairs_per_document=2,
            similarity_threshold=0.8,
            question_types=["factual", "conceptual", "analytical"],
            evaluation_metrics=["bleu", "rouge", "llm_judge"],
            include_detailed_analysis=True,
            include_contradiction_detection=True,
            output_format="markdown"
        )
        
        self.quality_lab = QualityLab(
            corpus_name="complete_tutorial",
            config=quality_lab_config
        )
        
        print(f"âœ… QualityLab initialized")
        
        # Load documents for evaluation / è©•ä¾¡ç”¨æ–‡æ›¸ã‚’ãƒ­ãƒ¼ãƒ‰
        documents = []
        for file_path in self.knowledge_base_dir.glob("*.txt"):
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                documents.append(Document(
                    id=file_path.stem,
                    content=content,
                    metadata={'source': str(file_path), 'topic': file_path.stem}
                ))
        
        # Generate QA pairs / QAãƒšã‚¢ã‚’ç”Ÿæˆ
        print(f"\nğŸ“ Generating QA pairs from {len(documents)} documents...")
        print(f"ğŸ“ {len(documents)}æ–‡æ›¸ã‹ã‚‰QAãƒšã‚¢ã‚’ç”Ÿæˆä¸­...")
        
        qa_pairs = self.quality_lab.generate_qa_pairs(
            documents=documents,
            num_pairs=12,
            question_types=["factual", "conceptual", "analytical"]
        )
        
        print(f"âœ… Generated {len(qa_pairs)} QA pairs")
        
        # Analyze QA distribution / QAåˆ†å¸ƒã‚’åˆ†æ
        type_dist = {}
        for qa_pair in qa_pairs:
            q_type = qa_pair.metadata.get('question_type', 'unknown')
            type_dist[q_type] = type_dist.get(q_type, 0) + 1
        
        print(f"   Question type distribution:")
        for q_type, count in type_dist.items():
            print(f"     {q_type}: {count}")
        
        # Evaluate QueryEngine / QueryEngineã‚’è©•ä¾¡
        print(f"\nğŸ” Evaluating QueryEngine performance...")
        print(f"ğŸ” QueryEngineãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ä¸­...")
        
        start_time = time.time()
        self.evaluation_results = self.quality_lab.evaluate_query_engine(
            query_engine=self.query_engine,
            qa_pairs=qa_pairs,
            evaluation_metrics=["bleu", "rouge"],  # Simplified for demo
            include_contradiction_detection=False,  # Simplified for demo
            detailed_analysis=True
        )
        evaluation_time = time.time() - start_time
        
        print(f"âœ… Evaluation completed in {evaluation_time:.2f}s")
        
        # Analyze evaluation results / è©•ä¾¡çµæœã‚’åˆ†æ
        test_results = self.evaluation_results.get('test_results', [])
        if test_results:
            passed_tests = sum(1 for result in test_results if result.get('passed', False))
            pass_rate = (passed_tests / len(test_results)) * 100
            
            avg_score = sum(result.get('score', 0) for result in test_results) / len(test_results)
            avg_confidence = sum(result.get('confidence', 0) for result in test_results) / len(test_results)
            
            evaluation_summary = {
                'total_tests': len(test_results),
                'passed_tests': passed_tests,
                'pass_rate': pass_rate,
                'average_score': avg_score,
                'average_confidence': avg_confidence,
                'evaluation_time': evaluation_time
            }
            
            print(f"\nğŸ“Š Evaluation Summary:")
            print(f"   Total tests: {len(test_results)}")
            print(f"   Pass rate: {pass_rate:.1f}% ({passed_tests}/{len(test_results)})")
            print(f"   Average score: {avg_score:.3f}")
            print(f"   Average confidence: {avg_confidence:.3f}")
            
            self.performance_metrics['evaluation'] = evaluation_summary
        
        # Generate evaluation report / è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        print(f"\nğŸ“Š Generating comprehensive evaluation report...")
        print(f"ğŸ“Š åŒ…æ‹¬çš„è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        
        try:
            report_path = self.reports_dir / "evaluation_report.md"
            report = self.quality_lab.generate_evaluation_report(
                evaluation_results=self.evaluation_results,
                output_file=str(report_path),
                include_detailed_analysis=True,
                include_recommendations=True
            )
            
            print(f"âœ… Evaluation report generated: {report_path}")
            print(f"   Report length: {len(report)} characters")
            
        except Exception as e:
            print(f"âš ï¸  Report generation had issues: {e}")
            print(f"   Evaluation data still available for analysis")
        
        # Save evaluation data / è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        eval_data_file = self.reports_dir / "evaluation_data.json"
        with open(eval_data_file, 'w', encoding='utf-8') as f:
            # Prepare JSON-serializable data / JSONåŒ–å¯èƒ½ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            json_data = {
                'qa_pairs_count': len(qa_pairs),
                'qa_type_distribution': type_dist,
                'evaluation_summary': evaluation_summary if test_results else None,
                'test_results_count': len(test_results),
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Evaluation data saved to: {eval_data_file}")
        
        return {
            'qa_pairs': qa_pairs,
            'evaluation_results': self.evaluation_results,
            'evaluation_summary': evaluation_summary if test_results else None
        }
    
    def generate_final_report(self) -> str:
        """
        Generate comprehensive final report
        åŒ…æ‹¬çš„æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        """
        
        print("\n" + "="*70)
        print("ğŸ“‹ GENERATING FINAL REPORT / æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        print("="*70)
        
        report_content = f"""# Complete RAG Tutorial Report
# å®Œå…¨RAGãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ

Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}
ç”Ÿæˆæ—¥æ™‚: {time.strftime('%Y-%m-%d %H:%M:%S')}

## Executive Summary / è¦ç´„

This report presents the results of a comprehensive RAG (Retrieval-Augmented Generation) 
system implementation and evaluation using refinire-rag. The tutorial demonstrated 
the complete workflow from corpus creation to evaluation.

ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ã€refinire-ragã‚’ä½¿ç”¨ã—ãŸåŒ…æ‹¬çš„ãªRAGï¼ˆæ¤œç´¢æ‹¡å¼µç”Ÿæˆï¼‰ã‚·ã‚¹ãƒ†ãƒ ã®
å®Ÿè£…ã¨è©•ä¾¡ã®çµæœã‚’ç¤ºã—ã¾ã™ã€‚ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆã‹ã‚‰è©•ä¾¡ã¾ã§
ã®å®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿæ¼”ã—ã¾ã—ãŸã€‚

## Part 1: Corpus Creation Results / ãƒ‘ãƒ¼ãƒˆ1: ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆçµæœ

### Knowledge Base / çŸ¥è­˜ãƒ™ãƒ¼ã‚¹
- Documents: {len(list(self.knowledge_base_dir.glob('*.txt')))} files
- Topics: AI Fundamentals, Machine Learning, Deep Learning, NLP, Computer Vision
- Total chunks created: {self.corpus_stats.total_chunks_created if self.corpus_stats else 'N/A'}
- Processing time: {self.corpus_stats.total_processing_time:.3f}s

### Corpus Building Approaches Comparison / ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ¯”è¼ƒ
Multiple corpus building strategies were tested and compared for effectiveness.

"""
        
        # Add performance metrics if available / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚Œã°è¿½åŠ 
        if 'query_engine' in self.performance_metrics:
            qe_metrics = self.performance_metrics['query_engine']
            report_content += f"""
## Part 2: QueryEngine Performance / ãƒ‘ãƒ¼ãƒˆ2: QueryEngineãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

### Overall Performance / å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- Success Rate: {qe_metrics['success_rate']:.1f}%
- Average Response Time: {qe_metrics['average_response_time']:.3f}s
- Average Confidence: {qe_metrics['average_confidence']:.3f}
- Average Sources per Query: {qe_metrics['average_sources']:.1f}
- Throughput: {qe_metrics['throughput']:.2f} queries/sec

### Query Testing / ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ
The QueryEngine was tested with {qe_metrics['total_queries']} diverse queries covering 
AI fundamentals, machine learning concepts, and technical implementations.
QueryEngineã¯ã€AIåŸºç¤ã€æ©Ÿæ¢°å­¦ç¿’æ¦‚å¿µã€æŠ€è¡“å®Ÿè£…ã‚’ã‚«ãƒãƒ¼ã™ã‚‹
{qe_metrics['total_queries']}ã®å¤šæ§˜ãªã‚¯ã‚¨ãƒªã§ãƒ†ã‚¹ãƒˆã•ã‚Œã¾ã—ãŸã€‚
"""
        
        if 'evaluation' in self.performance_metrics:
            eval_metrics = self.performance_metrics['evaluation']
            report_content += f"""
## Part 3: Evaluation Results / ãƒ‘ãƒ¼ãƒˆ3: è©•ä¾¡çµæœ

### QA Evaluation / QAè©•ä¾¡
- Total Test Cases: {eval_metrics['total_tests']}
- Pass Rate: {eval_metrics['pass_rate']:.1f}%
- Average Score: {eval_metrics['average_score']:.3f}
- Average Confidence: {eval_metrics['average_confidence']:.3f}
- Evaluation Time: {eval_metrics['evaluation_time']:.2f}s

### Quality Assessment / å“è³ªè©•ä¾¡
The evaluation demonstrated the system's ability to provide accurate and relevant 
answers across diverse question types including factual, conceptual, and analytical queries.
è©•ä¾¡ã§ã¯ã€äº‹å®Ÿã€æ¦‚å¿µã€åˆ†æçš„è³ªå•ã‚’å«ã‚€å¤šæ§˜ãªè³ªå•ã‚¿ã‚¤ãƒ—ã«ã‚ãŸã£ã¦ã€
æ­£ç¢ºã§é–¢é€£æ€§ã®é«˜ã„å›ç­”ã‚’æä¾›ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã®èƒ½åŠ›ãŒå®Ÿè¨¼ã•ã‚Œã¾ã—ãŸã€‚
"""
        
        report_content += f"""
## Key Findings / ä¸»è¦ãªç™ºè¦‹

### Strengths / å¼·ã¿
1. **Comprehensive Architecture**: Successfully integrated corpus creation, 
   query processing, and evaluation in a unified framework.
   **åŒ…æ‹¬çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆã€ã‚¯ã‚¨ãƒªå‡¦ç†ã€è©•ä¾¡ã‚’
   çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§æ­£å¸¸ã«çµ±åˆã€‚

2. **Flexible Configuration**: Multiple corpus building approaches allow 
   optimization for different use cases.
   **æŸ”è»Ÿãªè¨­å®š**: è¤‡æ•°ã®ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€
   ç•°ãªã‚‹ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«æœ€é©åŒ–å¯èƒ½ã€‚

3. **Quality Evaluation**: Automated evaluation system provides 
   comprehensive performance assessment.
   **å“è³ªè©•ä¾¡**: è‡ªå‹•è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ãŒåŒ…æ‹¬çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã‚’æä¾›ã€‚

### Areas for Improvement / æ”¹å–„é ˜åŸŸ
1. **Response Time Optimization**: Further optimization could improve query response times.
   **å¿œç­”æ™‚é–“æœ€é©åŒ–**: ã•ã‚‰ãªã‚‹æœ€é©åŒ–ã«ã‚ˆã‚Šã‚¯ã‚¨ãƒªå¿œç­”æ™‚é–“ã‚’æ”¹å–„å¯èƒ½ã€‚

2. **Domain Specialization**: Specialized embeddings and models could enhance 
   domain-specific performance.
   **ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–**: ç‰¹åŒ–ã—ãŸåŸ‹ã‚è¾¼ã¿ã¨ãƒ¢ãƒ‡ãƒ«ã§ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®
   ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šå¯èƒ½ã€‚

## Recommendations / æ¨å¥¨äº‹é …

### For Production Deployment / æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ç”¨
1. Implement persistent vector storage (e.g., Chroma, FAISS)
2. Add monitoring and logging for production readiness
3. Implement caching for frequently asked questions
4. Consider domain-specific fine-tuning

### For Further Development / ã•ã‚‰ãªã‚‹é–‹ç™ºç”¨
1. Explore advanced retrieval strategies (hybrid search)
2. Implement custom evaluation metrics for specific domains
3. Add multi-language support
4. Develop specialized processors for different document types

## Conclusion / çµè«–

The complete RAG tutorial successfully demonstrated the full lifecycle of 
a retrieval-augmented generation system using refinire-rag. The system 
showed strong performance across all evaluated dimensions and provides 
a solid foundation for production RAG applications.

å®Œå…¨RAGãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¯ã€refinire-ragã‚’ä½¿ç”¨ã—ãŸæ¤œç´¢æ‹¡å¼µç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã®
å…¨ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚’æ­£å¸¸ã«å®Ÿæ¼”ã—ã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ã¯è©•ä¾¡ã•ã‚ŒãŸã™ã¹ã¦ã®
æ¬¡å…ƒã§å¼·ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€æœ¬ç•ªRAGã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®
å …å®ŸãªåŸºç›¤ã‚’æä¾›ã—ã¾ã™ã€‚

## Generated Files / ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«
- Knowledge Base: {self.knowledge_base_dir}
- Reports: {self.reports_dir}
- Data: {self.data_dir}

---
*Report generated by refinire-rag Complete Tutorial*
*refinire-ragå®Œå…¨ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã«ã‚ˆã‚Šç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ*
"""
        
        # Save final report / æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
        final_report_path = self.reports_dir / "complete_tutorial_report.md"
        with open(final_report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… Final report generated: {final_report_path}")
        print(f"   Report length: {len(report_content)} characters")
        
        return str(final_report_path)
    
    def run_complete_tutorial(self) -> bool:
        """
        Run the complete RAG tutorial workflow
        å®Œå…¨RAGãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
        """
        
        print("ğŸš€ Starting Complete RAG Tutorial")
        print("ğŸš€ å®Œå…¨RAGãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«é–‹å§‹")
        print("="*70)
        print("This tutorial demonstrates the complete RAG workflow:")
        print("  Part 1: Corpus Creation (Document processing & indexing)")
        print("  Part 2: Query Engine (Search & answer generation)")
        print("  Part 3: Evaluation (Performance assessment & reporting)")
        print("")
        print("ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¯å®Œå…¨ãªRAGãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿæ¼”ã—ã¾ã™ï¼š")
        print("  ãƒ‘ãƒ¼ãƒˆ1: ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆï¼ˆæ–‡æ›¸å‡¦ç†ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰")
        print("  ãƒ‘ãƒ¼ãƒˆ2: ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆæ¤œç´¢ã¨å›ç­”ç”Ÿæˆï¼‰")
        print("  ãƒ‘ãƒ¼ãƒˆ3: è©•ä¾¡ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã¨ãƒ¬ãƒãƒ¼ãƒˆï¼‰")
        
        tutorial_start_time = time.time()
        
        try:
            # Create knowledge base / çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ
            file_paths = self.create_knowledge_base()
            
            # Part 1: Corpus Creation / ãƒ‘ãƒ¼ãƒˆ1: ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆ
            corpus_results = self.part1_corpus_creation(file_paths)
            
            # Part 2: Query Engine / ãƒ‘ãƒ¼ãƒˆ2: ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³
            query_results = self.part2_query_engine()
            
            # Part 3: Evaluation / ãƒ‘ãƒ¼ãƒˆ3: è©•ä¾¡
            evaluation_results = self.part3_evaluation()
            
            # Generate final report / æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
            final_report_path = self.generate_final_report()
            
            # Tutorial completion / ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å®Œäº†
            tutorial_time = time.time() - tutorial_start_time
            
            print("\n" + "="*70)
            print("ğŸ‰ COMPLETE RAG TUTORIAL FINISHED / å®Œå…¨RAGãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å®Œäº†")
            print("="*70)
            print("âœ… All parts completed successfully!")
            print("âœ… ã™ã¹ã¦ã®ãƒ‘ãƒ¼ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
            print("")
            print(f"ğŸ“Š Tutorial Statistics / ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«çµ±è¨ˆ:")
            print(f"   Total time: {tutorial_time:.2f}s")
            print(f"   Knowledge base documents: {len(file_paths)}")
            if self.corpus_stats:
                print(f"   Corpus chunks created: {self.corpus_stats.total_chunks_created}")
            if 'query_engine' in self.performance_metrics:
                qe_metrics = self.performance_metrics['query_engine']
                print(f"   Queries tested: {qe_metrics['total_queries']}")
                print(f"   Query success rate: {qe_metrics['success_rate']:.1f}%")
            if 'evaluation' in self.performance_metrics:
                eval_metrics = self.performance_metrics['evaluation']
                print(f"   Evaluation tests: {eval_metrics['total_tests']}")
                print(f"   Evaluation pass rate: {eval_metrics['pass_rate']:.1f}%")
            
            print(f"\nğŸ“ Generated Files / ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«:")
            print(f"   Working directory: {self.work_dir}")
            print(f"   Knowledge base: {self.knowledge_base_dir}")
            print(f"   Reports: {self.reports_dir}")
            print(f"   Final report: {final_report_path}")
            
            print(f"\nğŸ“ Tutorial Learning Outcomes / ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å­¦ç¿’æˆæœ:")
            print(f"   âœ“ Corpus creation with multiple strategies")
            print(f"     è¤‡æ•°æˆ¦ç•¥ã§ã®ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆ")
            print(f"   âœ“ QueryEngine configuration and optimization")
            print(f"     QueryEngineã®è¨­å®šã¨æœ€é©åŒ–")
            print(f"   âœ“ Comprehensive RAG system evaluation")
            print(f"     åŒ…æ‹¬çš„RAGã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡")
            print(f"   âœ“ End-to-end workflow integration")
            print(f"     ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±åˆ")
            
            print(f"\nğŸš€ Next Steps / æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
            print(f"   â€¢ Explore the generated reports for detailed insights")
            print(f"     è©³ç´°ãªæ´å¯Ÿã®ãŸã‚ã«ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’æ¢ç´¢")
            print(f"   â€¢ Customize the system for your specific domain")
            print(f"     ç‰¹å®šãƒ‰ãƒ¡ã‚¤ãƒ³ç”¨ã«ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º")
            print(f"   â€¢ Deploy to production with monitoring")
            print(f"     ç›£è¦–ä»˜ãã§æœ¬ç•ªç’°å¢ƒã«ãƒ‡ãƒ—ãƒ­ã‚¤")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ Tutorial failed / ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """
    Main function to run the complete RAG tutorial
    å®Œå…¨RAGãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    
    # Create working directory / ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    temp_dir = Path(tempfile.mkdtemp(prefix="complete_rag_tutorial_"))
    
    try:
        # Initialize and run tutorial / ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’åˆæœŸåŒ–ã—ã¦å®Ÿè¡Œ
        tutorial = CompleteRAGTutorial(temp_dir)
        success = tutorial.run_complete_tutorial()
        
        if success:
            print(f"\nğŸŠ Tutorial completed successfully!")
            print(f"ğŸŠ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
            print(f"\nğŸ“‚ All files are available in: {temp_dir}")
            print(f"ğŸ“‚ ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒåˆ©ç”¨å¯èƒ½: {temp_dir}")
        else:
            print(f"\nğŸ’¥ Tutorial encountered errors")
            print(f"ğŸ’¥ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        
        return success
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  Tutorial interrupted by user")
        print(f"â¹ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        return False
    
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error: {e}")
        print(f"ğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    finally:
        # Note: Keeping temp directory for inspection
        # æ³¨æ„: æ¤œæŸ»ã®ãŸã‚ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä¿æŒ
        print(f"\nğŸ’¡ Tip: Temporary files kept for inspection at {temp_dir}")
        print(f"ğŸ’¡ ãƒ’ãƒ³ãƒˆ: æ¤œæŸ»ç”¨ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’{temp_dir}ã«ä¿æŒ")


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)