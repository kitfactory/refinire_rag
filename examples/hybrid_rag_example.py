#!/usr/bin/env python3
"""
Simple Hybrid RAG Example - 3 Clear Steps

ã“ã®ä¾‹ã¯ã€refinire-ragãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ãŸåŸºæœ¬çš„ãªRAGãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ã€
4ã¤ã®æ˜ç¢ºãªã‚¹ãƒ†ãƒƒãƒ—ã§ç¤ºã—ã¾ã™ï¼š

1. ç’°å¢ƒå¤‰æ•°ã®è¨­å®šï¼ˆEnvironment Variable Setupï¼‰
2. ã‚³ãƒ¼ãƒ‘ã‚¹ã®ä½œæˆï¼ˆCorpus Creationï¼‰  
3. ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã§ã®æ¤œç´¢ï¼ˆQuery Engine Searchï¼‰
4. å“è³ªè©•ä¾¡ï¼ˆQuality Evaluation with QualityLabï¼‰

Requirements:
- Optional: refinire-rag[bm25,chroma] for hybrid search capabilities
- Environment variables for LLM integration (OpenAI recommended)

ä½¿ç”¨æ–¹æ³• / Usage:
```bash
# OpenAI API keyã‚’è¨­å®šï¼ˆæ¨å¥¨ï¼‰
export OPENAI_API_KEY="your-api-key-here"

# ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Ÿè¡Œ
python examples/hybrid_rag_example.py
```
"""

import os
import sys
import shutil
import glob
from pathlib import Path

# Disable ChromaDB telemetry before any imports
os.environ["CHROMA_TELEMETRY_DISABLED"] = "true"
os.environ["ANONYMIZED_TELEMETRY"] = "false"
os.environ["CHROMA_ANALYTICS_ENABLED"] = "false"

# Add src to Python path for direct execution
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from refinire_rag.application import CorpusManager, QueryEngine, QualityLab
from refinire_rag.registry import PluginRegistry

def cleanup_existing_data():
    """æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    print("ğŸ§¹ Cleaning up existing data...")
    
    cleanup_targets = [
        "./business_rag.db", "./data/documents.db", "./refinire/rag/",
        "./bm25s_index/", "./bm25s_data/", "./data/bm25s/", "./data/bm25s_index/", 
        "*.bm25s", "*.index", "./data/bm25s_keyword_store.db"
    ]
    
    cleaned_count = 0
    for target in cleanup_targets:
        if "*" in target:
            for file_path in glob.glob(target):
                try:
                    Path(file_path).unlink()
                    cleaned_count += 1
                except Exception:
                    pass
        else:
            target_path = Path(target)
            if target_path.exists():
                try:
                    if target_path.is_file():
                        target_path.unlink()
                    else:
                        shutil.rmtree(target_path)
                    cleaned_count += 1
                except Exception:
                    pass
    
    print(f"   âœ… Cleaned up {cleaned_count} items" if cleaned_count > 0 else "   âœ¨ Starting fresh")

def step1_setup_environment():
    """
    ã‚¹ãƒ†ãƒƒãƒ—1: ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
    
    refinire-ragãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ç’°å¢ƒå¤‰æ•°ã‚’ä½¿ã£ã¦å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’è‡ªå‹•è¨­å®šã—ã¾ã™ã€‚
    ã“ã“ã§ã¯åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’æ¤œå‡ºã—ã€æœ€é©ãªè¨­å®šã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¾ã™ã€‚
    """
    print("\n" + "="*60)
    print("ğŸ”§ STEP 1: Environment Variable Setup")
    print("="*60)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    cleanup_existing_data()
    
    # åŸºæœ¬è¨­å®š: Document Storeï¼ˆæ–‡æ›¸ä¿å­˜ï¼‰
    print("\nğŸ“ Setting up Document Store...")
    os.environ.setdefault("REFINIRE_RAG_DOCUMENT_STORES", "sqlite")
    os.environ.setdefault("REFINIRE_RAG_SQLITE_DB_PATH", "./business_rag.db")
    print("   âœ… SQLite document store configured")
    
    # Embedderè¨­å®š: ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ç”¨ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆæ­£ã—ã„åå‰ã‚’ä½¿ç”¨ï¼‰
    print("\nğŸ§  Setting up Embedder...")
    if os.environ.get("OPENAI_API_KEY"):
        os.environ.setdefault("REFINIRE_RAG_EMBEDDERS", "openai")
        print("   âœ… OpenAI embedder configured (high quality)")
    else:
        os.environ.setdefault("REFINIRE_RAG_EMBEDDERS", "tfidf")
        print("   âš ï¸  TF-IDF embedder configured (no API key required)")
        print("      ğŸ’¡ For better results, set OPENAI_API_KEY environment variable")
    
    # ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
    print("\nğŸ“¦ Checking Plugin Availability...")
    available_plugins = {
        'chroma': PluginRegistry.get_plugin_class('vector_stores', 'chroma') is not None,
        'bm25s': PluginRegistry.get_plugin_class('keyword_stores', 'bm25s_keyword') is not None
    }
    
    # Vector Storeè¨­å®š: ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢
    if available_plugins['chroma']:
        os.environ.setdefault("REFINIRE_RAG_VECTOR_STORES", "chroma")
        print("   âœ… Chroma vector store configured (external plugin)")
    else:
        os.environ.setdefault("REFINIRE_RAG_VECTOR_STORES", "inmemory_vector")
        print("   âœ… InMemory vector store configured (built-in)")
    
    # Keyword Storeè¨­å®š: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
    if available_plugins['bm25s']:
        os.environ.setdefault("REFINIRE_RAG_KEYWORD_STORES", "bm25s_keyword")
        os.environ.setdefault("REFINIRE_RAG_BM25S_INDEX_PATH", "./data/bm25s_index")
        print("   âœ… BM25s keyword store configured (external plugin)")
    else:
        print("   âš ï¸  No keyword store available (install refinire-rag[bm25] for hybrid search)")
    
    # Retrieverè¨­å®š: ä¸€è²«ã—ãŸæ¤œç´¢è¨­å®š
    print("\nğŸ¯ Configuring Unified Search Components...")
    
    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢è¨­å®šï¼šChromaVectorStore + BM25sKeywordStore ã®ã¿ä½¿ç”¨
    # REFINIRE_RAG_RETRIEVERS ã¯ä¸è¦ - ç›´æ¥ Vector Store ã¨ Keyword Store ã‚’ä½¿ç”¨
    hybrid_components = []
    if available_plugins['chroma']:
        hybrid_components.append("Chroma vector search")
    if available_plugins['bm25s']:
        hybrid_components.append("BM25s keyword search")
    
    if hybrid_components:
        print(f"   âœ… Direct hybrid search configured: {', '.join(hybrid_components)}")
        print("   ğŸ’¡ Using direct storage access (no wrapper retrievers needed)")
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªRetrieverã‚’è¨­å®š
        os.environ.setdefault("REFINIRE_RAG_RETRIEVERS", "simple")
        print("   âœ… Simple retriever configured (fallback - no plugins available)")
    
    # Rerankerè¨­å®š: æ¤œç´¢çµæœã®å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰
    available_rerankers = PluginRegistry.list_available_plugins('rerankers')
    has_openai = bool(os.environ.get("OPENAI_API_KEY"))
    
    if "llm" in available_rerankers and has_openai:
        os.environ.setdefault("REFINIRE_RAG_RERANKERS", "llm")
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–è¨­å®š
        os.environ.setdefault("REFINIRE_RAG_LLM_RERANKER_BATCH_SIZE", "15")  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯å…ƒã«æˆ»ã™ï¼ˆ1ãƒãƒƒãƒãªã®ã§åŠ¹æœãªã—ï¼‰
        os.environ.setdefault("REFINIRE_RAG_LLM_RERANKER_TEMPERATURE", "0.0")  # ã‚ˆã‚Šä¸€è²«ã—ãŸã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        print("   âœ… LLM reranker configured (highest quality, optimized batching)")
    elif "rrf" in available_rerankers:
        os.environ.setdefault("REFINIRE_RAG_RERANKERS", "rrf")
        print("   âœ… RRF reranker configured (mathematical fusion, fastest)")
    elif "heuristic" in available_rerankers:
        os.environ.setdefault("REFINIRE_RAG_RERANKERS", "heuristic")
        print("   âœ… Heuristic reranker configured (keyword-based, fast)")
    
    # Answer Synthesizerè¨­å®š: å›ç­”ç”Ÿæˆ
    os.environ.setdefault("REFINIRE_RAG_SYNTHESIZERS", "answer")
    print("   âœ… Answer synthesizer configured")
    
    if has_openai:
        os.environ.setdefault("REFINIRE_RAG_LLM_MODEL", "gpt-4o-mini")
        print("   âœ… LLM model: gpt-4o-mini")
    
    if len(hybrid_components) > 1:
        print("   ğŸš€ Hybrid search ready: Vector + Keyword + Reranking + LLM")
    else:
        print("   ğŸ” Single-mode search ready: Vector/Keyword + Reranking + LLM")
    
    print(f"\nğŸ“‹ Environment Setup Summary:")
    print(f"   â€¢ Document Store: {os.environ.get('REFINIRE_RAG_DOCUMENT_STORES', 'None')}")
    print(f"   â€¢ Vector Store: {os.environ.get('REFINIRE_RAG_VECTOR_STORES', 'None')}")
    print(f"   â€¢ Keyword Store: {os.environ.get('REFINIRE_RAG_KEYWORD_STORES', 'None')}")
    print(f"   â€¢ Reranker: {os.environ.get('REFINIRE_RAG_RERANKERS', 'None')}")
    print(f"   â€¢ Synthesizer: {os.environ.get('REFINIRE_RAG_SYNTHESIZERS', 'None')}")
    print(f"   â€¢ Embedder: {os.environ.get('REFINIRE_RAG_EMBEDDERS', 'None')}")
    
    return available_plugins

def step2_create_corpus():
    """
    ã‚¹ãƒ†ãƒƒãƒ—2: CorpusManagerã§ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆ
    
    CorpusManagerã¯ç’°å¢ƒå¤‰æ•°ã®è¨­å®šã«åŸºã¥ã„ã¦è‡ªå‹•çš„ã«ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã€
    ãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆæ¤œç´¢å¯èƒ½ãªæ–‡æ›¸é›†åˆï¼‰ã‚’ä½œæˆã—ã¾ã™ã€‚
    ChromaVectorStore ã¨ BM25sKeywordStore ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    """
    print("\n" + "="*60)
    print("ğŸ“š STEP 2: Corpus Creation with CorpusManager")
    print("="*60)
    
    # CorpusManagerä½œæˆï¼ˆChroma + BM25s ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è¨­å®šï¼‰
    corpus_manager = CorpusManager()
    data_path = Path(__file__).parent.parent / "tests" / "data" / "business_dataset"
    
    # æ–‡æ›¸ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    import_stats = corpus_manager.import_original_documents(
        corpus_name="business_knowledge",
        directory=str(data_path),
        glob="*.txt"
    )
    print(f"   âœ… Documents imported: {import_stats.total_documents_created}")
    print(f"   â±ï¸  Import time: {import_stats.total_processing_time:.2f}s")
    
    # ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰ï¼ˆåŸ‹ã‚è¾¼ã¿ç”Ÿæˆãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼‰
    print(f"\nğŸ”¨ Building corpus with embeddings and indexing...")
    build_stats = corpus_manager.rebuild_corpus_from_original(
        corpus_name="business_knowledge"
    )
    print(f"   âœ… Chunks created: {build_stats.total_chunks_created}")
    print(f"   â±ï¸  Build time: {build_stats.total_processing_time:.2f}s")
    
    # ã‚³ãƒ¼ãƒ‘ã‚¹æƒ…å ±è¡¨ç¤ºï¼ˆæ­£ã—ã„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è¨­å®šã®ç¢ºèªï¼‰
    print(f"\nğŸ“ˆ Corpus Information:")
    print(f"   ğŸ·ï¸  Name: business_knowledge")
    print(f"   ğŸ“„ Documents: {import_stats.total_documents_created}")
    print(f"   ğŸ“ Chunks: {build_stats.total_chunks_created}")
    print(f"   ğŸ” Total Retrievers Available: {len(corpus_manager.retrievers)}")
    
    # å®Ÿéš›ã«ä½¿ç”¨ã™ã‚‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ç”¨ã®Retrieverã‚’ç‰¹å®š
    hybrid_retrievers = []
    for i, retriever in enumerate(corpus_manager.retrievers):
        retriever_type = type(retriever).__name__
        print(f"      {i}: {retriever_type}")
        if retriever_type in ['ChromaVectorStore', 'BM25sKeywordStore']:
            hybrid_retrievers.append(retriever)
    
    print(f"   ğŸš€ Hybrid Search Retrievers: {len(hybrid_retrievers)} (Chroma + BM25s)")
    print(f"   ğŸ§  Embedder: {os.environ.get('REFINIRE_RAG_EMBEDDERS')}")
    
    # Return both corpus_manager and the hybrid retrievers for Step 3
    return corpus_manager, hybrid_retrievers

def step3_query_engine_search(hybrid_retrievers):
    """
    ã‚¹ãƒ†ãƒƒãƒ—3: QueryEngineã§æ¤œç´¢ãƒ»å›ç­”ç”Ÿæˆ
    
    QueryEngineã¯Step2ã®CorpusManagerã¨åŒã˜ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢è¨­å®šã‚’ä½¿ç”¨ã—ã€
    ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦æ¤œç´¢ãƒ»å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ»å›ç­”ç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚
    ChromaVectorStore + BM25sKeywordStore ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    """
    print("\n" + "="*60)
    print("ğŸ” STEP 3: Query Engine Search & Answer Generation")
    print("="*60)
    
    # QueryEngineä½œæˆï¼ˆStep2ã¨åŒã˜ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢è¨­å®šã‚’ä½¿ç”¨ï¼‰
    print("ğŸ—ï¸  Creating QueryEngine with Step2 hybrid retriever configuration...")
    query_engine = QueryEngine(retrievers=hybrid_retrievers)
    
    print(f"   âœ… QueryEngine initialized with hybrid search:")
    print(f"      â€¢ Retrievers: {[type(r).__name__ for r in query_engine.retrievers]}")
    print(f"      â€¢ Reranker: {type(query_engine.reranker).__name__ if query_engine.reranker else 'None'}")
    print(f"      â€¢ Synthesizer: {type(query_engine.synthesizer).__name__ if query_engine.synthesizer else 'None'}")
    
    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®ç¢ºèª
    hybrid_types = [type(r).__name__ for r in hybrid_retrievers]
    print(f"   ğŸš€ Using Step2 hybrid configuration: {', '.join(hybrid_types)}")
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã§æ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆæ—¥æœ¬èªãƒ“ã‚¸ãƒã‚¹é–¢é€£ï¼‰
    sample_queries = [
        "ä¼šç¤¾ã®ä¸»ãªäº‹æ¥­å†…å®¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "AIã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã®è£½å“ãƒ©ã‚¤ãƒ³ãƒŠãƒƒãƒ—ã‚’æ•™ãˆã¦ãã ã•ã„",
        "2023å¹´åº¦ã®å£²ä¸Šé«˜ã¨å–¶æ¥­åˆ©ç›Šã¯ã„ãã‚‰ã§ã™ã‹ï¼Ÿ",
        "ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆ¶åº¦ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "æƒ…å ±ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®å–ã‚Šçµ„ã¿ã¯ã©ã®ã‚ˆã†ãªã‚‚ã®ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
    ]
    
    print(f"\nğŸ” Testing {len(sample_queries)} sample queries...")
    
    for i, query_text in enumerate(sample_queries, 1):
        print(f"\nğŸ“ Query {i}: {query_text}")
        print("-" * 50)
        
        try:
            # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
            result = query_engine.query(query_text)
            
            if result.sources:
                print(f"   ğŸ“„ Found {len(result.sources)} relevant documents:")
                
                # æ¤œç´¢çµæœè¡¨ç¤º
                for j, source in enumerate(result.sources, 1):
                    print(f"      {j}. Score: {source.score:.3f}")
                    print(f"         Doc ID: {source.document_id}")
                    print(f"         Content: {source.document.content[:100]}...")
                    
                    # Rerankeræƒ…å ±è¡¨ç¤º
                    if "reranked_by" in source.metadata:
                        reranker_type = source.metadata["reranked_by"]
                        original_score = source.metadata.get("original_score", "N/A")
                        llm_score = source.metadata.get("llm_score", "N/A")
                        print(f"         ğŸ¯ Reranked by: {reranker_type}")
                        print(f"         ğŸ“Š Original â†’ LLM: {original_score:.3f} â†’ {llm_score:.3f}")
                    
                    # Use the retriever_type metadata to show actual storage technology
                    retriever_info = source.metadata.get("retriever_type", "Unknown")
                    retrieval_method = source.metadata.get("retrieval_method", "unknown")
                    
                    if retriever_info != "Unknown":
                        if retrieval_method == "vector_similarity":
                            print(f"         ğŸ” Source: {retriever_info} (vector search)")
                        elif retrieval_method == "keyword_search":
                            print(f"         ğŸ” Source: {retriever_info} (keyword search)")
                        else:
                            print(f"         ğŸ” Source: {retriever_info}")
                    else:
                        print(f"         ğŸ” Source: Unknown")
                
                # ç”Ÿæˆã•ã‚ŒãŸå›ç­”è¡¨ç¤º
                if result.answer:
                    print(f"\n   ğŸ¤– Generated Answer:")
                    print(f"      {result.answer[:150]}{'...' if len(result.answer) > 150 else ''}")
                else:
                    print(f"\n   âš ï¸  No answer generated")
                    if not query_engine.synthesizer:
                        print(f"      ğŸ’¡ No synthesizer configured - only search results available")
                
                print(f"   â±ï¸  Processing time: {result.processing_time:.3f}s")
            else:
                print("   âš ï¸  No relevant documents found")
                
        except Exception as e:
            print(f"   âŒ Query failed: {e}")
    
    return query_engine


def step4_quality_evaluation(query_engine, sample_queries, hybrid_retrievers):
    """
    ã‚¹ãƒ†ãƒƒãƒ—4: QualityLabã§å“è³ªè©•ä¾¡
    
    QualityLabã¯Step2/3ã¨åŒã˜ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢è¨­å®šã‚’ä½¿ç”¨ã—ã€
    RAGã‚·ã‚¹ãƒ†ãƒ ã®å“è³ªã‚’åŒ…æ‹¬çš„ã«è©•ä¾¡ã—ã¾ã™ã€‚
    ChromaVectorStore + BM25sKeywordStore ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    """
    print("\n" + "="*60)
    print("ğŸ”¬ STEP 4: Quality Evaluation with QualityLab")
    print("="*60)
    
    # QualityLabè¨­å®šç”¨ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šï¼ˆçµ±ä¸€è¨­å®šã®ç¢ºä¿ï¼‰
    print("ğŸ”§ Setting up QualityLab environment variables...")
    
    # çµ±ä¸€ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
    unified_dataset_path = str(Path(__file__).parent.parent / "tests" / "data" / "business_dataset")
    os.environ.setdefault("REFINIRE_RAG_UNIFIED_DATASET_PATH", unified_dataset_path)
    
    # QualityLabå›ºæœ‰è¨­å®š
    os.environ.setdefault("REFINIRE_RAG_TEST_SUITES", "llm")
    os.environ.setdefault("REFINIRE_RAG_EVALUATORS", "standard") 
    os.environ.setdefault("REFINIRE_RAG_CONTRADICTION_DETECTORS", "llm")
    os.environ.setdefault("REFINIRE_RAG_INSIGHT_REPORTERS", "standard")
    os.environ.setdefault("REFINIRE_RAG_QA_PAIRS_PER_DOCUMENT", "2")
    os.environ.setdefault("REFINIRE_RAG_EVALUATION_TIMEOUT", "30")
    os.environ.setdefault("REFINIRE_RAG_INCLUDE_CONTRADICTION_DETECTION", "true")
    
    # çµ±åˆæ€§ç¢ºä¿ã®ãŸã‚ã€å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§åŒã˜ã‚³ãƒ¼ãƒ‘ã‚¹åã‚’ä½¿ç”¨
    os.environ.setdefault("REFINIRE_RAG_DEFAULT_CORPUS_NAME", "business_knowledge")
    
    print("   âœ… QualityLab evaluation environment configured")
    print(f"      â€¢ Test Suite: {os.environ.get('REFINIRE_RAG_TEST_SUITES')}")
    print(f"      â€¢ Evaluator: {os.environ.get('REFINIRE_RAG_EVALUATORS')}")
    print(f"      â€¢ Contradiction Detector: {os.environ.get('REFINIRE_RAG_CONTRADICTION_DETECTORS')}")
    print(f"      â€¢ Insight Reporter: {os.environ.get('REFINIRE_RAG_INSIGHT_REPORTERS')}")
    
    # çµ±ä¸€è¨­å®šã®æ¤œè¨¼
    print(f"\nğŸ”— Unified Configuration Validation:")
    print(f"   â€¢ Dataset Path: {unified_dataset_path}")
    print(f"   â€¢ Default Corpus: {os.environ.get('REFINIRE_RAG_DEFAULT_CORPUS_NAME')}")
    print(f"   â€¢ Document Store: {os.environ.get('REFINIRE_RAG_DOCUMENT_STORES')}")
    print(f"   â€¢ Vector Store: {os.environ.get('REFINIRE_RAG_VECTOR_STORES')}")
    print(f"   â€¢ Retrievers: {os.environ.get('REFINIRE_RAG_RETRIEVERS')}")
    print(f"   â€¢ Reranker: {os.environ.get('REFINIRE_RAG_RERANKERS')}")
    
    # QualityLabä½œæˆï¼ˆStep2/3ã¨åŒã˜ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢è¨­å®šã‚’ä½¿ç”¨ï¼‰
    print("\nğŸ—ï¸  Creating QualityLab with Step2/3 hybrid retriever configuration...")
    try:
        quality_lab = QualityLab(retrievers=hybrid_retrievers)
        print("   âœ… QualityLab initialized with hybrid search successfully")
        print(f"      â€¢ Test Suite: {type(quality_lab.test_suite).__name__}")
        print(f"      â€¢ Evaluator: {type(quality_lab.evaluator).__name__}")
        print(f"      â€¢ Contradiction Detector: {type(quality_lab.contradiction_detector).__name__}")
        print(f"      â€¢ Insight Reporter: {type(quality_lab.insight_reporter).__name__}")
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®ç¢ºèª
        hybrid_types = [type(r).__name__ for r in hybrid_retrievers]
        print(f"   ğŸš€ Using Step2/3 hybrid configuration: {', '.join(hybrid_types)}")
    except Exception as e:
        print(f"   âš ï¸  QualityLab initialization failed: {e}")
        print("   ğŸ’¡ Continuing without quality evaluation...")
        return None
    
    # ã‚µãƒ³ãƒ—ãƒ«è©•ä¾¡ã®å®Ÿè¡Œ
    print(f"\nğŸ§ª Running comprehensive RAG evaluation...")
    
    # ã¾ãšå€‹åˆ¥ã®è©•ä¾¡æ©Ÿèƒ½ã‚’ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print(f"\nğŸ“‹ Step 4.1: Generating QA pairs from documents...")
    try:
        qa_pairs = quality_lab.generate_qa_pairs(
            qa_set_name="demo_evaluation",
            corpus_name="business_knowledge", 
            num_pairs=3
        )
        print(f"   âœ… Generated {len(qa_pairs)} QA pairs for evaluation")
        
        # QAãƒšã‚¢ã®ä¾‹ã‚’è¡¨ç¤º
        for i, qa_pair in enumerate(qa_pairs[:2], 1):  # æœ€åˆã®2ã¤ã‚’è¡¨ç¤º
            print(f"      Q{i}: {qa_pair.question[:80]}...")
            print(f"      A{i}: {qa_pair.answer[:80]}...")
            
    except Exception as e:
        print(f"   âš ï¸  QA generation failed: {e}")
        qa_pairs = []
    
    print(f"\nğŸ” Step 4.2: Evaluating QueryEngine responses...")
    try:
        if qa_pairs:
            # ç”Ÿæˆã•ã‚ŒãŸQAãƒšã‚¢ã§è©•ä¾¡
            evaluation_result = quality_lab.evaluate_query_engine(
                query_engine=query_engine, 
                qa_pairs=qa_pairs[:3]  # æœ€åˆã®3ã¤ã§è©•ä¾¡
            )
        else:
            # ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã§QAãƒšã‚¢ã‚’æ‰‹å‹•ä½œæˆ
            print(f"   ğŸ“ Creating test QA pairs from sample queries...")
            import uuid
            from refinire_rag.models.qa_pair import QAPair
            
            manual_qa_pairs = []
            for i, query in enumerate(sample_queries[:3]):
                qa_pair = QAPair(
                    question=query,
                    answer="Expected answer placeholder",  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å›ç­”
                    document_id=f"test_doc_{i}",
                    metadata={
                        "qa_set_name": "manual_demo",
                        "question_type": "factual",
                        "source": "manual_creation"
                    }
                )
                manual_qa_pairs.append(qa_pair)
            
            evaluation_result = quality_lab.evaluate_query_engine(
                query_engine=query_engine,
                qa_pairs=manual_qa_pairs
            )
            
        print(f"   âœ… QueryEngine evaluation completed")
        
        # è©•ä¾¡çµæœã®è¡¨ç¤ºï¼ˆè¾æ›¸å½¢å¼ï¼‰
        if evaluation_result and "evaluation_summary" in evaluation_result:
            summary = evaluation_result["evaluation_summary"]
            print(f"   ğŸ“Š Success Rate: {summary.get('success_rate', 0):.1%}")
            print(f"   ğŸ¯ Average Confidence: {summary.get('average_confidence', 0):.2f}")
            print(f"   â±ï¸  Average Response Time: {summary.get('average_processing_time', 0):.2f}s")
        else:
            print(f"   âš ï¸  Evaluation completed but no summary available")
            
    except Exception as e:
        print(f"   âš ï¸  QueryEngine evaluation failed: {e}")
        evaluation_result = None
    
    print(f"\nğŸ”¬ Step 4.3: Generating comprehensive evaluation report...")
    try:
        # åŒ…æ‹¬çš„ãªè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        if evaluation_result:
            report = quality_lab.generate_evaluation_report(
                evaluation_results=evaluation_result  # æ­£ã—ã„å¼•æ•°å
            )
            
            print(f"   âœ… Evaluation report generated")
            
            # ãƒ¬ãƒãƒ¼ãƒˆã®ä¸€éƒ¨ã‚’è¡¨ç¤º
            if report and len(report) > 200:
                print(f"\nğŸ“„ Evaluation Report Preview:")
                print(f"   {'-' * 50}")
                # ãƒ¬ãƒãƒ¼ãƒˆã®æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤º
                report_lines = report.split('\n')[:10]
                for line in report_lines:
                    if line.strip():
                        print(f"   {line}")
                print(f"   {'-' * 50}")
                print(f"   ğŸ“ Full report: {len(report)} characters generated")
            
            return evaluation_result
        else:
            print(f"   âš ï¸  Cannot generate report without evaluation results")
            return None
            
    except Exception as e:
        print(f"   âŒ Report generation failed: {e}")
        return None
    
    print(f"\nğŸ¥ Step 4.4: Quality health check...")
    try:
        # QualityLabã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        stats = quality_lab.get_lab_stats()
        print(f"   âœ… Quality health check completed")
        print(f"   ğŸ“Š QA Pairs Generated: {stats.get('qa_pairs_generated', 0)}")
        print(f"   ğŸ§ª Evaluations Completed: {stats.get('evaluations_completed', 0)}")
        print(f"   ğŸ“‹ Reports Generated: {stats.get('reports_generated', 0)}")
        print(f"   â±ï¸  Total Processing Time: {stats.get('total_processing_time', 0):.2f}s")
        
        # ãŠã™ã™ã‚ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        print(f"\nğŸ’¡ Quality Recommendations:")
        if stats.get('evaluations_completed', 0) > 0:
            print(f"   âœ… RAG system evaluation is working properly")
            print(f"   ğŸ”§ Consider fine-tuning parameters for better performance")
            print(f"   ğŸ“ˆ Monitor evaluation metrics regularly for quality assurance")
        else:
            print(f"   âš ï¸  Consider running more comprehensive evaluations")
            print(f"   ğŸ“š Add more diverse test cases for better coverage")
        
        return evaluation_result
        
    except Exception as e:
        print(f"   âŒ Health check failed: {e}")
        return evaluation_result


def main():
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°: 4ã‚¹ãƒ†ãƒƒãƒ—ã§ã®åŒ…æ‹¬çš„RAGã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰ãƒ»è©•ä¾¡
    
    ã“ã®é–¢æ•°ã¯ä»¥ä¸‹ã®4ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’é †æ¬¡å®Ÿè¡Œã—ã€æœ¬æ ¼çš„ãªRAGã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ï¼š
    1. Environment Setup - ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã¨ç’°å¢ƒå¤‰æ•°ã®è‡ªå‹•è¨­å®š
    2. Corpus Creation - CorpusManagerã«ã‚ˆã‚‹æ–‡æ›¸å‡¦ç†ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    3. Query Engine - QueryEngineã«ã‚ˆã‚‹æ¤œç´¢ãƒ»å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ»å›ç­”ç”Ÿæˆ
    4. Quality Evaluation - QualityLabã«ã‚ˆã‚‹åŒ…æ‹¬çš„å“è³ªè©•ä¾¡ã¨ç›£è¦–
    
    å„ã‚¹ãƒ†ãƒƒãƒ—ã§è‡ªå‹•çš„ã«ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’æ¤œå‡ºãƒ»è¨­å®šã—ã€åˆ©ç”¨å¯èƒ½ãªæœ€é«˜ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    """
    print("ğŸš€ Simple Hybrid RAG Example - 4 Clear Steps")
    print("=" * 60)
    print("This example demonstrates a complete RAG workflow in 4 simple steps:")
    print("1. Environment Variable Setup")
    print("2. Corpus Creation with CorpusManager")
    print("3. Query Engine Search & Answer Generation")
    print("4. Quality Evaluation with QualityLab")
    print()
    print("All components are automatically configured from environment variables!")
    
    try:
        # Step 1: ç’°å¢ƒå¤‰æ•°è¨­å®š
        available_plugins = step1_setup_environment()
        
        # Step 2: ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢è¨­å®šï¼‰
        corpus_manager, hybrid_retrievers = step2_create_corpus()
        
        # Step 3: ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³æ¤œç´¢ï¼ˆStep2ã¨åŒã˜ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢è¨­å®šï¼‰
        query_engine = step3_query_engine_search(hybrid_retrievers)
        
        # Step 4: å“è³ªè©•ä¾¡ï¼ˆStep2/3ã¨åŒã˜ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢è¨­å®šï¼‰
        sample_queries = [
            "ä¼šç¤¾ã®ä¸»ãªäº‹æ¥­å†…å®¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "AIã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã®è£½å“ãƒ©ã‚¤ãƒ³ãƒŠãƒƒãƒ—ã‚’æ•™ãˆã¦ãã ã•ã„",
            "2023å¹´åº¦ã®å£²ä¸Šé«˜ã¨å–¶æ¥­åˆ©ç›Šã¯ã„ãã‚‰ã§ã™ã‹ï¼Ÿ",
            "ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆ¶åº¦ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
            "æƒ…å ±ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®å–ã‚Šçµ„ã¿ã¯ã©ã®ã‚ˆã†ãªã‚‚ã®ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
        ]
        evaluation_result = step4_quality_evaluation(query_engine, sample_queries, hybrid_retrievers)
        
        # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        print("\n" + "="*60)
        print("ğŸ‰ SUCCESS: Complete RAG System with Quality Evaluation!")
        print("="*60)
        print("Your RAG system is now fully configured, tested, and ready for production use.")
        print()
        print("ğŸ—ï¸  System Architecture (Unified Hybrid Search):")
        print(f"   â€¢ CorpusManager: {len(corpus_manager.retrievers)} retrievers configured")
        print(f"   â€¢ QueryEngine: {[type(r).__name__ for r in query_engine.retrievers]}")
        print(f"   â€¢ QualityLab: Same hybrid retriever configuration")
        print(f"   â€¢ Reranker: {type(query_engine.reranker).__name__ if query_engine.reranker else 'None'}")
        print(f"   â€¢ Synthesizer: {type(query_engine.synthesizer).__name__ if query_engine.synthesizer else 'None'}")
        print(f"   ğŸš€ All steps use consistent ChromaVectorStore + BM25sKeywordStore hybrid search")
        
        print(f"\nğŸ”¬ Quality Assurance:")
        if evaluation_result and "evaluation_summary" in evaluation_result:
            summary = evaluation_result["evaluation_summary"]
            success_rate = summary.get('success_rate', 0)
            print(f"   â€¢ QualityLab: Comprehensive evaluation completed")
            print(f"   â€¢ Success Rate: {success_rate:.1%}")
            print(f"   â€¢ Evaluation Components: TestSuite â†’ Evaluator â†’ Insights")
        else:
            print(f"   â€¢ QualityLab: Ready for evaluation (check configuration)")
        
        print(f"\nğŸš€ Production Ready Features:")
        print(f"   â€¢ Plugin-based architecture with environment variable configuration")
        print(f"   â€¢ Automatic fallback mechanisms for missing components")
        print(f"   â€¢ Comprehensive quality evaluation and monitoring")
        print(f"   â€¢ Scalable corpus management and query processing")
        
        print()
        print("ğŸ’¡ Next Steps & Best Practices:")
        print("   ğŸ” Query Testing:")
        print("     query_engine.query('your business question here')")
        print("   ğŸ“Š Quality Monitoring:")
        print("     quality_lab.evaluate_query_engine(query_engine, custom_queries)")
        print("   âš™ï¸  Configuration Tuning:")
        print("     - Adjust environment variables for different plugins")
        print("     - Test different reranker and embedder combinations")
        print("   ğŸ“š Content Management:")
        print("     - Add your business documents to the corpus")
        print("     - Monitor evaluation metrics as content grows")
        print("   ğŸ”§ Production Deployment:")
        print("     - Set up regular quality evaluation schedules")
        print("     - Monitor performance metrics and adjust accordingly")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ERROR: {e}")
        print("Check the error messages above and ensure all dependencies are installed.")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)