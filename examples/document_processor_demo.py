#!/usr/bin/env python3
"""
DocumentProcessor Demo - Comprehensive example of unified document processing

This example demonstrates the complete DocumentProcessor architecture,
showing how different processors work together in a unified pipeline.
"""

import sys
import time
from pathlib import Path
from typing import List

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from refinire_rag.models.document import Document
from refinire_rag.processing.document_pipeline import DocumentPipeline
from refinire_rag.processing.dictionary_maker import DictionaryMaker, DictionaryMakerConfig
from refinire_rag.processing.normalizer import Normalizer, NormalizerConfig
from refinire_rag.processing.graph_builder import GraphBuilder, GraphBuilderConfig
from refinire_rag.processing.chunker import Chunker, ChunkingConfig


def create_sample_documents() -> List[Document]:
    """Create sample documents for demonstration"""
    
    documents = [
        Document(
            id="rag_overview_001",
            content="""
            RAGÔºàRetrieval-Augmented GenerationÔºâ„ÅØ„ÄÅÊ§úÁ¥¢Êã°ÂºµÁîüÊàê„Å®Âëº„Å∞„Çå„Çã
            AIÊäÄË°ì„ÅÆÈù©Êñ∞ÁöÑ„Å™„Ç¢„Éó„É≠„Éº„ÉÅ„Åß„Åô„ÄÇ„Åì„ÅÆÊâãÊ≥ï„Åß„ÅØ„ÄÅÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÔºàLLMÔºâ„ÅÆ
            ËÉΩÂäõ„ÇíÂ§ñÈÉ®Áü•Ë≠ò„Éô„Éº„Çπ„Å®ÁµÑ„ÅøÂêà„Çè„Åõ„Çã„Åì„Å®„Åß„ÄÅ„Çà„ÇäÊ≠£Á¢∫„ÅßÊ†πÊã†„ÅÆ„ÅÇ„ÇãÂõûÁ≠î„Çí
            ÁîüÊàê„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ
            
            RAG„Ç∑„Çπ„ÉÜ„É†„ÅÆ‰∏ªË¶Å„Å™ÊßãÊàêË¶ÅÁ¥†„ÅØ‰ª•‰∏ã„ÅÆÈÄö„Çä„Åß„ÅôÔºö
            - ÊñáÊõ∏Âüã„ÇÅËæº„ÅøÔºàDocument EmbeddingÔºâ
            - „Éô„ÇØ„Éà„É´„Éá„Éº„Çø„Éô„Éº„ÇπÔºàVector DatabaseÔºâ 
            - Ê§úÁ¥¢Ê©üËÉΩÔºàRetrieval FunctionÔºâ
            - ÁîüÊàêÊ©üËÉΩÔºàGeneration FunctionÔºâ
            
            „Åì„ÅÆ„Ç¢„Éó„É≠„Éº„ÉÅ„Å´„Çà„Çä„ÄÅ„Éè„É´„Ç∑„Éç„Éº„Ç∑„Éß„É≥ÔºàÂπªË¶öÔºâ„ÅÆÂïèÈ°å„ÇíÂ§ßÂπÖ„Å´ËªΩÊ∏õ„Åó„ÄÅ
            ‰ø°È†ºÊÄß„ÅÆÈ´ò„ÅÑAI„Ç∑„Çπ„ÉÜ„É†„ÇíÊßãÁØâ„Åô„Çã„Åì„Å®„ÅåÂèØËÉΩ„Å´„Å™„Çä„Åæ„Åô„ÄÇ
            """,
            metadata={
                "title": "RAG„Ç∑„Çπ„ÉÜ„É†„ÅÆÊ¶ÇË¶Å",
                "author": "AIÁ†îÁ©∂ËÄÖ",
                "domain": "Ê©üÊ¢∞Â≠¶Áøí",
                "creation_date": "2024-01-15",
                "source": "ÊäÄË°ìÊñáÊõ∏"
            }
        ),
        
        Document(
            id="vector_search_002", 
            content="""
            „Éô„ÇØ„Éà„É´Ê§úÁ¥¢ÔºàVector SearchÔºâ„ÅØ„ÄÅ„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØÊ§úÁ¥¢„Å®„ÇÇÂëº„Å∞„Çå„ÄÅ
            ÂæìÊù•„ÅÆ„Ç≠„Éº„ÉØ„Éº„Éâ„Éô„Éº„ÇπÊ§úÁ¥¢„Å®„ÅØÊ†πÊú¨ÁöÑ„Å´Áï∞„Å™„Çã„Ç¢„Éó„É≠„Éº„ÉÅ„Åß„Åô„ÄÇ
            
            „Åì„ÅÆÊäÄË°ì„Åß„ÅØ„ÄÅÊñáÊõ∏„ÇÑ„ÇØ„Ç®„É™„ÇíÈ´òÊ¨°ÂÖÉ„Éô„ÇØ„Éà„É´Á©∫Èñì„Å´Âüã„ÇÅËæº„Åø„ÄÅ
            È°û‰ººÂ∫¶Ë®àÁÆó„Å´„Çà„ÇäÈñ¢ÈÄ£ÊÄß„ÇíÂà§ÂÆö„Åó„Åæ„Åô„ÄÇ‰∏ª„Å™Âà©ÁÇπ„Å®„Åó„Å¶„ÄÅ
            - ÊÑèÂë≥ÁöÑÈ°û‰ººÊÄß„ÅÆÊ§úÁ¥¢„ÅåÂèØËÉΩ
            - Â§öË®ÄË™ûÂØæÂøú
            - ÂêåÁæ©Ë™û„ÇÑË°®Áèæ„ÅÆÊè∫„Çâ„Åé„Å´ÂØæÂøú
            
            ÂÆüË£Ö„Å´„Åä„ÅÑ„Å¶„ÅØ„ÄÅChroma„ÄÅFaiss„ÄÅPinecone„Å™„Å©„ÅÆ„Éô„ÇØ„Éà„É´„Éá„Éº„Çø„Éô„Éº„Çπ„Åå
            Â∫É„Åè‰ΩøÁî®„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åæ„Åü„ÄÅÂüã„ÇÅËæº„Åø„É¢„Éá„É´„Å®„Åó„Å¶„ÅØ„ÄÅ
            OpenAI„ÅÆtext-embedding-ada-002„ÇÑ„ÄÅ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ„ÅÆ
            all-MiniLM-L6-v2„Å™„Å©„Åå‰∫∫Ê∞ó„Åß„Åô„ÄÇ
            """,
            metadata={
                "title": "„Éô„ÇØ„Éà„É´Ê§úÁ¥¢„ÅÆÊäÄË°ìË©≥Á¥∞",
                "author": "Ê§úÁ¥¢„Ç®„É≥„Ç∏„Éã„Ç¢", 
                "domain": "ÊÉÖÂ†±Ê§úÁ¥¢",
                "creation_date": "2024-01-20",
                "source": "ÊäÄË°ì„Éñ„É≠„Ç∞"
            }
        ),
        
        Document(
            id="evaluation_metrics_003",
            content="""
            RAG„Ç∑„Çπ„ÉÜ„É†„ÅÆË©ï‰æ°„ÅØ„ÄÅÂæìÊù•„ÅÆÊ©üÊ¢∞Â≠¶Áøí„É¢„Éá„É´„Å®„ÅØÁï∞„Å™„ÇãË™≤È°å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ
            ‰∏ªË¶Å„Å™Ë©ï‰æ°ÊåáÊ®ô„Å®„Åó„Å¶‰ª•‰∏ã„ÅåÊåô„Åí„Çâ„Çå„Åæ„ÅôÔºö
            
            „ÄêÊ§úÁ¥¢ÊÄßËÉΩ„ÅÆË©ï‰æ°„Äë
            - Á≤æÂ∫¶ÔºàPrecisionÔºâ: Ê§úÁ¥¢„Åï„Çå„ÅüÊñáÊõ∏„ÅÆ‰∏≠„ÅßÈñ¢ÈÄ£ÊÄß„ÅÆ„ÅÇ„ÇãÊñáÊõ∏„ÅÆÂâ≤Âêà
            - ÂÜçÁèæÁéáÔºàRecallÔºâ: Èñ¢ÈÄ£ÊÄß„ÅÆ„ÅÇ„ÇãÂÖ®ÊñáÊõ∏„ÅÆ‰∏≠„ÅßÊ§úÁ¥¢„Åï„Çå„ÅüÊñáÊõ∏„ÅÆÂâ≤Âêà  
            - F1„Çπ„Ç≥„Ç¢: Á≤æÂ∫¶„Å®ÂÜçÁèæÁéá„ÅÆË™øÂíåÂπ≥Âùá
            - NDCGÔºàNormalized Discounted Cumulative GainÔºâ: „É©„É≥„Ç≠„É≥„Ç∞ÂìÅË≥™„ÅÆË©ï‰æ°
            
            „ÄêÁîüÊàêÂìÅË≥™„ÅÆË©ï‰æ°„Äë
            - BLEU Score: ÂèÇÁÖßÂõûÁ≠î„Å®„ÅÆÈ°û‰ººÂ∫¶
            - ROUGE Score: Ë¶ÅÁ¥ÑÂìÅË≥™„ÅÆË©ï‰æ°
            - ‰∫∫ÊâãË©ï‰æ°: ÊµÅÊö¢„Åï„ÄÅÊ≠£Á¢∫ÊÄß„ÄÅÊúâÁî®ÊÄß
            
            „Åæ„Åü„ÄÅRAGÁâπÊúâ„ÅÆË™≤È°å„Å®„Åó„Å¶„ÄÅÊ§úÁ¥¢„Åó„ÅüÊñáÊõ∏„Å®ÁîüÊàê„Åï„Çå„ÅüÂõûÁ≠î„ÅÆ
            ‰∏ÄË≤´ÊÄß„ÇÑÁüõÁõæ„ÅÆÊ§úÂá∫„ÇÇÈáçË¶Å„Åß„Åô„ÄÇNLIÔºàNatural Language InferenceÔºâ
            „É¢„Éá„É´„Çí‰ΩøÁî®„Åó„ÅüËá™ÂãïÁüõÁõæÊ§úÂá∫„Ç∑„Çπ„ÉÜ„É†„ÅÆÁ†îÁ©∂„ÇÇÊ¥ªÁô∫„Åß„Åô„ÄÇ
            """,
            metadata={
                "title": "RAG„Ç∑„Çπ„ÉÜ„É†Ë©ï‰æ°ÊåáÊ®ô",
                "author": "ML„Ç®„É≥„Ç∏„Éã„Ç¢",
                "domain": "Ê©üÊ¢∞Â≠¶ÁøíË©ï‰æ°",
                "creation_date": "2024-01-25", 
                "source": "Â≠¶Ë°ìË´ñÊñá"
            }
        )
    ]
    
    return documents


def setup_processing_pipeline() -> DocumentPipeline:
    """Set up the complete document processing pipeline"""
    
    # Configure each processor
    dictionary_config = DictionaryMakerConfig(
        dictionary_file_path="./examples/demo_dictionary.md",
        llm_model="gpt-4o-mini",
        focus_on_technical_terms=True,
        extract_abbreviations=True,
        detect_expression_variations=True,
        min_term_importance="medium"
    )
    
    normalizer_config = NormalizerConfig(
        dictionary_file_path="./examples/demo_dictionary.md",
        normalize_variations=True,
        expand_abbreviations=True,
        standardize_technical_terms=True,
        whole_word_only=True
    )
    
    graph_config = GraphBuilderConfig(
        graph_file_path="./examples/demo_knowledge_graph.md",
        dictionary_file_path="./examples/demo_dictionary.md",
        llm_model="gpt-4o-mini",
        focus_on_important_relationships=True,
        extract_hierarchical_relationships=True,
        extract_causal_relationships=True,
        min_relationship_importance="medium"
    )
    
    chunking_config = ChunkingConfig(
        chunk_size=256,
        overlap=64,
        split_by_sentence=True
    )
    
    # Create processors
    processors = [
        DictionaryMaker(dictionary_config),
        Normalizer(normalizer_config), 
        GraphBuilder(graph_config),
        Chunker(chunking_config)
    ]
    
    # Create pipeline
    pipeline = DocumentPipeline(processors)
    
    return pipeline


def display_processing_results(documents: List[Document], final_chunks: List[Document], 
                             pipeline: DocumentPipeline):
    """Display comprehensive processing results"""
    
    print("\n" + "="*80)
    print("üìä DOCUMENT PROCESSING RESULTS")
    print("="*80)
    
    # Input summary
    print(f"\nüìÑ Input Documents: {len(documents)}")
    for doc in documents:
        print(f"   ‚Ä¢ {doc.id}: {doc.metadata.get('title', 'Untitled')}")
    
    # Output summary
    print(f"\nüìë Output Chunks: {len(final_chunks)}")
    
    # Group chunks by original document
    chunks_by_doc = {}
    for chunk in final_chunks:
        orig_id = chunk.metadata.get("original_document_id")
        if orig_id not in chunks_by_doc:
            chunks_by_doc[orig_id] = []
        chunks_by_doc[orig_id].append(chunk)
    
    for orig_id, chunks in chunks_by_doc.items():
        print(f"   ‚Ä¢ {orig_id}: {len(chunks)} chunks")
    
    # Pipeline statistics
    stats = pipeline.get_pipeline_stats()
    print(f"\n‚ö° Pipeline Performance:")
    print(f"   ‚Ä¢ Total processing time: {stats.get('total_processing_time', 0):.3f}s")
    print(f"   ‚Ä¢ Documents processed: {stats.get('total_documents_processed', 0)}")
    print(f"   ‚Ä¢ Processors executed: {len(stats.get('processors_executed', []))}")
    
    # Processor-specific statistics
    print(f"\nüîß Processor Statistics:")
    for i, processor in enumerate(pipeline.processors):
        processor_stats = processor.get_processing_stats()
        processor_name = processor.__class__.__name__
        print(f"   {i+1}. {processor_name}:")
        
        if processor_name == "DictionaryMaker":
            print(f"      - Terms extracted: {processor_stats.get('terms_extracted', 0)}")
            print(f"      - Variations detected: {processor_stats.get('variations_detected', 0)}")
            print(f"      - LLM API calls: {processor_stats.get('llm_api_calls', 0)}")
            
        elif processor_name == "Normalizer":
            print(f"      - Total replacements: {processor_stats.get('total_replacements', 0)}")
            print(f"      - Variations normalized: {processor_stats.get('variations_normalized', 0)}")
            print(f"      - Abbreviations expanded: {processor_stats.get('abbreviations_expanded', 0)}")
            
        elif processor_name == "GraphBuilder":
            print(f"      - Relationships extracted: {processor_stats.get('relationships_extracted', 0)}")
            print(f"      - Graph updates: {processor_stats.get('graph_updates', 0)}")
            print(f"      - LLM API calls: {processor_stats.get('llm_api_calls', 0)}")
            
        elif processor_name == "Chunker":
            print(f"      - Chunks created: {processor_stats.get('chunks_created', 0)}")
            print(f"      - Average chunk size: {processor_stats.get('average_chunk_size', 0):.1f}")
        
        print(f"      - Documents processed: {processor_stats.get('documents_processed', 0)}")


def display_generated_files():
    """Display information about generated files"""
    
    print(f"\nüìÅ Generated Files:")
    
    dict_path = Path("./examples/demo_dictionary.md")
    graph_path = Path("./examples/demo_knowledge_graph.md")
    
    if dict_path.exists():
        print(f"   üìñ Dictionary: {dict_path}")
        print(f"      Size: {dict_path.stat().st_size:,} bytes")
        
        # Show dictionary preview
        content = dict_path.read_text(encoding='utf-8')
        lines = content.split('\n')[:10]
        print(f"      Preview:")
        for line in lines:
            if line.strip():
                print(f"        {line}")
        lines_total = len(content.split('\n'))
        if lines_total > 10:
            print(f"        ... ({lines_total - 10} more lines)")
    
    if graph_path.exists():
        print(f"\n   üï∏Ô∏è  Knowledge Graph: {graph_path}")
        print(f"      Size: {graph_path.stat().st_size:,} bytes")
        
        # Show graph preview
        content = graph_path.read_text(encoding='utf-8')
        lines = content.split('\n')[:10]
        print(f"      Preview:")
        for line in lines:
            if line.strip():
                print(f"        {line}")
        lines_total = len(content.split('\n'))
        if lines_total > 10:
            print(f"        ... ({lines_total - 10} more lines)")


def display_sample_chunks(final_chunks: List[Document], num_samples: int = 3):
    """Display sample processed chunks"""
    
    print(f"\nüìù Sample Processed Chunks (showing {min(num_samples, len(final_chunks))}):")
    
    for i, chunk in enumerate(final_chunks[:num_samples]):
        print(f"\n   Chunk {i+1}/{len(final_chunks)} (ID: {chunk.id}):")
        print(f"   Original Document: {chunk.metadata.get('original_document_id')}")
        print(f"   Processing Stage: {chunk.metadata.get('processing_stage')}")
        print(f"   Chunk Position: {chunk.metadata.get('chunk_position', 0)}")
        
        # Show content preview
        content_preview = chunk.content[:200] + "..." if len(chunk.content) > 200 else chunk.content
        print(f"   Content: {content_preview}")
        
        # Show relevant metadata
        if "normalization_stats" in chunk.metadata:
            norm_stats = chunk.metadata["normalization_stats"]
            if norm_stats.get("total_replacements", 0) > 0:
                print(f"   Normalizations: {norm_stats['total_replacements']} replacements")
        
        if "graph_metadata" in chunk.metadata:
            graph_meta = chunk.metadata["graph_metadata"]
            if graph_meta.get("new_relationships_extracted", 0) > 0:
                print(f"   Graph Relations: {graph_meta['new_relationships_extracted']} extracted")


def main():
    """Main demonstration function"""
    
    print("üöÄ DocumentProcessor Architecture Demo")
    print("="*50)
    print("This demo showcases the unified DocumentProcessor architecture")
    print("with LLM-powered term extraction and knowledge graph building.")
    
    # Create sample documents
    print("\nüìö Creating sample documents...")
    documents = create_sample_documents()
    print(f"Created {len(documents)} sample documents")
    
    # Set up processing pipeline
    print("\nüîß Setting up processing pipeline...")
    pipeline = setup_processing_pipeline()
    print("Pipeline configured with:")
    for i, processor in enumerate(pipeline.processors):
        print(f"  {i+1}. {processor.__class__.__name__}")
    
    # Process documents
    print("\n‚ö° Processing documents through pipeline...")
    start_time = time.time()
    
    all_chunks = []
    for doc in documents:
        print(f"  Processing: {doc.id}")
        chunks = pipeline.process_document(doc)
        all_chunks.extend(chunks)
    
    processing_time = time.time() - start_time
    print(f"‚úÖ Processing completed in {processing_time:.3f} seconds")
    
    # Display results
    display_processing_results(documents, all_chunks, pipeline)
    display_generated_files()
    display_sample_chunks(all_chunks)
    
    print(f"\nüéâ Demo completed successfully!")
    print(f"   ‚Ä¢ {len(documents)} documents ‚Üí {len(all_chunks)} chunks")
    print(f"   ‚Ä¢ Total processing time: {processing_time:.3f}s")
    print(f"   ‚Ä¢ Average time per document: {processing_time/len(documents):.3f}s")
    
    return all_chunks


if __name__ == "__main__":
    try:
        result_chunks = main()
        print("\n‚úÖ DocumentProcessor demo completed successfully!")
    except Exception as e:
        print(f"\n‚ùå Demo failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)