#!/usr/bin/env python3
"""
QualityLabçµ±åˆä¾‹

TestSuiteã€Evaluatorã€ContradictionDetectorã€InsightReporterã‚’
çµ±åˆã—ãŸRAGã‚·ã‚¹ãƒ†ãƒ å“è³ªè©•ä¾¡ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚
"""

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from refinire_rag.processing.test_suite import TestSuite, TestSuiteConfig
from refinire_rag.processing.evaluator import Evaluator, EvaluatorConfig
from refinire_rag.processing.contradiction_detector import ContradictionDetector, ContradictionDetectorConfig
from refinire_rag.processing.insight_reporter import InsightReporter, InsightReporterConfig
from refinire_rag.processing.document_pipeline import DocumentPipeline
from refinire_rag.models.document import Document


def create_quality_lab_pipeline():
    """QualityLabãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ"""
    
    print("ğŸ”¬ QualityLabçµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆä¸­...")
    
    # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®è¨­å®š
    test_suite_config = TestSuiteConfig(
        auto_generate_cases=True,
        max_cases_per_document=5,
        include_negative_cases=True
    )
    
    evaluator_config = EvaluatorConfig(
        include_category_analysis=True,
        include_failure_analysis=True,
        accuracy_threshold=0.8,
        response_time_threshold=2.0,
        confidence_threshold=0.7
    )
    
    contradiction_config = ContradictionDetectorConfig(
        enable_claim_extraction=True,
        enable_nli_detection=True,
        contradiction_threshold=0.7,
        check_within_document=True,
        check_across_documents=True
    )
    
    insight_config = InsightReporterConfig(
        enable_trend_analysis=True,
        enable_comparative_analysis=True,
        enable_root_cause_analysis=True,
        include_executive_summary=True,
        include_action_items=True
    )
    
    # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆä½œæˆ
    test_suite = TestSuite(test_suite_config)
    evaluator = Evaluator(evaluator_config)
    contradiction_detector = ContradictionDetector(contradiction_config)
    insight_reporter = InsightReporter(insight_config)
    
    print("âœ… QualityLabã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½œæˆå®Œäº†")
    
    return {
        "test_suite": test_suite,
        "evaluator": evaluator,
        "contradiction_detector": contradiction_detector,
        "insight_reporter": insight_reporter
    }


def create_sample_documents():
    """è©•ä¾¡ç”¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ"""
    
    documents = [
        Document(
            id="rag_analysis",
            content="""
            RAGï¼ˆRetrieval-Augmented Generationï¼‰ã¯é©æ–°çš„ãªæŠ€è¡“ã§ã™ã€‚
            ã“ã®æŠ€è¡“ã«ã‚ˆã‚Šã€LLMã®çŸ¥è­˜åˆ¶é™ã‚’å…‹æœã§ãã¾ã™ã€‚
            RAGã‚·ã‚¹ãƒ†ãƒ ã¯é«˜ã„ç²¾åº¦ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
            ã—ã‹ã—ã€RAGã‚·ã‚¹ãƒ†ãƒ ã®ç²¾åº¦ã¯ä½ã„ã¨ã„ã†å ±å‘Šã‚‚ã‚ã‚Šã¾ã™ã€‚
            å®Ÿè£…ã‚³ã‚¹ãƒˆã¯æ¯”è¼ƒçš„ä½ãæŠ‘ãˆã‚‰ã‚Œã¾ã™ã€‚
            è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦BLEUã€ROUGEã€BERTScoreãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
            """,
            metadata={"title": "RAGåˆ†æ", "category": "æŠ€è¡“è§£èª¬"}
        ),
        
        Document(
            id="evaluation_results",
            content="""
            # ãƒ†ã‚¹ãƒˆçµæœ
            
            ## âœ… PASS test_rag_basic
            **Query**: RAGã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ
            **Confidence**: 0.85
            **Processing Time**: 1.2s
            **Sources Found**: 2
            
            ## âŒ FAIL test_complex_query
            **Query**: è¤‡é›‘ãªæŠ€è¡“çš„å•é¡Œã«ã¤ã„ã¦
            **Confidence**: 0.3
            **Processing Time**: 4.5s
            **Sources Found**: 0
            
            ## âœ… PASS test_simple_fact
            **Query**: åŸºæœ¬çš„ãªäº‹å®Ÿã«ã¤ã„ã¦
            **Confidence**: 0.9
            **Processing Time**: 0.8s
            **Sources Found**: 3
            """,
            metadata={
                "processing_stage": "test_execution",
                "tests_run": 3,
                "tests_passed": 2,
                "success_rate": 0.67,
                "source_document_id": "rag_analysis"
            }
        ),
        
        Document(
            id="system_metrics",
            content="""
            # ã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ
            
            ## ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            - **ç²¾åº¦ (Accuracy)**: 0.75
            - **é©åˆç‡ (Precision)**: 0.82
            - **å†ç¾ç‡ (Recall)**: 0.68
            - **F1ã‚¹ã‚³ã‚¢**: 0.744
            - **å¹³å‡ä¿¡é ¼åº¦**: 0.72
            - **å¹³å‡å¿œç­”æ™‚é–“**: 2.1ç§’
            - **ã‚½ãƒ¼ã‚¹ç²¾åº¦**: 0.8
            - **ä¸€è²«æ€§**: 0.65
            """,
            metadata={
                "processing_stage": "evaluation",
                "overall_score": 0.75,
                "source_document_id": "evaluation_results"
            }
        )
    ]
    
    return documents


def demo_quality_lab_workflow():
    """QualityLabãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ‡ãƒ¢"""
    
    print("\nğŸš€ QualityLabãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ‡ãƒ¢")
    print("="*60)
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
    components = create_quality_lab_pipeline()
    documents = create_sample_documents()
    
    print(f"ğŸ“„ {len(documents)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ã—ã¾ã™")
    
    # å‡¦ç†çµæœã‚’ä¿å­˜
    all_results = []
    
    # 1. ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆãƒ»å®Ÿè¡Œ
    print("\n1ï¸âƒ£ TestSuite: ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆãƒ»å®Ÿè¡Œ")
    test_suite = components["test_suite"]
    
    for doc in documents[:1]:  # æœ€åˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã¿ãƒ†ã‚¹ãƒˆç”Ÿæˆ
        results = test_suite.process(doc)
        all_results.extend(results)
        
        result_doc = results[0]
        print(f"   ğŸ“‹ {doc.id}: {result_doc.metadata['generated_cases_count']}ä»¶ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆ")
    
    # 2. çŸ›ç›¾æ¤œå‡º
    print("\n2ï¸âƒ£ ContradictionDetector: çŸ›ç›¾æ¤œå‡º")
    contradiction_detector = components["contradiction_detector"]
    
    for doc in documents:
        results = contradiction_detector.process(doc)
        all_results.extend(results)
        
        result_doc = results[0]
        claims_extracted = result_doc.metadata['claims_extracted']
        contradictions_found = result_doc.metadata['contradictions_found']
        print(f"   ğŸ” {doc.id}: {claims_extracted}ã‚¯ãƒ¬ãƒ¼ãƒ , {contradictions_found}çŸ›ç›¾")
    
    # 3. è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
    print("\n3ï¸âƒ£ Evaluator: ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—")
    evaluator = components["evaluator"]
    
    evaluation_docs = [doc for doc in documents if doc.metadata.get("processing_stage") in ["test_execution", "evaluation"]]
    
    for doc in evaluation_docs:
        results = evaluator.process(doc)
        all_results.extend(results)
        
        result_doc = results[0]
        metrics_computed = result_doc.metadata['metrics_computed']
        overall_score = result_doc.metadata['overall_score']
        print(f"   ğŸ“Š {doc.id}: {metrics_computed}ãƒ¡ãƒˆãƒªã‚¯ã‚¹, ã‚¹ã‚³ã‚¢{overall_score:.2f}")
    
    # 4. ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ
    print("\n4ï¸âƒ£ InsightReporter: ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ")
    insight_reporter = components["insight_reporter"]
    
    insight_docs = [doc for doc in all_results if doc.metadata.get("processing_stage") == "evaluation"]
    
    for doc in insight_docs:
        results = insight_reporter.process(doc)
        all_results.extend(results)
        
        result_doc = results[0]
        insights_generated = result_doc.metadata['insights_generated']
        critical_insights = result_doc.metadata['critical_insights']
        health_score = result_doc.metadata['overall_health_score']
        print(f"   ğŸ’¡ {doc.id}: {insights_generated}ã‚¤ãƒ³ã‚µã‚¤ãƒˆ({critical_insights}é‡è¦), ãƒ˜ãƒ«ã‚¹{health_score:.2f}")
    
    return all_results, components


def demo_integrated_pipeline():
    """çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ‡ãƒ¢"""
    
    print("\nğŸ”— çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¢")
    print("="*60)
    
    # å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å«ã‚€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
    components = create_quality_lab_pipeline()
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ï¼ˆçŸ›ç›¾æ¤œå‡ºâ†’è©•ä¾¡â†’ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆï¼‰
    quality_pipeline = DocumentPipeline([
        components["contradiction_detector"],
        components["evaluator"], 
        components["insight_reporter"]
    ])
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_document = Document(
        id="integrated_test",
        content="""
        # çµ±åˆãƒ†ã‚¹ãƒˆçµæœ
        
        ## ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        - **ç²¾åº¦**: 0.65
        - **å¿œç­”æ™‚é–“**: 3.2ç§’  
        - **ä¿¡é ¼åº¦**: 0.45
        - **F1ã‚¹ã‚³ã‚¢**: 0.612
        
        ## ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼
        - ç·ãƒ†ã‚¹ãƒˆæ•°: 10
        - æˆåŠŸæ•°: 6
        - å¤±æ•—æ•°: 4
        - æˆåŠŸç‡: 60%
        """,
        metadata={
            "processing_stage": "evaluation",
            "overall_score": 0.65,
            "tests_run": 10,
            "tests_passed": 6,
            "success_rate": 0.6
        }
    )
    
    print("ğŸ“‹ çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œä¸­...")
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    final_results = quality_pipeline.process(test_document)
    
    print(f"âœ… ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†: {len(final_results)}ä»¶ã®çµæœç”Ÿæˆ")
    
    # æœ€çµ‚çµæœã®è¡¨ç¤º
    for result in final_results:
        stage = result.metadata.get("processing_stage", "unknown")
        print(f"\nğŸ“„ çµæœãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {result.id} ({stage})")
        
        if stage == "insight_reporting":
            # ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ¬ãƒãƒ¼ãƒˆã®è¦ç´„ã‚’è¡¨ç¤º
            insights_count = result.metadata.get("insights_generated", 0)
            critical_count = result.metadata.get("critical_insights", 0)
            health_score = result.metadata.get("overall_health_score", 0)
            
            print(f"   ğŸ’¡ ã‚¤ãƒ³ã‚µã‚¤ãƒˆ: {insights_count}ä»¶ (ç·Šæ€¥: {critical_count}ä»¶)")
            print(f"   ğŸ¥ ãƒ˜ãƒ«ã‚¹ã‚¹ã‚³ã‚¢: {health_score:.2f}")
            
            # ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã®ä¸€éƒ¨ã‚’è¡¨ç¤º
            content_lines = result.content.split('\n')[:15]
            print(f"   ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆæŠœç²‹:")
            for line in content_lines:
                if line.strip():
                    print(f"      {line}")
            if len(result.content.split('\n')) > 15:
                print(f"      ... (ç¶šãã‚ã‚Š)")
    
    return final_results


def demo_quality_lab_summary():
    """QualityLabã‚µãƒãƒªãƒ¼ã®ãƒ‡ãƒ¢"""
    
    print("\nğŸ“‹ QualityLabã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¢")
    print("="*60)
    
    # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ
    results, components = demo_quality_lab_workflow()
    
    # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
    print("\nğŸ“Š ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ¥ã‚µãƒãƒªãƒ¼:")
    
    # TestSuiteã‚µãƒãƒªãƒ¼
    test_summary = components["test_suite"].get_test_summary()
    print(f"\nğŸ§ª TestSuite:")
    if "total_tests" in test_summary:
        print(f"   - ç·ãƒ†ã‚¹ãƒˆæ•°: {test_summary['total_tests']}")
        print(f"   - æˆåŠŸç‡: {test_summary['success_rate']:.1%}")
        print(f"   - å¹³å‡ä¿¡é ¼åº¦: {test_summary['average_confidence']:.3f}")
    else:
        print(f"   - {test_summary.get('message', 'ãƒ‡ãƒ¼ã‚¿ãªã—')}")
    
    # Evaluatorã‚µãƒãƒªãƒ¼  
    eval_summary = components["evaluator"].get_summary_metrics()
    print(f"\nğŸ“ˆ Evaluator:")
    if eval_summary:
        print(f"   - ç·åˆã‚¹ã‚³ã‚¢: {eval_summary.get('overall_score', 0):.2f}")
        print(f"   - ç²¾åº¦: {eval_summary.get('accuracy', 0):.1%}")
        print(f"   - F1ã‚¹ã‚³ã‚¢: {eval_summary.get('f1_score', 0):.3f}")
    else:
        print("   - ãƒ‡ãƒ¼ã‚¿ãªã—")
    
    # ContradictionDetectorã‚µãƒãƒªãƒ¼
    contradiction_summary = components["contradiction_detector"].get_contradiction_summary()
    print(f"\nğŸ” ContradictionDetector:")
    print(f"   - ç·çŸ›ç›¾æ•°: {contradiction_summary['total_contradictions']}")
    print(f"   - ç·ã‚¯ãƒ¬ãƒ¼ãƒ æ•°: {contradiction_summary['total_claims']}")
    print(f"   - ä¸€è²«æ€§ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {contradiction_summary['consistency_status']}")
    
    # InsightReporterã‚µãƒãƒªãƒ¼
    insight_summary = components["insight_reporter"].get_insight_summary()
    print(f"\nğŸ’¡ InsightReporter:")
    if "total_insights" in insight_summary:
        print(f"   - ç·ã‚¤ãƒ³ã‚µã‚¤ãƒˆæ•°: {insight_summary['total_insights']}")
        print(f"   - å¹³å‡ä¿¡é ¼åº¦: {insight_summary['average_confidence']:.3f}")
        print(f"   - æ¨å¥¨äº‹é …æ•°: {insight_summary['recommendations_count']}")
    else:
        print(f"   - {insight_summary.get('message', 'ãƒ‡ãƒ¼ã‚¿ãªã—')}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    print("ğŸš€ QualityLabçµ±åˆãƒ‡ãƒ¢")
    print("RAGã‚·ã‚¹ãƒ†ãƒ ã®åŒ…æ‹¬çš„ãªå“è³ªè©•ä¾¡ã‚’å®Ÿè¡Œã—ã¾ã™")
    
    try:
        # 1. åŸºæœ¬ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
        demo_quality_lab_workflow()
        
        # 2. çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        demo_integrated_pipeline()
        
        # 3. ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        demo_quality_lab_summary()
        
        print("\nğŸ‰ QualityLabçµ±åˆãƒ‡ãƒ¢ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("\nğŸ“š QualityLabã®ä¸»ãªæ©Ÿèƒ½:")
        print("   âœ… TestSuite: è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ç”Ÿæˆãƒ»å®Ÿè¡Œ")
        print("   âœ… Evaluator: åŒ…æ‹¬çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ãƒ»åˆ†æ")
        print("   âœ… ContradictionDetector: ã‚¯ãƒ¬ãƒ¼ãƒ æŠ½å‡ºãƒ»çŸ›ç›¾æ¤œå‡º")
        print("   âœ… InsightReporter: é–¾å€¤ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ»æ¨å¥¨äº‹é …ç”Ÿæˆ")
        print("   âœ… çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®é€£æºå‹•ä½œ")
        
        print("\nğŸ¯ QualityLabã«ã‚ˆã‚Šå®Ÿç¾ã§ãã‚‹ã“ã¨:")
        print("   ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ã®å®šé‡çš„è©•ä¾¡")
        print("   ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ»ä¸€è²«æ€§ã®æ¤œè¨¼")
        print("   ğŸ’¡ æ”¹å–„é ˜åŸŸã®ç‰¹å®šã¨å„ªå…ˆé †ä½ä»˜ã‘")
        print("   ğŸ“‹ ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼å‘ã‘ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        print("   ğŸš€ ç¶™ç¶šçš„å“è³ªæ”¹å–„ã®æ”¯æ´")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)