#!/usr/bin/env python3
"""
Part 3: RAG Evaluation Tutorial Example
RAGè©•ä¾¡ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ä¾‹

This example demonstrates comprehensive RAG system evaluation using refinire-rag's QualityLab
with automated QA generation, performance evaluation, contradiction detection, and reporting.

ã“ã®ä¾‹ã§ã¯ã€refinire-ragã®QualityLabã‚’ä½¿ç”¨ã—ãŸåŒ…æ‹¬çš„ãªRAGã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡ã‚’ã€
è‡ªå‹•QAç”Ÿæˆã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã€çŸ›ç›¾æ¤œå‡ºã€ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã¨ã¨ã‚‚ã«å®Ÿæ¼”ã—ã¾ã™ã€‚
"""

import sys
import tempfile
import time
import json
from pathlib import Path
from typing import List, Dict, Any

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from refinire_rag.application.corpus_manager_new import CorpusManager
from refinire_rag.application.query_engine import QueryEngine, QueryEngineConfig
from refinire_rag.application.quality_lab import QualityLab, QualityLabConfig
from refinire_rag.storage.sqlite_store import SQLiteDocumentStore
from refinire_rag.storage.in_memory_vector_store import InMemoryVectorStore
from refinire_rag.retrieval import SimpleRetriever, SimpleReranker, SimpleReader
from refinire_rag.models.document import Document
from refinire_rag.models.qa_pair import QAPair
from refinire_rag.models.evaluation_result import EvaluationResult


def setup_evaluation_corpus(temp_dir: Path) -> tuple:
    """
    Set up a comprehensive corpus for evaluation demonstration
    è©•ä¾¡ãƒ‡ãƒ¢ç”¨ã®åŒ…æ‹¬çš„ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    """
    
    print("ğŸ“š Setting up evaluation corpus...")
    print("ğŸ“š è©•ä¾¡ç”¨ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
    
    # Create knowledge base directory / çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    kb_dir = temp_dir / "evaluation_kb"
    kb_dir.mkdir(exist_ok=True)
    
    # AI Overview / AIæ¦‚è¦
    (kb_dir / "ai_overview.txt").write_text("""
äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰æ¦‚è¦

äººå·¥çŸ¥èƒ½ï¼ˆArtificial Intelligence, AIï¼‰ã¯ã€äººé–“ã®çŸ¥çš„æ´»å‹•ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§æ¨¡å€£ãƒ»å®Ÿç¾ã™ã‚‹æŠ€è¡“åˆ†é‡ã§ã™ã€‚

## ä¸»è¦åˆ†é‡
1. æ©Ÿæ¢°å­¦ç¿’ï¼ˆMachine Learning, MLï¼‰
   - ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«å­¦ç¿’ã—ã€äºˆæ¸¬ã‚„åˆ¤æ–­ã‚’è¡Œã†æŠ€è¡“
   - ä»£è¡¨çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼šç·šå½¢å›å¸°ã€æ±ºå®šæœ¨ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

2. æ·±å±¤å­¦ç¿’ï¼ˆDeep Learning, DLï¼‰
   - å¤šå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•
   - CNNï¼ˆç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ã€RNNï¼ˆå†å¸°ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰

3. è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNatural Language Processing, NLPï¼‰
   - äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ç†è§£ãƒ»ç”Ÿæˆã™ã‚‹æŠ€è¡“
   - æ©Ÿæ¢°ç¿»è¨³ã€æ„Ÿæƒ…åˆ†æã€è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ 

4. ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ï¼ˆComputer Vision, CVï¼‰
   - ç”»åƒã‚„å‹•ç”»ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºãƒ»ç†è§£ã™ã‚‹æŠ€è¡“
   - ç‰©ä½“æ¤œå‡ºã€é¡”èªè­˜ã€åŒ»ç™‚ç”»åƒè§£æ

## AIã®æ­´å²
- 1950å¹´ä»£ï¼šã‚¢ãƒ©ãƒ³ãƒ»ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆã®ææ¡ˆ
- 1960å¹´ä»£ï¼šã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®é–‹ç™º
- 1980å¹´ä»£ï¼šãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¾©æ´»
- 2010å¹´ä»£ï¼šæ·±å±¤å­¦ç¿’ã®å°é ­ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç™»å ´

## ç¾åœ¨ã®å¿œç”¨åˆ†é‡
- è‡ªå‹•é‹è»¢è»Š
- åŒ»ç™‚è¨ºæ–­æ”¯æ´
- é‡‘èå–å¼•ã®è‡ªå‹•åŒ–
- éŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼ˆSiri, Alexaç­‰ï¼‰
- æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆNetflix, Amazonç­‰ï¼‰

AIã¯æ€¥é€Ÿã«ç™ºå±•ã—ç¶šã‘ã€ä»Šå¾Œã‚‚äººé–“ç¤¾ä¼šã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã¨äºˆæƒ³ã•ã‚Œã¦ã„ã¾ã™ã€‚
""", encoding='utf-8')
    
    # Machine Learning Details / æ©Ÿæ¢°å­¦ç¿’è©³ç´°
    (kb_dir / "machine_learning.txt").write_text("""
æ©Ÿæ¢°å­¦ç¿’è©³ç´°è§£èª¬

æ©Ÿæ¢°å­¦ç¿’ã¯ã€æ˜ç¤ºçš„ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã•ã‚Œã‚‹ã“ã¨ãªãã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã™ã‚‹èƒ½åŠ›ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ä¸ãˆã‚‹æŠ€è¡“ã§ã™ã€‚

## å­¦ç¿’æ–¹å¼ã®åˆ†é¡

### 1. æ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆSupervised Learningï¼‰
æ­£è§£ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ©ãƒ™ãƒ«ï¼‰ã‚’ç”¨ã„ã¦å­¦ç¿’ã™ã‚‹æ–¹å¼ã€‚

#### åˆ†é¡ï¼ˆClassificationï¼‰
- ç›®çš„ï¼šãƒ‡ãƒ¼ã‚¿ã‚’äºˆã‚å®šç¾©ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡
- ä¾‹ï¼šã‚¹ãƒ‘ãƒ ãƒ¡ãƒ¼ãƒ«æ¤œå‡ºã€ç”»åƒèªè­˜ã€åŒ»ç™‚è¨ºæ–­
- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
  * ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
  * ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ï¼ˆSVMï¼‰
  * ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ
  * å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°

#### å›å¸°ï¼ˆRegressionï¼‰
- ç›®çš„ï¼šé€£ç¶šå€¤ã®äºˆæ¸¬
- ä¾‹ï¼šæ ªä¾¡äºˆæ¸¬ã€æ°—æ¸©äºˆæ¸¬ã€å£²ä¸Šäºˆæ¸¬
- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
  * ç·šå½¢å›å¸°
  * å¤šé …å¼å›å¸°
  * ãƒªãƒƒã‚¸å›å¸°ã€ãƒ©ãƒƒã‚½å›å¸°

### 2. æ•™å¸«ãªã—å­¦ç¿’ï¼ˆUnsupervised Learningï¼‰
æ­£è§£ãƒ‡ãƒ¼ã‚¿ãªã—ã§ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹ã€‚

#### ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
- ç›®çš„ï¼šé¡ä¼¼ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
- ä¾‹ï¼šé¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€éºä¼å­åˆ†æ
- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
  * K-means
  * éšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
  * DBSCAN

#### æ¬¡å…ƒå‰Šæ¸›
- ç›®çš„ï¼šãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒã‚’æ¸›ã‚‰ã—ã¤ã¤é‡è¦ãªæƒ…å ±ã‚’ä¿æŒ
- ä¾‹ï¼šãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ã€ç‰¹å¾´é¸æŠ
- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
  * ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰
  * t-SNE
  * UMAP

### 3. å¼·åŒ–å­¦ç¿’ï¼ˆReinforcement Learningï¼‰
ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’ã€‚

- æ§‹æˆè¦ç´ ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ç’°å¢ƒã€è¡Œå‹•ã€å ±é…¬
- ä¾‹ï¼šã‚²ãƒ¼ãƒ AIã€ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ã€è‡ªå‹•å–å¼•
- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
  * Qå­¦ç¿’
  * Deep Q Network (DQN)
  * Policy Gradient

## æ©Ÿæ¢°å­¦ç¿’ã®ãƒ—ãƒ­ã‚»ã‚¹

1. ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»å‰å‡¦ç†
2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
3. ãƒ¢ãƒ‡ãƒ«é¸æŠ
4. å­¦ç¿’ãƒ»è¨“ç·´
5. è©•ä¾¡ãƒ»æ¤œè¨¼
6. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
7. æœ¬ç•ªé‹ç”¨

## è©•ä¾¡æŒ‡æ¨™

### åˆ†é¡å•é¡Œ
- ç²¾åº¦ï¼ˆAccuracyï¼‰
- é©åˆç‡ï¼ˆPrecisionï¼‰
- å†ç¾ç‡ï¼ˆRecallï¼‰
- F1ã‚¹ã‚³ã‚¢
- AUC-ROC

### å›å¸°å•é¡Œ
- å¹³å‡çµ¶å¯¾èª¤å·®ï¼ˆMAEï¼‰
- å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSEï¼‰
- æ±ºå®šä¿‚æ•°ï¼ˆRÂ²ï¼‰

æ©Ÿæ¢°å­¦ç¿’ã¯ç¾ä»£ã®AIã®åŸºç›¤æŠ€è¡“ã¨ã—ã¦ã€æ§˜ã€…ãªåˆ†é‡ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
""", encoding='utf-8')
    
    # Deep Learning / æ·±å±¤å­¦ç¿’
    (kb_dir / "deep_learning.txt").write_text("""
æ·±å±¤å­¦ç¿’æŠ€è¡“è§£èª¬

æ·±å±¤å­¦ç¿’ï¼ˆDeep Learningï¼‰ã¯ã€äººé–“ã®è„³ã®ç¥çµŒå›è·¯ã‚’æ¨¡å€£ã—ãŸå¤šå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ã‚‹æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã§ã™ã€‚

## ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åŸºç¤

### åŸºæœ¬æ§‹é€ 
- ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ï¼ˆãƒãƒ¼ãƒ‰ï¼‰ï¼šæƒ…å ±å‡¦ç†ã®åŸºæœ¬å˜ä½
- é‡ã¿ï¼ˆWeightï¼‰ï¼šãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é–“ã®æ¥ç¶šå¼·åº¦
- ãƒã‚¤ã‚¢ã‚¹ï¼ˆBiasï¼‰ï¼šæ´»æ€§åŒ–ã®é–¾å€¤èª¿æ•´
- æ´»æ€§åŒ–é–¢æ•°ï¼šéç·šå½¢å¤‰æ›ã‚’æä¾›

### å­¦ç¿’ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 
1. é †ä¼æ’­ï¼ˆForward Propagationï¼‰ï¼šå…¥åŠ›ã‹ã‚‰å‡ºåŠ›ã¸ã®æƒ…å ±ä¼é”
2. æå¤±è¨ˆç®—ï¼šäºˆæ¸¬å€¤ã¨å®Ÿéš›å€¤ã®å·®ã‚’è¨ˆç®—
3. é€†ä¼æ’­ï¼ˆBackpropagationï¼‰ï¼šèª¤å·®ã‚’é€†å‘ãã«ä¼æ’­
4. å‹¾é…é™ä¸‹æ³•ï¼šé‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã‚’æ›´æ–°

## ä¸»è¦ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### 1. ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆCNNï¼‰
ç”»åƒå‡¦ç†ã«ç‰¹åŒ–ã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚

#### æ§‹æˆè¦ç´ 
- ç•³ã¿è¾¼ã¿å±¤ï¼ˆConvolution Layerï¼‰ï¼šç‰¹å¾´æŠ½å‡º
- ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ï¼ˆPooling Layerï¼‰ï¼šãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
- å…¨çµåˆå±¤ï¼ˆFully Connected Layerï¼‰ï¼šåˆ†é¡

#### å¿œç”¨
- ç”»åƒåˆ†é¡ï¼šImageNetã€CIFAR-10
- ç‰©ä½“æ¤œå‡ºï¼šYOLOã€R-CNN
- åŒ»ç™‚ç”»åƒï¼šXç·šã€MRIè§£æ
- è‡ªå‹•é‹è»¢ï¼šé“è·¯æ¨™è­˜èªè­˜

### 2. å†å¸°ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆRNNï¼‰
æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«é©ã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚

#### æ”¹è‰¯ç‰ˆ
- LSTMï¼ˆLong Short-Term Memoryï¼‰ï¼šé•·æœŸä¾å­˜é–¢ä¿‚ã®å­¦ç¿’
- GRUï¼ˆGated Recurrent Unitï¼‰ï¼šLSTMã®ç°¡ç´ åŒ–ç‰ˆ

#### å¿œç”¨
- è‡ªç„¶è¨€èªå‡¦ç†ï¼šæ©Ÿæ¢°ç¿»è¨³ã€æ–‡ç« ç”Ÿæˆ
- éŸ³å£°èªè­˜ï¼šéŸ³å£°ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã¸ã®å¤‰æ›
- æ™‚ç³»åˆ—äºˆæ¸¬ï¼šæ ªä¾¡ã€æ°—è±¡äºˆæ¸¬

### 3. ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆTransformerï¼‰
æ³¨æ„æ©Ÿæ§‹ï¼ˆAttention Mechanismï¼‰ã‚’æ ¸ã¨ã—ãŸé©æ–°çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚

#### ç‰¹å¾´
- Self-Attentionï¼šæ–‡è„ˆå†…ã®é–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‹
- ä¸¦åˆ—å‡¦ç†ï¼šRNNã‚ˆã‚Šé«˜é€Ÿãªå­¦ç¿’
- é•·è·é›¢ä¾å­˜é–¢ä¿‚ï¼šã‚ˆã‚Šé•·ã„æ–‡è„ˆã‚’ç†è§£

#### ä»£è¡¨ãƒ¢ãƒ‡ãƒ«
- BERTï¼šåŒæ–¹å‘ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
- GPTï¼šç”Ÿæˆå‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼
- T5ï¼šText-to-Text Transfer Transformer

## æ·±å±¤å­¦ç¿’ã®ç™ºå±•

### ç”Ÿæˆãƒ¢ãƒ‡ãƒ«
- GANï¼ˆGenerative Adversarial Networksï¼‰ï¼šç”»åƒç”Ÿæˆ
- VAEï¼ˆVariational Autoencoderï¼‰ï¼šãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- Diffusion Modelsï¼šé«˜å“è³ªç”»åƒç”Ÿæˆ

### å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«
- GPT-3/4ï¼š1750å„„ã€œ1å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- PaLMï¼š5400å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- ChatGPTï¼šå¯¾è©±å‹AI

## æ·±å±¤å­¦ç¿’ã®èª²é¡Œ

### æŠ€è¡“çš„èª²é¡Œ
- å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã¨è¨ˆç®—è³‡æºãŒå¿…è¦
- ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹å•é¡Œï¼šè§£é‡ˆå¯èƒ½æ€§ã®æ¬ å¦‚
- éå­¦ç¿’ï¼šæ±åŒ–æ€§èƒ½ã®ä½ä¸‹
- æ•µå¯¾çš„æ”»æ’ƒï¼šå¾®å°ãªå¤‰æ›´ã«ã‚ˆã‚‹èª¤åˆ†é¡

### ç¤¾ä¼šçš„èª²é¡Œ
- ãƒã‚¤ã‚¢ã‚¹ï¼šå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®åã‚Šã«ã‚ˆã‚‹å·®åˆ¥
- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ï¼šå€‹äººæƒ…å ±ã®ä¿è­·
- é›‡ç”¨ã¸ã®å½±éŸ¿ï¼šè‡ªå‹•åŒ–ã«ã‚ˆã‚‹è·æ¥­ã®å¤‰åŒ–
- ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¶ˆè²»ï¼šå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ç’°å¢ƒè² è·

æ·±å±¤å­¦ç¿’ã¯ç¾åœ¨ã®AIãƒ–ãƒ¼ãƒ ã‚’ç‰½å¼•ã™ã‚‹ä¸­æ ¸æŠ€è¡“ã¨ã—ã¦ã€æ€¥é€Ÿãªç™ºå±•ã‚’ç¶šã‘ã¦ã„ã¾ã™ã€‚
""", encoding='utf-8')
    
    # NLP Applications / NLPå¿œç”¨
    (kb_dir / "nlp_applications.txt").write_text("""
è‡ªç„¶è¨€èªå‡¦ç†ã®å¿œç”¨

è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰ã¯ã€äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ç†è§£ãƒ»ç”Ÿæˆãƒ»æ“ä½œã™ã‚‹æŠ€è¡“åˆ†é‡ã§ã™ã€‚

## åŸºæœ¬çš„ãªNLPã‚¿ã‚¹ã‚¯

### 1. å‰å‡¦ç†ï¼ˆPreprocessingï¼‰
- ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆTokenizationï¼‰ï¼šæ–‡ç« ã‚’å˜èªã‚„æ–‡ã«åˆ†å‰²
- æ­£è¦åŒ–ï¼ˆNormalizationï¼‰ï¼šè¡¨è¨˜ã®çµ±ä¸€
- ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»ï¼šæ„å‘³ã®è–„ã„èªã®å‰Šé™¤
- ã‚¹ãƒ†ãƒŸãƒ³ã‚°ãƒ»ãƒ¬ãƒ³ãƒåŒ–ï¼šèªå½¢ã®æ­£è¦åŒ–

### 2. å½¢æ…‹ç´ è§£æ
- å“è©ã‚¿ã‚°ä»˜ã‘ï¼ˆPOS Taggingï¼‰ï¼šå˜èªã®å“è©ã‚’è­˜åˆ¥
- å›ºæœ‰è¡¨ç¾èªè­˜ï¼ˆNERï¼‰ï¼šäººåã€åœ°åã€çµ„ç¹”åç­‰ã®æŠ½å‡º
- æ§‹æ–‡è§£æï¼ˆParsingï¼‰ï¼šæ–‡ã®æ–‡æ³•æ§‹é€ ã‚’åˆ†æ

## ä¸»è¦ãªNLPã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

### 1. æ©Ÿæ¢°ç¿»è¨³ï¼ˆMachine Translationï¼‰
#### ç™ºå±•ã®æ­´å²
- è¦å‰‡ãƒ™ãƒ¼ã‚¹ç¿»è¨³ï¼ˆ1950å¹´ä»£ã€œï¼‰
- çµ±è¨ˆçš„æ©Ÿæ¢°ç¿»è¨³ï¼ˆ1990å¹´ä»£ã€œï¼‰
- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ©Ÿæ¢°ç¿»è¨³ï¼ˆ2010å¹´ä»£ã€œï¼‰

#### ç¾ä»£ã®æ‰‹æ³•
- Transformer Basedï¼šGoogle Translate, DeepL
- å¤šè¨€èªãƒ¢ãƒ‡ãƒ«ï¼šmBERT, XLM-R
- ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç¿»è¨³ï¼šç›´æ¥ãƒšã‚¢ãªã—ã§ç¿»è¨³

#### è©•ä¾¡æŒ‡æ¨™
- BLEU Scoreï¼šn-gramãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦
- ROUGE Scoreï¼šè¦ç´„å“è³ªè©•ä¾¡
- äººæ‰‹è©•ä¾¡ï¼šæµæš¢ã•ã€æ­£ç¢ºæ€§

### 2. æ„Ÿæƒ…åˆ†æï¼ˆSentiment Analysisï¼‰
#### åˆ†æãƒ¬ãƒ™ãƒ«
- æ–‡æ›¸ãƒ¬ãƒ™ãƒ«ï¼šæ–‡æ›¸å…¨ä½“ã®æ„Ÿæƒ…
- æ–‡ãƒ¬ãƒ™ãƒ«ï¼šå„æ–‡ã®æ„Ÿæƒ…
- ã‚¢ã‚¹ãƒšã‚¯ãƒˆãƒ¬ãƒ™ãƒ«ï¼šç‰¹å®šè¦³ç‚¹ã®æ„Ÿæƒ…

#### å¿œç”¨åˆ†é‡
- ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢åˆ†æ
- è£½å“ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æ
- é¡§å®¢æº€è¶³åº¦èª¿æŸ»
- æ ªå¼å¸‚å ´äºˆæ¸¬

### 3. è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ï¼ˆQuestion Answeringï¼‰
#### ã‚·ã‚¹ãƒ†ãƒ åˆ†é¡
- æŠ½å‡ºå‹QAï¼šæ–‡æ›¸ã‹ã‚‰è©²å½“ç®‡æ‰€ã‚’æŠ½å‡º
- ç”Ÿæˆå‹QAï¼šå›ç­”ã‚’ç”Ÿæˆ
- çŸ¥è­˜ãƒ™ãƒ¼ã‚¹QAï¼šæ§‹é€ åŒ–çŸ¥è­˜ã‚’æ´»ç”¨

#### ä»£è¡¨çš„ã‚·ã‚¹ãƒ†ãƒ 
- SQuADï¼šèª­è§£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
- Natural Questionsï¼šå®Ÿä¸–ç•Œã®è³ªå•
- MS MARCOï¼šå¤§è¦æ¨¡QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

### 4. æ–‡æ›¸è¦ç´„ï¼ˆText Summarizationï¼‰
#### è¦ç´„æ‰‹æ³•
- æŠ½å‡ºå‹ï¼šé‡è¦æ–‡ã‚’é¸æŠ
- ç”Ÿæˆå‹ï¼šæ–°ã—ã„æ–‡ã‚’ç”Ÿæˆ
- ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼šä¸¡è€…ã®çµ„ã¿åˆã‚ã›

#### è¦ç´„ã®ç¨®é¡
- å˜ä¸€æ–‡æ›¸è¦ç´„
- è¤‡æ•°æ–‡æ›¸è¦ç´„
- æ›´æ–°è¦ç´„ï¼šæ–°æƒ…å ±ã®è¿½åŠ 

### 5. å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ï¼ˆDialogue Systemsï¼‰
#### ã‚·ã‚¹ãƒ†ãƒ åˆ†é¡
- ã‚¿ã‚¹ã‚¯æŒ‡å‘ï¼šç‰¹å®šæ¥­å‹™ã®é‚è¡Œ
- é›‘è«‡å‹ï¼šè‡ªç„¶ãªä¼šè©±ã‚’ç›®æŒ‡ã™
- è³ªå•å¿œç­”å‹ï¼šæƒ…å ±æä¾›ã«ç‰¹åŒ–

#### æŠ€è¡“è¦ç´ 
- è‡ªç„¶è¨€èªç†è§£ï¼ˆNLUï¼‰
- å¯¾è©±ç®¡ç†ï¼ˆDMï¼‰
- è‡ªç„¶è¨€èªç”Ÿæˆï¼ˆNLGï¼‰

## æœ€æ–°ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«

### GPTã‚·ãƒªãƒ¼ã‚º
- GPT-1ï¼ˆ2018ï¼‰ï¼š1.17å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- GPT-2ï¼ˆ2019ï¼‰ï¼š15å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- GPT-3ï¼ˆ2020ï¼‰ï¼š1750å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- GPT-4ï¼ˆ2023ï¼‰ï¼šæ¨å®š1å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

### BERTã¨ãã®ç™ºå±•
- BERTï¼ˆ2018ï¼‰ï¼šåŒæ–¹å‘ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
- RoBERTaï¼šBERTã®æ”¹è‰¯ç‰ˆ
- DeBERTaï¼šDisentangled Attention

### æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«
- äº¬éƒ½å¤§å­¦BERT
- æ±åŒ—å¤§å­¦BERT
- rinna GPT
- Japanese T5

## NLPã®è©•ä¾¡ã¨èª²é¡Œ

### è©•ä¾¡æŒ‡æ¨™
- è‡ªå‹•è©•ä¾¡ï¼šBLEU, ROUGE, METEOR
- äººæ‰‹è©•ä¾¡ï¼šæµæš¢ã•ã€æ­£ç¢ºæ€§ã€é©åˆ‡æ€§
- ã‚¿ã‚¹ã‚¯ç‰¹åŒ–æŒ‡æ¨™ï¼šF1, Exact Match

### ç¾åœ¨ã®èª²é¡Œ
- ãƒã‚¤ã‚¢ã‚¹ã¨å…¬å¹³æ€§
- è§£é‡ˆå¯èƒ½æ€§
- å¤šè¨€èªå¯¾å¿œ
- è¨ˆç®—ã‚³ã‚¹ãƒˆã®å‰Šæ¸›
- ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã®å‘ä¸Š

NLPã¯ç¾åœ¨ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç™»å ´ã«ã‚ˆã‚Šé©å‘½çš„ãªé€²æ­©ã‚’é‚ã’ã¦ãŠã‚Šã€
ä»Šå¾Œã‚‚äººé–“ã¨ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®è‡ªç„¶ãªå¯¾è©±ã‚’å®Ÿç¾ã™ã‚‹é‡è¦æŠ€è¡“ã¨ã—ã¦ç™ºå±•ã—ç¶šã‘ã¾ã™ã€‚
""", encoding='utf-8')
    
    # Setup storage and build corpus / ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¦ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
    doc_store = SQLiteDocumentStore(":memory:")
    vector_store = InMemoryVectorStore()
    
    # Build corpus with semantic RAG / ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯RAGã§ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰
    corpus_manager = CorpusManager.create_semantic_rag(doc_store, vector_store)
    file_paths = [str(f) for f in kb_dir.glob("*.txt")]
    stats = corpus_manager.build_corpus(file_paths)
    
    print(f"âœ… Evaluation corpus setup completed:")
    print(f"   Files processed: {stats.total_files_processed}")
    print(f"   Documents created: {stats.total_documents_created}")
    print(f"   Chunks created: {stats.total_chunks_created}")
    print(f"   Processing time: {stats.total_processing_time:.3f}s")
    
    return doc_store, vector_store, file_paths


def setup_query_engine(doc_store, vector_store) -> QueryEngine:
    """
    Set up QueryEngine for evaluation
    è©•ä¾¡ç”¨QueryEngineã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    """
    
    print("\nğŸ” Setting up QueryEngine for evaluation...")
    print("ğŸ” è©•ä¾¡ç”¨QueryEngineã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
    
    # Initialize QueryEngine with optimal settings / æœ€é©è¨­å®šã§QueryEngineã‚’åˆæœŸåŒ–
    query_engine = QueryEngine(
        document_store=doc_store,
        vector_store=vector_store,
        retriever=SimpleRetriever(vector_store),
        reranker=SimpleReranker(),
        reader=SimpleReader(),
        config=QueryEngineConfig(
            enable_query_normalization=True,
            include_sources=True,
            include_confidence=True,
            max_response_time=30.0
        )
    )
    
    print("âœ… QueryEngine setup completed")
    return query_engine


def demonstrate_qa_generation(quality_lab: QualityLab, documents: List[Document]):
    """
    Demonstrate automated QA pair generation
    è‡ªå‹•QAãƒšã‚¢ç”Ÿæˆã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    
    print("\n" + "="*60)
    print("ğŸ“ QA PAIR GENERATION DEMONSTRATION")
    print("ğŸ“ QAãƒšã‚¢ç”Ÿæˆã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("="*60)
    
    # Generate QA pairs with different question types / ç•°ãªã‚‹è³ªå•ã‚¿ã‚¤ãƒ—ã§QAãƒšã‚¢ã‚’ç”Ÿæˆ
    print("\nğŸ“Œ Generating QA pairs with multiple question types...")
    print("ğŸ“Œ è¤‡æ•°ã®è³ªå•ã‚¿ã‚¤ãƒ—ã§QAãƒšã‚¢ã‚’ç”Ÿæˆä¸­...")
    
    qa_pairs = quality_lab.generate_qa_pairs(
        documents=documents,
        num_pairs=15,
        question_types=["factual", "conceptual", "analytical", "comparative"]
    )
    
    print(f"âœ… Generated {len(qa_pairs)} QA pairs")
    
    # Analyze QA pair distribution / QAãƒšã‚¢åˆ†å¸ƒã‚’åˆ†æ
    type_distribution = {}
    difficulty_distribution = {}
    
    for qa_pair in qa_pairs:
        q_type = qa_pair.metadata.get('question_type', 'unknown')
        difficulty = qa_pair.metadata.get('difficulty', 'unknown')
        
        type_distribution[q_type] = type_distribution.get(q_type, 0) + 1
        difficulty_distribution[difficulty] = difficulty_distribution.get(difficulty, 0) + 1
    
    print(f"\nğŸ“Š QA Pair Analysis / QAãƒšã‚¢åˆ†æ:")
    print(f"   Question type distribution / è³ªå•ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ:")
    for q_type, count in type_distribution.items():
        print(f"     {q_type}: {count}")
    
    print(f"   Difficulty distribution / é›£æ˜“åº¦åˆ†å¸ƒ:")
    for difficulty, count in difficulty_distribution.items():
        print(f"     {difficulty}: {count}")
    
    # Show sample QA pairs / ã‚µãƒ³ãƒ—ãƒ«QAãƒšã‚¢ã‚’è¡¨ç¤º
    print(f"\nğŸ“– Sample QA Pairs / ã‚µãƒ³ãƒ—ãƒ«QAãƒšã‚¢:")
    for i, qa_pair in enumerate(qa_pairs[:3]):
        print(f"\n{i+1}. Document: {qa_pair.document_id}")
        print(f"   Type: {qa_pair.metadata.get('question_type', 'N/A')}")
        print(f"   Difficulty: {qa_pair.metadata.get('difficulty', 'N/A')}")
        print(f"   Question: {qa_pair.question}")
        print(f"   Expected Answer: {qa_pair.answer[:150]}...")
    
    return qa_pairs


def demonstrate_performance_evaluation(quality_lab: QualityLab, 
                                     query_engine: QueryEngine, 
                                     qa_pairs: List[QAPair]):
    """
    Demonstrate comprehensive performance evaluation
    åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    
    print("\n" + "="*60)
    print("ğŸ”¬ PERFORMANCE EVALUATION DEMONSTRATION")
    print("ğŸ”¬ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("="*60)
    
    print(f"\nğŸ“Š Evaluating QueryEngine with {len(qa_pairs)} test cases...")
    print(f"ğŸ“Š {len(qa_pairs)}ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§QueryEngineã‚’è©•ä¾¡ä¸­...")
    
    # Run comprehensive evaluation / åŒ…æ‹¬çš„è©•ä¾¡ã‚’å®Ÿè¡Œ
    start_time = time.time()
    evaluation_results = quality_lab.evaluate_query_engine(
        query_engine=query_engine,
        qa_pairs=qa_pairs,
        evaluation_metrics=["bleu", "rouge", "llm_judge"],
        include_contradiction_detection=True,
        detailed_analysis=True
    )
    
    evaluation_time = time.time() - start_time
    
    print(f"âœ… Evaluation completed in {evaluation_time:.2f}s")
    
    # Analyze overall results / å…¨ä½“çµæœã‚’åˆ†æ
    test_results = evaluation_results['test_results']
    passed_tests = sum(1 for result in test_results if result['passed'])
    pass_rate = (passed_tests / len(test_results)) * 100
    
    print(f"\nğŸ“ˆ Overall Results / å…¨ä½“çµæœ:")
    print(f"   Total tests: {len(test_results)}")
    print(f"   Passed tests: {passed_tests}")
    print(f"   Pass rate: {pass_rate:.1f}%")
    print(f"   Average processing time: {evaluation_time/len(test_results):.3f}s per test")
    
    # Performance by question type / è³ªå•ã‚¿ã‚¤ãƒ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    performance_by_type = {}
    for result in test_results:
        q_type = result.get('question_type', 'unknown')
        if q_type not in performance_by_type:
            performance_by_type[q_type] = {'total': 0, 'passed': 0, 'scores': []}
        
        performance_by_type[q_type]['total'] += 1
        if result['passed']:
            performance_by_type[q_type]['passed'] += 1
        performance_by_type[q_type]['scores'].append(result.get('score', 0))
    
    print(f"\nğŸ“Š Performance by Question Type / è³ªå•ã‚¿ã‚¤ãƒ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
    for q_type, stats in performance_by_type.items():
        type_pass_rate = (stats['passed'] / stats['total']) * 100
        avg_score = sum(stats['scores']) / len(stats['scores']) if stats['scores'] else 0
        print(f"   {q_type.capitalize()}:")
        print(f"     Pass rate: {type_pass_rate:.1f}% ({stats['passed']}/{stats['total']})")
        print(f"     Average score: {avg_score:.3f}")
    
    # Metric analysis / ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æ
    if 'metric_summaries' in evaluation_results:
        print(f"\nğŸ¯ Metric Analysis / ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æ:")
        for metric, summary in evaluation_results['metric_summaries'].items():
            print(f"   {metric.upper()}:")
            print(f"     Average: {summary.get('average', 0):.3f}")
            print(f"     Standard deviation: {summary.get('std_dev', 0):.3f}")
            print(f"     Range: {summary.get('min', 0):.3f} - {summary.get('max', 0):.3f}")
    
    # Show sample results / ã‚µãƒ³ãƒ—ãƒ«çµæœã‚’è¡¨ç¤º
    print(f"\nğŸ“ Sample Test Results / ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆçµæœ:")
    for i, result in enumerate(test_results[:3]):
        status = "âœ… PASSED" if result['passed'] else "âŒ FAILED"
        print(f"\n{i+1}. {status}")
        print(f"   Question: {result['query'][:80]}...")
        print(f"   Generated Answer: {result['generated_answer'][:100]}...")
        print(f"   Score: {result.get('score', 0):.3f}")
        print(f"   Confidence: {result.get('confidence', 0):.3f}")
        print(f"   Processing time: {result.get('processing_time', 0):.3f}s")
        
        if not result['passed'] and 'failure_reason' in result:
            print(f"   Failure reason: {result['failure_reason']}")
    
    return evaluation_results


def demonstrate_contradiction_detection(quality_lab: QualityLab, 
                                      documents: List[Document],
                                      query_engine: QueryEngine):
    """
    Demonstrate contradiction detection capabilities
    çŸ›ç›¾æ¤œå‡ºæ©Ÿèƒ½ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    
    print("\n" + "="*60)
    print("ğŸ•µï¸ CONTRADICTION DETECTION DEMONSTRATION")
    print("ğŸ•µï¸ çŸ›ç›¾æ¤œå‡ºã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("="*60)
    
    # Test queries for contradiction detection / çŸ›ç›¾æ¤œå‡ºç”¨ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
    test_queries = [
        "æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "æ·±å±¤å­¦ç¿’ã®ä¸»è¦ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ï¼Ÿ",
        "è‡ªç„¶è¨€èªå‡¦ç†ã®å¿œç”¨åˆ†é‡ã¯ï¼Ÿ",
        "AIã®æ­´å²ã«ã¤ã„ã¦æ•™ãˆã¦",
        "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’æ–¹æ³•ã¯ï¼Ÿ"
    ]
    
    print(f"\nğŸ” Running contradiction detection with {len(test_queries)} queries...")
    print(f"ğŸ” {len(test_queries)}ã‚¯ã‚¨ãƒªã§çŸ›ç›¾æ¤œå‡ºã‚’å®Ÿè¡Œä¸­...")
    
    # Detect contradictions / çŸ›ç›¾ã‚’æ¤œå‡º
    contradiction_results = quality_lab.detect_contradictions(
        corpus_documents=documents,
        query_engine=query_engine,
        test_queries=test_queries
    )
    
    print(f"âœ… Contradiction analysis completed")
    
    # Analyze contradiction results / çŸ›ç›¾çµæœã‚’åˆ†æ
    contradictions = contradiction_results.get('contradictions', [])
    print(f"\nğŸ“Š Contradiction Analysis / çŸ›ç›¾åˆ†æ:")
    print(f"   Documents analyzed: {len(documents)}")
    print(f"   Queries tested: {len(test_queries)}")
    print(f"   Contradictions found: {len(contradictions)}")
    
    if contradictions:
        print(f"\nâš ï¸  Detected Contradictions / æ¤œå‡ºã•ã‚ŒãŸçŸ›ç›¾:")
        for i, contradiction in enumerate(contradictions[:3]):
            print(f"\n{i+1}. Contradiction Type: {contradiction.get('type', 'Unknown')}")
            print(f"   Confidence: {contradiction.get('confidence', 0):.3f}")
            print(f"   Statement 1: {contradiction.get('statement_1', '')[:100]}...")
            print(f"   Statement 2: {contradiction.get('statement_2', '')[:100]}...")
            print(f"   Source documents: {contradiction.get('source_documents', [])}")
    else:
        print(f"\nâœ… No contradictions detected in the corpus")
    
    # Consistency check / ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
    print(f"\nğŸ”„ Running consistency check...")
    print(f"ğŸ”„ ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œä¸­...")
    
    similar_query_groups = [
        ["æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ", "MLã®å®šç¾©ã‚’æ•™ãˆã¦", "æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦èª¬æ˜ã—ã¦"],
        ["æ·±å±¤å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ", "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®èª¬æ˜", "DLã®æ¦‚è¦ã‚’æ•™ãˆã¦"]
    ]
    
    consistency_results = quality_lab.check_answer_consistency(
        query_engine=query_engine,
        similar_queries=similar_query_groups
    )
    
    print(f"âœ… Consistency analysis completed")
    print(f"   Query groups tested: {len(similar_query_groups)}")
    print(f"   Average consistency score: {consistency_results.get('average_consistency', 0):.3f}")
    
    # Show consistency details / ä¸€è²«æ€§è©³ç´°ã‚’è¡¨ç¤º
    if 'group_results' in consistency_results:
        print(f"\nğŸ“Š Consistency by Query Group / ã‚¯ã‚¨ãƒªã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ä¸€è²«æ€§:")
        for i, group_result in enumerate(consistency_results['group_results']):
            print(f"   Group {i+1}: {group_result.get('consistency_score', 0):.3f}")
            print(f"     Queries: {group_result.get('query_count', 0)}")
            print(f"     Similarity: {group_result.get('average_similarity', 0):.3f}")
    
    return contradiction_results, consistency_results


def demonstrate_advanced_evaluation(quality_lab: QualityLab,
                                   query_engine: QueryEngine,
                                   qa_pairs: List[QAPair]):
    """
    Demonstrate advanced evaluation techniques
    é«˜åº¦ãªè©•ä¾¡æŠ€è¡“ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    
    print("\n" + "="*60)
    print("ğŸš€ ADVANCED EVALUATION DEMONSTRATION")
    print("ğŸš€ é«˜åº¦ãªè©•ä¾¡ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("="*60)
    
    # 1. LLM-based evaluation / LLMãƒ™ãƒ¼ã‚¹è©•ä¾¡
    print(f"\nğŸ“Œ 1. LLM-based Quality Assessment")
    print(f"ğŸ“Œ 1. LLMãƒ™ãƒ¼ã‚¹å“è³ªè©•ä¾¡")
    print("-" * 40)
    
    llm_evaluation = quality_lab.evaluate_with_llm_judge(
        query_engine=query_engine,
        qa_pairs=qa_pairs[:5],  # Test with subset for demo
        criteria=[
            "accuracy",      # æ­£ç¢ºæ€§
            "completeness",  # å®Œå…¨æ€§
            "relevance",     # é–¢é€£æ€§
            "clarity"        # æ˜ç¢ºæ€§
        ]
    )
    
    print(f"âœ… LLM evaluation completed for {len(llm_evaluation['evaluations'])} answers")
    
    # Aggregate LLM scores / LLMã‚¹ã‚³ã‚¢ã‚’é›†è¨ˆ
    criteria_scores = {}
    for evaluation in llm_evaluation['evaluations']:
        for criterion, score_data in evaluation['scores'].items():
            if criterion not in criteria_scores:
                criteria_scores[criterion] = []
            criteria_scores[criterion].append(score_data['score'])
    
    print(f"\nğŸ“Š LLM Judge Scores / LLMåˆ¤å®šã‚¹ã‚³ã‚¢:")
    for criterion, scores in criteria_scores.items():
        avg_score = sum(scores) / len(scores)
        print(f"   {criterion.capitalize()}: {avg_score:.2f}/5.0")
    
    # 2. Robustness testing / å …ç‰¢æ€§ãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ“Œ 2. Robustness Testing")
    print(f"ğŸ“Œ 2. å …ç‰¢æ€§ãƒ†ã‚¹ãƒˆ")
    print("-" * 40)
    
    # Generate adversarial cases / æ•µå¯¾çš„ã‚±ãƒ¼ã‚¹ã‚’ç”Ÿæˆ
    adversarial_queries = [
        "",  # Empty query
        "ã“ã‚Œã¯æ„å‘³ä¸æ˜ãªè³ªå•ã§ã™ quantum blockchain AI unicorn",  # Nonsensical
        "æ©Ÿæ¢°å­¦ç¿’" * 50,  # Very long repetitive query
        "æ·±å±¤å­¦ç¿’ã¯æ©Ÿæ¢°å­¦ç¿’ã¨åŒã˜ã§ã™ã‹ï¼Ÿé•ã„ã¾ã™ã‹ï¼ŸåŒã˜ã§ã™ã‹ï¼Ÿ",  # Contradictory
    ]
    
    robustness_results = []
    for query in adversarial_queries:
        try:
            start_time = time.time()
            result = query_engine.answer(query)
            end_time = time.time()
            
            robustness_results.append({
                'query': query[:50] + "..." if len(query) > 50 else query,
                'success': True,
                'time': end_time - start_time,
                'answer_length': len(result.answer),
                'confidence': result.confidence
            })
        except Exception as e:
            robustness_results.append({
                'query': query[:50] + "..." if len(query) > 50 else query,
                'success': False,
                'error': str(e)
            })
    
    print(f"âœ… Robustness testing completed for {len(adversarial_queries)} adversarial cases")
    
    successful_cases = [r for r in robustness_results if r['success']]
    print(f"   Success rate: {len(successful_cases)}/{len(adversarial_queries)} ({len(successful_cases)/len(adversarial_queries)*100:.1f}%)")
    
    print(f"\nğŸ“Š Robustness Results / å …ç‰¢æ€§çµæœ:")
    for i, result in enumerate(robustness_results):
        status = "âœ…" if result['success'] else "âŒ"
        print(f"   {i+1}. {status} Query: {result['query']}")
        if result['success']:
            print(f"      Time: {result['time']:.3f}s, Confidence: {result['confidence']:.3f}")
        else:
            print(f"      Error: {result['error']}")
    
    # 3. Performance benchmarking / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    print(f"\nğŸ“Œ 3. Performance Benchmarking")
    print(f"ğŸ“Œ 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("-" * 40)
    
    benchmark_queries = [
        "AIã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "æ©Ÿæ¢°å­¦ç¿’ã®ç¨®é¡ã‚’æ•™ãˆã¦",
        "æ·±å±¤å­¦ç¿’ã®å¿œç”¨åˆ†é‡ã¯ï¼Ÿ",
        "è‡ªç„¶è¨€èªå‡¦ç†ã§ã§ãã‚‹ã“ã¨ã¯ï¼Ÿ"
    ]
    
    benchmark_results = []
    total_start_time = time.time()
    
    for query in benchmark_queries:
        query_start = time.time()
        result = query_engine.answer(query)
        query_end = time.time()
        
        benchmark_results.append({
            'query': query,
            'time': query_end - query_start,
            'confidence': result.confidence,
            'source_count': len(result.sources),
            'answer_length': len(result.answer)
        })
    
    total_time = time.time() - total_start_time
    
    # Calculate benchmark metrics / ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
    avg_time = sum(r['time'] for r in benchmark_results) / len(benchmark_results)
    avg_confidence = sum(r['confidence'] for r in benchmark_results) / len(benchmark_results)
    avg_sources = sum(r['source_count'] for r in benchmark_results) / len(benchmark_results)
    
    print(f"âœ… Benchmark completed for {len(benchmark_queries)} queries")
    print(f"\nğŸ“Š Benchmark Results / ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
    print(f"   Total time: {total_time:.3f}s")
    print(f"   Average time per query: {avg_time:.3f}s")
    print(f"   Throughput: {len(benchmark_queries)/total_time:.2f} queries/sec")
    print(f"   Average confidence: {avg_confidence:.3f}")
    print(f"   Average sources per answer: {avg_sources:.1f}")
    
    return llm_evaluation, robustness_results, benchmark_results


def demonstrate_report_generation(quality_lab: QualityLab, 
                                evaluation_results: Dict[str, Any],
                                temp_dir: Path):
    """
    Demonstrate comprehensive report generation
    åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    
    print("\n" + "="*60)
    print("ğŸ“Š EVALUATION REPORT GENERATION")
    print("ğŸ“Š è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    print("="*60)
    
    # Generate comprehensive report / åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    print(f"\nğŸ“ Generating comprehensive evaluation report...")
    print(f"ğŸ“ åŒ…æ‹¬çš„è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    
    report_path = temp_dir / "comprehensive_evaluation_report.md"
    
    try:
        report = quality_lab.generate_evaluation_report(
            evaluation_results=evaluation_results,
            output_file=str(report_path),
            include_detailed_analysis=True,
            include_recommendations=True
        )
        
        print(f"âœ… Report generated successfully")
        print(f"   Report file: {report_path}")
        print(f"   Report length: {len(report)} characters")
        
        # Show report preview / ãƒ¬ãƒãƒ¼ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤º
        print(f"\nğŸ“– Report Preview / ãƒ¬ãƒãƒ¼ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
        print("-" * 50)
        preview_lines = report.split('\n')[:15]
        for line in preview_lines:
            print(line)
        print("...")
        print("-" * 50)
        
    except Exception as e:
        print(f"âŒ Report generation failed: {e}")
        return None
    
    # Generate executive summary / è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    print(f"\nğŸ“‹ Generating executive summary...")
    print(f"ğŸ“‹ è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    
    try:
        executive_summary = quality_lab.generate_executive_summary(
            evaluation_results=evaluation_results,
            target_audience="technical_management"
        )
        
        summary_path = temp_dir / "executive_summary.md"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(executive_summary)
        
        print(f"âœ… Executive summary generated")
        print(f"   Summary file: {summary_path}")
        print(f"   Summary length: {len(executive_summary)} characters")
        
        # Show summary preview / è¦ç´„ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤º
        print(f"\nğŸ“„ Executive Summary Preview / è¦ç´„ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
        print("-" * 50)
        summary_lines = executive_summary.split('\n')[:10]
        for line in summary_lines:
            if line.strip():
                print(line)
        print("...")
        print("-" * 50)
        
    except Exception as e:
        print(f"âŒ Executive summary generation failed: {e}")
    
    # Generate JSON report for programmatic access / ãƒ—ãƒ­ã‚°ãƒ©ãƒ ç”¨JSONãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    json_path = temp_dir / "evaluation_results.json"
    try:
        # Prepare JSON-serializable data / JSONåŒ–å¯èƒ½ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        json_data = {
            'evaluation_summary': {
                'total_tests': len(evaluation_results.get('test_results', [])),
                'passed_tests': sum(1 for r in evaluation_results.get('test_results', []) if r.get('passed', False)),
                'pass_rate': sum(1 for r in evaluation_results.get('test_results', []) if r.get('passed', False)) / len(evaluation_results.get('test_results', [])) * 100 if evaluation_results.get('test_results') else 0,
                'processing_time': evaluation_results.get('processing_time', 0)
            },
            'metric_summaries': evaluation_results.get('metric_summaries', {}),
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ JSON report generated: {json_path}")
        
    except Exception as e:
        print(f"âŒ JSON report generation failed: {e}")
    
    return str(report_path)


def main():
    """
    Main demonstration function
    ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°
    """
    
    print("ğŸš€ Part 3: RAG Evaluation Tutorial")
    print("ğŸš€ Part 3: RAGè©•ä¾¡ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«")
    print("="*60)
    print("Comprehensive demonstration of RAG system evaluation with refinire-rag")
    print("refinire-ragã‚’ä½¿ç”¨ã—ãŸRAGã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡ã®åŒ…æ‹¬çš„ãªãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("")
    print("Features demonstrated / ãƒ‡ãƒ¢æ©Ÿèƒ½:")
    print("âœ“ Automated QA pair generation / è‡ªå‹•QAãƒšã‚¢ç”Ÿæˆ")
    print("âœ“ Comprehensive performance evaluation / åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡")
    print("âœ“ Contradiction detection / çŸ›ç›¾æ¤œå‡º")
    print("âœ“ Advanced evaluation techniques / é«˜åº¦ãªè©•ä¾¡æŠ€è¡“")
    print("âœ“ Report generation / ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    
    # Create temporary directory / ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        # Setup evaluation corpus / è©•ä¾¡ç”¨ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        print(f"\nğŸ“ Setup: Creating evaluation corpus in {temp_dir}")
        print(f"ğŸ“ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—: {temp_dir} ã«è©•ä¾¡ç”¨ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½œæˆä¸­")
        doc_store, vector_store, file_paths = setup_evaluation_corpus(temp_dir)
        
        # Setup QueryEngine / QueryEngineã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        query_engine = setup_query_engine(doc_store, vector_store)
        
        # Load documents for evaluation / è©•ä¾¡ç”¨æ–‡æ›¸ã‚’ãƒ­ãƒ¼ãƒ‰
        documents = []
        for file_path in file_paths:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                doc_id = Path(file_path).stem
                documents.append(Document(
                    id=doc_id,
                    content=content,
                    metadata={'source': file_path, 'topic': doc_id}
                ))
        
        # Initialize QualityLab / QualityLabã‚’åˆæœŸåŒ–
        print(f"\nğŸ”¬ Initializing QualityLab...")
        print(f"ğŸ”¬ QualityLabã‚’åˆæœŸåŒ–ä¸­...")
        
        quality_lab_config = QualityLabConfig(
            qa_pairs_per_document=3,
            similarity_threshold=0.8,
            question_types=["factual", "conceptual", "analytical", "comparative"],
            evaluation_metrics=["bleu", "rouge", "llm_judge"],
            include_detailed_analysis=True,
            include_contradiction_detection=True,
            output_format="markdown"
        )
        
        quality_lab = QualityLab(
            corpus_name="evaluation_tutorial",
            config=quality_lab_config
        )
        
        print(f"âœ… QualityLab initialized successfully")
        
        # Demonstration sequence / ãƒ‡ãƒ¢ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
        qa_pairs = demonstrate_qa_generation(quality_lab, documents)
        evaluation_results = demonstrate_performance_evaluation(quality_lab, query_engine, qa_pairs)
        contradiction_results, consistency_results = demonstrate_contradiction_detection(quality_lab, documents, query_engine)
        llm_eval, robustness, benchmark = demonstrate_advanced_evaluation(quality_lab, query_engine, qa_pairs)
        report_path = demonstrate_report_generation(quality_lab, evaluation_results, temp_dir)
        
        print("\n" + "="*60)
        print("ğŸ‰ TUTORIAL COMPLETE / ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å®Œäº†")
        print("="*60)
        print("âœ… All RAG evaluation demonstrations completed successfully!")
        print("âœ… ã™ã¹ã¦ã®RAGè©•ä¾¡ãƒ‡ãƒ¢ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        print("")
        print("ğŸ“š What you learned / å­¦ç¿’å†…å®¹:")
        print("   â€¢ Automated QA pair generation with multiple question types")
        print("     è¤‡æ•°è³ªå•ã‚¿ã‚¤ãƒ—ã§ã®è‡ªå‹•QAãƒšã‚¢ç”Ÿæˆ")
        print("   â€¢ Comprehensive performance evaluation with multiple metrics")
        print("     è¤‡æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ã®åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡")
        print("   â€¢ Contradiction detection and consistency analysis")
        print("     çŸ›ç›¾æ¤œå‡ºã¨ä¸€è²«æ€§åˆ†æ")
        print("   â€¢ Advanced evaluation techniques (LLM judge, robustness testing)")
        print("     é«˜åº¦ãªè©•ä¾¡æŠ€è¡“ï¼ˆLLMåˆ¤å®šã€å …ç‰¢æ€§ãƒ†ã‚¹ãƒˆï¼‰")
        print("   â€¢ Comprehensive report generation")
        print("     åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        print("")
        print(f"ğŸ“ Generated files available in: {temp_dir}")
        print(f"ğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€: {temp_dir}")
        print("")
        print("Generated reports / ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ:")
        if report_path:
            print(f"   â€¢ Comprehensive evaluation report: {report_path}")
        print(f"   â€¢ Executive summary: {temp_dir}/executive_summary.md")
        print(f"   â€¢ JSON data: {temp_dir}/evaluation_results.json")
        print("")
        print("Next step / æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("â†’ End-to-End Integration Tutorial (ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰çµ±åˆãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«)")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Tutorial failed / ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # Cleanup (comment out to inspect generated files)
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã™ã‚‹å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
        # shutil.rmtree(temp_dir)
        pass


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)