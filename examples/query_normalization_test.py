#!/usr/bin/env python3
"""
Query Normalization Test

Test query normalization functionality in QueryEngine with a 
normalized corpus that has dictionary-based term standardization.
"""

import sys
import tempfile
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from refinire_rag.application.corpus_manager_new import CorpusManager
from refinire_rag.application.query_engine import QueryEngine, QueryEngineConfig
from refinire_rag.storage.sqlite_store import SQLiteDocumentStore
from refinire_rag.storage.in_memory_vector_store import InMemoryVectorStore
from refinire_rag.retrieval import SimpleRetriever, SimpleReranker, SimpleReader
from refinire_rag.embedding import TFIDFEmbedder, TFIDFEmbeddingConfig
from refinire_rag.models.document import Document


def create_sample_files_with_variations(temp_dir: Path) -> list:
    """Create sample files with term variations for normalization testing"""
    
    files = []
    
    # File 1: Uses "RAG" and various expressions
    file1 = temp_dir / "rag_doc1.txt"
    file1.write_text("""
RAGï¼ˆRetrieval-Augmented Generationï¼‰ã¯é©æ–°çš„ãªæŠ€è¡“ã§ã™ã€‚
æ¤œç´¢æ‹¡å¼µç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦ã€LLMã¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’çµ±åˆã—ã¾ã™ã€‚
ã“ã®RAGã‚·ã‚¹ãƒ†ãƒ ã¯ä¼æ¥­ã§åºƒãä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚
""", encoding='utf-8')
    files.append(str(file1))
    
    # File 2: Uses variations like "æ¤œç´¢å¼·åŒ–ç”Ÿæˆ"
    file2 = temp_dir / "rag_doc2.txt" 
    file2.write_text("""
æ¤œç´¢å¼·åŒ–ç”ŸæˆæŠ€è¡“ã¯æœ€æ–°ã®AIæ‰‹æ³•ã§ã™ã€‚
æ¤œç´¢æ‹¡å¼µç”Ÿæˆã¨ã‚‚å‘¼ã°ã‚Œã€æƒ…å ±æ¤œç´¢ã¨ç”Ÿæˆã‚’çµ„ã¿åˆã‚ã›ã¾ã™ã€‚
LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ãŒã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã™ã€‚
""", encoding='utf-8')
    files.append(str(file2))
    
    # File 3: Uses "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢" and "æ„å‘³æ¤œç´¢"
    file3 = temp_dir / "vector_doc.txt"
    file3.write_text("""
ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯æ„å‘³çš„é¡ä¼¼æ€§ã‚’åŸºã«ã—ãŸæ¤œç´¢æ‰‹æ³•ã§ã™ã€‚
ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã¨ã‚‚å‘¼ã°ã‚Œã¾ã™ã€‚
æ–‡æ›¸åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ã£ã¦æ„å‘³æ¤œç´¢ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
""", encoding='utf-8')
    files.append(str(file3))
    
    return files


def create_test_dictionary(temp_dir: Path) -> str:
    """Create a test dictionary file for normalization"""
    
    dict_file = temp_dir / "test_dictionary.md"
    dict_file.write_text("""# ãƒ‰ãƒ¡ã‚¤ãƒ³ç”¨èªè¾æ›¸

## æŠ€è¡“ç”¨èª

- **RAG** (Retrieval-Augmented Generation): æ¤œç´¢æ‹¡å¼µç”Ÿæˆ
  - è¡¨ç¾æºã‚‰ã: æ¤œç´¢æ‹¡å¼µç”Ÿæˆ, æ¤œç´¢å¼·åŒ–ç”Ÿæˆ, RAGã‚·ã‚¹ãƒ†ãƒ 

- **ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢** (Vector Search): ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢  
  - è¡¨ç¾æºã‚‰ã: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢, æ„å‘³æ¤œç´¢, ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢, æ„å‘³çš„æ¤œç´¢

- **LLM** (Large Language Model): å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«
  - è¡¨ç¾æºã‚‰ã: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«, è¨€èªãƒ¢ãƒ‡ãƒ«, LLMãƒ¢ãƒ‡ãƒ«

- **åŸ‹ã‚è¾¼ã¿** (Embedding): åŸ‹ã‚è¾¼ã¿
  - è¡¨ç¾æºã‚‰ã: åŸ‹ã‚è¾¼ã¿, ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°, ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾
""", encoding='utf-8')
    
    return str(dict_file)


def build_normalized_corpus(temp_dir: Path, file_paths: list, dict_path: str):
    """Build a corpus with normalization using dictionary"""
    
    print("ğŸ“š Building normalized corpus...")
    
    # Initialize stores
    doc_store = SQLiteDocumentStore(":memory:")
    vector_store = InMemoryVectorStore()
    
    # Create semantic RAG manager (includes normalization)
    corpus_manager = CorpusManager.create_semantic_rag(doc_store, vector_store)
    
    # Configure with custom dictionary
    stage_configs = {
        "dictionary_config": {
            "dictionary_file_path": dict_path,
            "focus_on_technical_terms": True
        },
        "normalizer_config": {
            "dictionary_file_path": dict_path,
            "normalize_variations": True,
            "expand_abbreviations": True
        }
    }
    
    try:
        stats = corpus_manager.build_corpus(file_paths, stage_configs=stage_configs)
        print(f"âœ… Corpus built with normalization:")
        print(f"   - Documents: {stats.total_documents_created}")
        print(f"   - Processing time: {stats.total_processing_time:.3f}s")
        print(f"   - Stages executed: {stats.pipeline_stages_executed}")
    except Exception as e:
        print(f"âŒ Corpus building failed: {e}")
        # Fallback: Build simple corpus
        print("ğŸ”„ Falling back to simple corpus...")
        simple_manager = CorpusManager.create_simple_rag(doc_store, vector_store)
        stats = simple_manager.build_corpus(file_paths)
    
    return doc_store, vector_store


def test_query_normalization(doc_store, vector_store, dict_path: str):
    """Test query normalization with various query expressions"""
    
    print("\n" + "="*60)
    print("ğŸ” QUERY NORMALIZATION TEST")
    print("="*60)
    
    # Create QueryEngine with normalization enabled
    config = TFIDFEmbeddingConfig(min_df=1, max_df=1.0)
    embedder = TFIDFEmbedder(config=config)
    
    # Manually fit embedder (simplified for testing)
    sample_texts = ["RAGæ¤œç´¢æ‹¡å¼µç”Ÿæˆ", "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æ„å‘³æ¤œç´¢", "LLMå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«"]
    embedder.fit(sample_texts)
    
    retriever = SimpleRetriever(vector_store, embedder=embedder)
    reranker = SimpleReranker()
    reader = SimpleReader()
    
    query_config = QueryEngineConfig(
        enable_query_normalization=True,
        auto_detect_corpus_state=True,
        include_processing_metadata=True
    )
    
    query_engine = QueryEngine(
        document_store=doc_store,
        vector_store=vector_store,
        retriever=retriever,
        reader=reader,
        reranker=reranker,
        config=query_config
    )
    
    print(f"ğŸ¤– QueryEngine initialized")
    print(f"   Corpus state: {query_engine.corpus_state}")
    print(f"   Query normalization: {'Enabled' if query_engine.normalizer else 'Disabled'}")
    
    # Test queries with variations that should be normalized
    test_queries = [
        {
            "query": "æ¤œç´¢å¼·åŒ–ç”Ÿæˆã«ã¤ã„ã¦æ•™ãˆã¦", 
            "expected_normalization": "æ¤œç´¢æ‹¡å¼µç”Ÿæˆã«ã¤ã„ã¦æ•™ãˆã¦",
            "description": "æ¤œç´¢å¼·åŒ–ç”Ÿæˆ â†’ æ¤œç´¢æ‹¡å¼µç”Ÿæˆ"
        },
        {
            "query": "æ„å‘³æ¤œç´¢ã®ä»•çµ„ã¿ã¯ï¼Ÿ",
            "expected_normalization": "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ä»•çµ„ã¿ã¯ï¼Ÿ", 
            "description": "æ„å‘³æ¤œç´¢ â†’ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"
        },
        {
            "query": "RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆ©ç‚¹ã‚’èª¬æ˜ã—ã¦",
            "expected_normalization": "æ¤œç´¢æ‹¡å¼µç”Ÿæˆã®åˆ©ç‚¹ã‚’èª¬æ˜ã—ã¦",
            "description": "RAGã‚·ã‚¹ãƒ†ãƒ  â†’ æ¤œç´¢æ‹¡å¼µç”Ÿæˆ"
        },
        {
            "query": "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "expected_normalization": "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "description": "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ â†’ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"
        }
    ]
    
    for i, test_case in enumerate(test_queries, 1):
        query = test_case["query"]
        description = test_case["description"]
        
        print(f"\nğŸ“Œ Test {i}: {description}")
        print(f"   Original: {query}")
        
        try:
            result = query_engine.answer(query)
            
            # Check if query was normalized
            normalized = result.metadata.get("query_normalized", False)
            normalized_query = result.normalized_query
            
            if normalized and normalized_query:
                print(f"   âœ… Normalized: {normalized_query}")
                print(f"   ğŸ”„ Normalization: {'Success' if normalized_query != query else 'No change'}")
            else:
                print(f"   âŒ No normalization applied")
            
            print(f"   ğŸ¤– Answer: {result.answer[:100]}{'...' if len(result.answer) > 100 else ''}")
            print(f"   ğŸ“Š Sources: {result.metadata.get('source_count', 0)}")
            print(f"   â±ï¸  Time: {result.metadata.get('processing_time', 0):.3f}s")
            
        except Exception as e:
            print(f"   âŒ Query failed: {e}")
    
    # Display normalization statistics
    print(f"\nğŸ“ˆ Normalization Statistics:")
    stats = query_engine.get_engine_stats()
    print(f"   - Total queries: {stats.get('queries_processed', 0)}")
    print(f"   - Normalized queries: {stats.get('queries_normalized', 0)}")
    print(f"   - Normalization rate: {stats.get('queries_normalized', 0) / max(stats.get('queries_processed', 1), 1) * 100:.1f}%")
    
    if query_engine.normalizer:
        normalizer_stats = stats.get('normalizer_stats', {})
        print(f"   - Normalizer processing time: {normalizer_stats.get('processing_time', 0):.3f}s")


def main():
    """Main test function"""
    
    print("ğŸš€ Query Normalization Test")
    print("="*60)
    print("Testing automatic query normalization with dictionary-based term standardization")
    
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        # Create test files and dictionary
        print("\nğŸ“ Creating test files with term variations...")
        file_paths = create_sample_files_with_variations(temp_dir)
        dict_path = create_test_dictionary(temp_dir)
        
        print(f"Created {len(file_paths)} files with term variations")
        print(f"Created dictionary: {dict_path}")
        
        # Build normalized corpus
        doc_store, vector_store = build_normalized_corpus(temp_dir, file_paths, dict_path)
        
        # Test query normalization
        test_query_normalization(doc_store, vector_store, dict_path)
        
        print("\nğŸ‰ Query normalization test completed!")
        print(f"ğŸ“ Test files: {temp_dir}")
        
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # Clean up (comment out to inspect files)
        # shutil.rmtree(temp_dir)
        pass
    
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)