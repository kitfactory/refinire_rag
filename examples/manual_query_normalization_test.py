#!/usr/bin/env python3
"""
Manual Query Normalization Test

Test query normalization functionality by manually setting up 
the normalizer to demonstrate the feature works correctly.
"""

import sys
import tempfile
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from refinire_rag.use_cases.query_engine import QueryEngine, QueryEngineConfig
from refinire_rag.storage.sqlite_store import SQLiteDocumentStore
from refinire_rag.storage.in_memory_vector_store import InMemoryVectorStore
from refinire_rag.retrieval import SimpleRetriever, SimpleReranker, SimpleReader
from refinire_rag.processing.normalizer import Normalizer, NormalizerConfig
from refinire_rag.embedding import TFIDFEmbedder, TFIDFEmbeddingConfig
from refinire_rag.models.document import Document
from refinire_rag.storage.vector_store import VectorEntry


def create_test_dictionary(temp_dir: Path) -> str:
    """Create a test dictionary file for normalization"""
    
    dict_file = temp_dir / "test_dictionary.md"
    dict_file.write_text("""# ãƒ‰ãƒ¡ã‚¤ãƒ³ç”¨èªè¾æ›¸

## æŠ€è¡“ç”¨èª

- **RAG** (Retrieval-Augmented Generation): æ¤œç´¢æ‹¡å¼µç”Ÿæˆ
  - è¡¨ç¾æºã‚‰ã: æ¤œç´¢æ‹¡å¼µç”Ÿæˆ, æ¤œç´¢å¼·åŒ–ç”Ÿæˆ, RAGã‚·ã‚¹ãƒ†ãƒ , æ¤œç´¢æ‹¡å¼µæŠ€è¡“

- **ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢** (Vector Search): ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢  
  - è¡¨ç¾æºã‚‰ã: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢, æ„å‘³æ¤œç´¢, ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢, æ„å‘³çš„æ¤œç´¢

- **LLM** (Large Language Model): å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«
  - è¡¨ç¾æºã‚‰ã: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«, è¨€èªãƒ¢ãƒ‡ãƒ«, LLMãƒ¢ãƒ‡ãƒ«

- **åŸ‹ã‚è¾¼ã¿** (Embedding): åŸ‹ã‚è¾¼ã¿
  - è¡¨ç¾æºã‚‰ã: åŸ‹ã‚è¾¼ã¿, ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°, ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾
""", encoding='utf-8')
    
    return str(dict_file)


def create_sample_documents() -> list:
    """Create sample documents for testing"""
    
    docs = [
        Document(
            id="doc1",
            content="""
æ¤œç´¢æ‹¡å¼µç”Ÿæˆï¼ˆRAGï¼‰ã¯é©æ–°çš„ãªAIæŠ€è¡“ã§ã™ã€‚
å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¨å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’çµ±åˆã—ã€
ã‚ˆã‚Šæ­£ç¢ºã§æ ¹æ‹ ã®ã‚ã‚‹å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
ã“ã®æŠ€è¡“ã¯ä¼æ¥­ã®æƒ…å ±æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã§åºƒãæ¡ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
""",
            metadata={"title": "RAG Technology", "type": "overview"}
        ),
        
        Document(
            id="doc2", 
            content="""
ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯æ„å‘³çš„é¡ä¼¼æ€§ã«åŸºã¥ãæ¤œç´¢æŠ€è¡“ã§ã™ã€‚
æ–‡æ›¸ã‚’é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã«åŸ‹ã‚è¾¼ã¿ã€
ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãªã©ã‚’ç”¨ã„ã¦é–¢é€£æ–‡æ›¸ã‚’ç™ºè¦‹ã—ã¾ã™ã€‚
å¾“æ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¨ã¯ç•°ãªã‚Šã€æ–‡è„ˆã‚’ç†è§£ã—ãŸæ¤œç´¢ãŒå¯èƒ½ã§ã™ã€‚
""",
            metadata={"title": "Vector Search", "type": "technical"}
        ),
        
        Document(
            id="doc3",
            content="""
å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã¯è‡ªç„¶è¨€èªå‡¦ç†ã®ä¸­æ ¸æŠ€è¡“ã§ã™ã€‚
GPTã€Claudeã€Geminiãªã©ã®å…ˆé€²çš„ãªãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ã¾ã™ã€‚
ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯æ–‡ç« ç”Ÿæˆã€ç¿»è¨³ã€è¦ç´„ãªã©ã®
å¹…åºƒã„ã‚¿ã‚¹ã‚¯ã«å¯¾å¿œã§ãã¾ã™ã€‚
""",
            metadata={"title": "LLM Overview", "type": "technical"}
        )
    ]
    
    return docs


def setup_manual_corpus(documents: list) -> tuple:
    """Manually setup corpus with vector embeddings"""
    
    print("ğŸ“š Setting up manual corpus...")
    
    # Create stores
    doc_store = SQLiteDocumentStore(":memory:")
    vector_store = InMemoryVectorStore()
    
    # Setup embedder
    config = TFIDFEmbeddingConfig(min_df=1, max_df=1.0)
    embedder = TFIDFEmbedder(config=config)
    
    # Fit embedder
    corpus_texts = [doc.content for doc in documents]
    embedder.fit(corpus_texts)
    
    # Add documents to stores
    for doc in documents:
        # Store in document store
        doc_store.store_document(doc)
        
        # Generate embedding and store in vector store
        embedding_result = embedder.embed_text(doc.content)
        vector_entry = VectorEntry(
            document_id=doc.id,
            content=doc.content[:200] + "..." if len(doc.content) > 200 else doc.content,
            embedding=embedding_result.vector.tolist(),
            metadata=doc.metadata
        )
        vector_store.add_vector(vector_entry)
        
        print(f"   âœ… Added {doc.id}: {doc.metadata.get('title', 'No title')}")
    
    print(f"ğŸ“Š Manual corpus setup completed with {len(documents)} documents")
    return doc_store, vector_store, embedder


def test_normalizer_standalone(dict_path: str):
    """Test the Normalizer component standalone"""
    
    print("\n" + "="*60)
    print("ğŸ”§ STANDALONE NORMALIZER TEST")
    print("="*60)
    
    # Create normalizer
    normalizer_config = NormalizerConfig(
        dictionary_file_path=dict_path,
        normalize_variations=True,
        expand_abbreviations=True,
        whole_word_only=False  # Disable for Japanese text
    )
    
    normalizer = Normalizer(normalizer_config)
    
    # Test queries
    test_queries = [
        "æ¤œç´¢å¼·åŒ–ç”Ÿæˆã«ã¤ã„ã¦æ•™ãˆã¦",
        "æ„å‘³æ¤œç´¢ã®ä»•çµ„ã¿ã¯ï¼Ÿ", 
        "RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆ©ç‚¹ã‚’èª¬æ˜ã—ã¦",
        "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "LLMãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã¯ï¼Ÿ"
    ]
    
    print(f"ğŸ“– Dictionary loaded: {dict_path}")
    print(f"ğŸ”§ Normalizer configured")
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ“Œ Test {i}:")
        print(f"   Original: {query}")
        
        try:
            # Create query document
            query_doc = Document(
                id=f"query_{i}",
                content=query,
                metadata={"is_query": True}
            )
            
            # Normalize
            normalized_docs = normalizer.process(query_doc)
            
            if normalized_docs:
                normalized_query = normalized_docs[0].content
                if normalized_query != query:
                    print(f"   âœ… Normalized: {normalized_query}")
                    print(f"   ğŸ”„ Change: Yes")
                else:
                    print(f"   âšª Normalized: {normalized_query}")
                    print(f"   ğŸ”„ Change: No")
            else:
                print(f"   âŒ Normalization failed")
                
        except Exception as e:
            print(f"   âŒ Error: {e}")


def test_query_engine_with_manual_normalizer(doc_store, vector_store, embedder, dict_path: str):
    """Test QueryEngine with manually configured normalizer"""
    
    print("\n" + "="*60)
    print("ğŸ¤– QUERY ENGINE WITH MANUAL NORMALIZER")
    print("="*60)
    
    # Create query engine components
    retriever = SimpleRetriever(vector_store, embedder=embedder)
    reranker = SimpleReranker()
    reader = SimpleReader()
    
    # Create query engine
    query_config = QueryEngineConfig(
        enable_query_normalization=True,
        auto_detect_corpus_state=False,  # Disable auto-detection
        include_processing_metadata=True
    )
    
    query_engine = QueryEngine(
        document_store=doc_store,
        vector_store=vector_store,
        retriever=retriever,
        reader=reader,
        reranker=reranker,
        config=query_config
    )
    
    # Manually set up normalizer
    normalizer_config = NormalizerConfig(
        dictionary_file_path=dict_path,
        normalize_variations=True,
        expand_abbreviations=True,
        whole_word_only=False  # Disable for Japanese text
    )
    query_engine.normalizer = Normalizer(normalizer_config)
    
    # Update corpus state to indicate normalization is available
    query_engine.corpus_state = {
        "has_normalization": True,
        "dictionary_path": dict_path,
        "manual_setup": True
    }
    
    print(f"ğŸ¤– QueryEngine configured with manual normalizer")
    print(f"   Normalizer: {'Enabled' if query_engine.normalizer else 'Disabled'}")
    print(f"   Dictionary: {dict_path}")
    
    # Test queries with variations
    test_queries = [
        {
            "query": "æ¤œç´¢å¼·åŒ–ç”Ÿæˆã«ã¤ã„ã¦æ•™ãˆã¦", 
            "description": "æ¤œç´¢å¼·åŒ–ç”Ÿæˆ â†’ æ¤œç´¢æ‹¡å¼µç”Ÿæˆ"
        },
        {
            "query": "æ„å‘³æ¤œç´¢ã®ä»•çµ„ã¿ã¯ï¼Ÿ",
            "description": "æ„å‘³æ¤œç´¢ â†’ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"
        },
        {
            "query": "RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆ©ç‚¹ã‚’èª¬æ˜ã—ã¦",
            "description": "RAGã‚·ã‚¹ãƒ†ãƒ  â†’ æ¤œç´¢æ‹¡å¼µç”Ÿæˆ"
        },
        {
            "query": "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "description": "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ â†’ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"
        }
    ]
    
    for i, test_case in enumerate(test_queries, 1):
        query = test_case["query"]
        description = test_case["description"]
        
        print(f"\nğŸ“Œ Test {i}: {description}")
        print(f"   Original: {query}")
        
        try:
            result = query_engine.answer(query)
            
            # Check normalization results
            normalized = result.metadata.get("query_normalized", False)
            normalized_query = result.normalized_query
            
            if normalized and normalized_query:
                print(f"   âœ… Normalized: {normalized_query}")
                print(f"   ğŸ”„ Applied: Yes")
            else:
                print(f"   âŒ Normalized: {normalized_query or 'None'}")
                print(f"   ğŸ”„ Applied: No")
            
            # Show results
            print(f"   ğŸ¤– Answer: {result.answer[:100]}{'...' if len(result.answer) > 100 else ''}")
            print(f"   ğŸ“Š Sources: {result.metadata.get('source_count', 0)}")
            print(f"   â±ï¸  Time: {result.metadata.get('processing_time', 0):.3f}s")
            
            if result.sources:
                print(f"   ğŸ“„ Top source: {result.sources[0].metadata.get('title', 'Unknown')}")
            
        except Exception as e:
            print(f"   âŒ Query failed: {e}")
            import traceback
            traceback.print_exc()
    
    # Show statistics
    print(f"\nğŸ“ˆ Final Statistics:")
    stats = query_engine.get_engine_stats()
    print(f"   - Total queries: {stats.get('queries_processed', 0)}")
    print(f"   - Normalized queries: {stats.get('queries_normalized', 0)}")
    print(f"   - Normalization rate: {stats.get('queries_normalized', 0) / max(stats.get('queries_processed', 1), 1) * 100:.1f}%")


def main():
    """Main test function"""
    
    print("ğŸš€ Manual Query Normalization Test")
    print("="*60)
    print("Testing query normalization with manually configured components")
    
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        # Create test dictionary
        print("\nğŸ“– Creating test dictionary...")
        dict_path = create_test_dictionary(temp_dir)
        print(f"Dictionary created: {dict_path}")
        
        # Test standalone normalizer first
        test_normalizer_standalone(dict_path)
        
        # Create sample documents and setup corpus
        print("\nğŸ“š Setting up test corpus...")
        documents = create_sample_documents()
        doc_store, vector_store, embedder = setup_manual_corpus(documents)
        
        # Test query engine with manual normalizer
        test_query_engine_with_manual_normalizer(doc_store, vector_store, embedder, dict_path)
        
        print("\nğŸ‰ Manual query normalization test completed!")
        print(f"ğŸ“ Test files: {temp_dir}")
        
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # Clean up (comment out to inspect files)
        # shutil.rmtree(temp_dir)
        pass
    
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)