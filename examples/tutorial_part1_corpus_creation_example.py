#!/usr/bin/env python3
"""
Part 1: Corpus Creation Tutorial Example
ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ä¾‹

This example demonstrates comprehensive corpus creation using refinire-rag's CorpusManager
with different approaches: preset configurations, stage selection, and custom pipelines.

ã“ã®ä¾‹ã§ã¯ã€refinire-ragã®CorpusManagerã‚’ä½¿ç”¨ã—ãŸåŒ…æ‹¬çš„ãªã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆã‚’ã€
ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šã€ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠã€ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§å®Ÿæ¼”ã—ã¾ã™ã€‚
"""

import sys
import tempfile
import shutil
from pathlib import Path
from typing import List

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from refinire_rag.application.corpus_manager_new import CorpusManager
from refinire_rag.storage.sqlite_store import SQLiteDocumentStore
from refinire_rag.storage.in_memory_vector_store import InMemoryVectorStore
from refinire_rag.processing.dictionary_maker import DictionaryMakerConfig
from refinire_rag.processing.normalizer import NormalizerConfig
from refinire_rag.processing.chunker import ChunkingConfig
from refinire_rag.loader.loader import LoaderConfig


def create_sample_documents(temp_dir: Path) -> List[str]:
    """
    Create sample documents for corpus creation demonstration
    ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆãƒ‡ãƒ¢ç”¨ã®ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ã‚’ä½œæˆ
    """
    
    print("ğŸ“„ Creating sample documents...")
    files = []
    
    # AI Overview document
    ai_doc = temp_dir / "ai_overview.txt"
    ai_doc.write_text("""
äººå·¥çŸ¥èƒ½ï¼ˆAIï¼šArtificial Intelligenceï¼‰ã¨ã¯ã€äººé–“ã®çŸ¥çš„æ´»å‹•ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ä»£è¡Œã•ã›ã‚‹æŠ€è¡“ã§ã™ã€‚
æ©Ÿæ¢°å­¦ç¿’ï¼ˆMachine Learning, MLï¼‰ã€æ·±å±¤å­¦ç¿’ï¼ˆDeep Learning, DLï¼‰ã€è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰ã€
ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ï¼ˆComputer Visionï¼‰ãªã©ã®æŠ€è¡“åˆ†é‡ã‚’å«ã¿ã¾ã™ã€‚

ä¸»è¦ãªå¿œç”¨åˆ†é‡ï¼š
- è‡ªå‹•é‹è»¢æŠ€è¡“ï¼ˆAutonomous Drivingï¼‰
- éŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ ï¼ˆSpeech Recognitionï¼‰
- ç”»åƒèªè­˜ãƒ»åˆ†é¡ï¼ˆImage Classificationï¼‰
- æ©Ÿæ¢°ç¿»è¨³ï¼ˆMachine Translationï¼‰
- æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆRecommendation Systemï¼‰

AIã®ç™ºå±•ã«ã‚ˆã‚Šã€å¾“æ¥äººé–“ãŒè¡Œã£ã¦ã„ãŸè¤‡é›‘ãªåˆ¤æ–­ã‚„å‰µé€ çš„ãªä½œæ¥­ã‚‚
ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒå®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ãã¦ã„ã¾ã™ã€‚
""", encoding='utf-8')
    files.append(str(ai_doc))
    
    # Machine Learning document
    ml_doc = temp_dir / "machine_learning.txt"
    ml_doc.write_text("""
æ©Ÿæ¢°å­¦ç¿’ï¼ˆMLï¼‰ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã€æ˜ç¤ºçš„ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã™ã‚‹ã“ã¨ãªã
ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ã€äºˆæ¸¬ã‚„åˆ¤æ–­ã‚’è¡Œã†æŠ€è¡“ã§ã™ã€‚

ä¸»è¦ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
- ç·šå½¢å›å¸°ï¼ˆLinear Regressionï¼‰
- ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆLogistic Regressionï¼‰
- æ±ºå®šæœ¨ï¼ˆDecision Treeï¼‰
- ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼ˆRandom Forestï¼‰
- ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ï¼ˆSVM: Support Vector Machineï¼‰
- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆNeural Networkï¼‰

å­¦ç¿’ã®ç¨®é¡ï¼š
1. æ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆSupervised Learningï¼‰ï¼šæ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
2. æ•™å¸«ãªã—å­¦ç¿’ï¼ˆUnsupervised Learningï¼‰ï¼šæ­£è§£ãƒ‡ãƒ¼ã‚¿ãªã—ã§ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹
3. å¼·åŒ–å­¦ç¿’ï¼ˆReinforcement Learningï¼‰ï¼šè©¦è¡ŒéŒ¯èª¤ã‚’é€šã˜ãŸå­¦ç¿’

æ©Ÿæ¢°å­¦ç¿’ã¯é‡‘èã€åŒ»ç™‚ã€è£½é€ æ¥­ã€ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãªã©
ã‚ã‚‰ã‚†ã‚‹åˆ†é‡ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
""", encoding='utf-8')
    files.append(str(ml_doc))
    
    # Deep Learning document
    dl_doc = temp_dir / "deep_learning.txt"
    dl_doc.write_text("""
æ·±å±¤å­¦ç¿’ï¼ˆDLï¼‰ã¯æ©Ÿæ¢°å­¦ç¿’ã®ä¸€æ‰‹æ³•ã§ã€äººé–“ã®è„³ã®ç¥çµŒå›è·¯ã‚’æ¨¡å€£ã—ãŸ
å¤šå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã™ã‚‹æŠ€è¡“ã§ã™ã€‚

ä¸»è¦ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼š
- ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆCNN: Convolutional Neural Networkï¼‰
- å†å¸°ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆRNN: Recurrent Neural Networkï¼‰
- é•·çŸ­æœŸè¨˜æ†¶ï¼ˆLSTM: Long Short-Term Memoryï¼‰
- ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆTransformerï¼‰
- ç”Ÿæˆçš„æ•µå¯¾ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆGAN: Generative Adversarial Networkï¼‰

å¿œç”¨ä¾‹ï¼š
- ç”»åƒèªè­˜ãƒ»ç‰©ä½“æ¤œå‡º
- è‡ªç„¶è¨€èªå‡¦ç†ãƒ»æ©Ÿæ¢°ç¿»è¨³
- éŸ³å£°èªè­˜ãƒ»åˆæˆ
- ã‚²ãƒ¼ãƒ AIï¼ˆå›²ç¢ã€ãƒã‚§ã‚¹ï¼‰
- è‡ªå‹•é‹è»¢

æ·±å±¤å­¦ç¿’ã®ç™ºé”ã«ã‚ˆã‚Šã€å¾“æ¥å›°é›£ã¨ã•ã‚Œã¦ã„ãŸ
ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã‚„äºˆæ¸¬ã‚¿ã‚¹ã‚¯ãŒå¤§å¹…ã«æ”¹å–„ã•ã‚Œã¾ã—ãŸã€‚
""", encoding='utf-8')
    files.append(str(dl_doc))
    
    # RAG Technology document
    rag_doc = temp_dir / "rag_technology.txt"
    rag_doc.write_text("""
RAGï¼ˆRetrieval-Augmented Generationï¼šæ¤œç´¢æ‹¡å¼µç”Ÿæˆï¼‰ã¯ã€
å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã¨å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’çµ„ã¿åˆã‚ã›ãŸæŠ€è¡“ã§ã™ã€‚

RAGã®æ§‹æˆè¦ç´ ï¼š
- æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆDocument Databaseï¼‰
- ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆVector Search Engineï¼‰
- åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆEmbedding Modelï¼‰
- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLarge Language Modelï¼‰
- æ¤œç´¢ãƒ»ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆRetrieval-Generation Pipelineï¼‰

å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼š
1. è³ªå•ã®åŸ‹ã‚è¾¼ã¿å¤‰æ›
2. ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ€§ã«ã‚ˆã‚‹æ–‡æ›¸æ¤œç´¢
3. é–¢é€£æ–‡æ›¸ã®å–å¾—
4. LLMã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆ

åˆ©ç‚¹ï¼š
- æœ€æ–°æƒ…å ±ã¸ã®å¯¾å¿œ
- ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå¹»è¦šï¼‰ã®æ¸›å°‘
- å°‚é–€åˆ†é‡ã¸ã®é©å¿œæ€§
- æ ¹æ‹ ã®æ˜ç¤º

RAGã¯ä¼æ¥­ã®æ–‡æ›¸æ¤œç´¢ã€é¡§å®¢ã‚µãƒãƒ¼ãƒˆã€ç ”ç©¶æ”¯æ´ãªã©ã§
åºƒãæ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
""", encoding='utf-8')
    files.append(str(rag_doc))
    
    print(f"âœ… Created {len(files)} sample documents")
    return files


def demonstrate_preset_configurations(temp_dir: Path, file_paths: List[str]):
    """
    Demonstrate preset configurations
    ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    
    print("\n" + "="*60)
    print("ğŸ¯ PRESET CONFIGURATIONS DEMONSTRATION")
    print("ğŸ¯ ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("="*60)
    
    # 1. Simple RAG
    print("\nğŸ“Œ 1. Simple RAG (Load â†’ Chunk â†’ Vector)")
    print("ğŸ“Œ 1. ã‚·ãƒ³ãƒ—ãƒ«RAGï¼ˆãƒ­ãƒ¼ãƒ‰ â†’ ãƒãƒ£ãƒ³ã‚¯ â†’ ãƒ™ã‚¯ãƒˆãƒ«ï¼‰")
    print("-" * 50)
    
    doc_store1 = SQLiteDocumentStore(":memory:")
    vector_store1 = InMemoryVectorStore()
    
    simple_manager = CorpusManager.create_simple_rag(doc_store1, vector_store1)
    simple_stats = simple_manager.build_corpus(file_paths)
    
    print(f"âœ… Simple RAG Results / ã‚·ãƒ³ãƒ—ãƒ«RAGçµæœ:")
    print(f"   - Files processed / å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {simple_stats.total_files_processed}")
    print(f"   - Documents created / ä½œæˆæ–‡æ›¸æ•°: {simple_stats.total_documents_created}")
    print(f"   - Chunks created / ä½œæˆãƒãƒ£ãƒ³ã‚¯æ•°: {simple_stats.total_chunks_created}")
    print(f"   - Processing time / å‡¦ç†æ™‚é–“: {simple_stats.total_processing_time:.3f}s")
    print(f"   - Pipeline stages / ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¸: {simple_stats.pipeline_stages_executed}")
    
    # 2. Semantic RAG
    print("\nğŸ“Œ 2. Semantic RAG (Load â†’ Dictionary â†’ Normalize â†’ Chunk â†’ Vector)")
    print("ğŸ“Œ 2. ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯RAGï¼ˆãƒ­ãƒ¼ãƒ‰ â†’ è¾æ›¸ â†’ æ­£è¦åŒ– â†’ ãƒãƒ£ãƒ³ã‚¯ â†’ ãƒ™ã‚¯ãƒˆãƒ«ï¼‰")
    print("-" * 50)
    
    doc_store2 = SQLiteDocumentStore(":memory:")
    vector_store2 = InMemoryVectorStore()
    
    semantic_manager = CorpusManager.create_semantic_rag(doc_store2, vector_store2)
    semantic_stats = semantic_manager.build_corpus(file_paths)
    
    print(f"âœ… Semantic RAG Results / ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯RAGçµæœ:")
    print(f"   - Files processed / å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {semantic_stats.total_files_processed}")
    print(f"   - Documents created / ä½œæˆæ–‡æ›¸æ•°: {semantic_stats.total_documents_created}")
    print(f"   - Chunks created / ä½œæˆãƒãƒ£ãƒ³ã‚¯æ•°: {semantic_stats.total_chunks_created}")
    print(f"   - Processing time / å‡¦ç†æ™‚é–“: {semantic_stats.total_processing_time:.3f}s")
    print(f"   - Enhanced with / å¼·åŒ–æ©Ÿèƒ½: Domain-specific dictionary / ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰è¾æ›¸")
    
    # 3. Knowledge RAG
    print("\nğŸ“Œ 3. Knowledge RAG (Load â†’ Dictionary â†’ Graph â†’ Normalize â†’ Chunk â†’ Vector)")
    print("ğŸ“Œ 3. ãƒŠãƒ¬ãƒƒã‚¸RAGï¼ˆãƒ­ãƒ¼ãƒ‰ â†’ è¾æ›¸ â†’ ã‚°ãƒ©ãƒ• â†’ æ­£è¦åŒ– â†’ ãƒãƒ£ãƒ³ã‚¯ â†’ ãƒ™ã‚¯ãƒˆãƒ«ï¼‰")
    print("-" * 50)
    
    doc_store3 = SQLiteDocumentStore(":memory:")
    vector_store3 = InMemoryVectorStore()
    
    knowledge_manager = CorpusManager.create_knowledge_rag(doc_store3, vector_store3)
    knowledge_stats = knowledge_manager.build_corpus(file_paths)
    
    print(f"âœ… Knowledge RAG Results / ãƒŠãƒ¬ãƒƒã‚¸RAGçµæœ:")
    print(f"   - Files processed / å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {knowledge_stats.total_files_processed}")
    print(f"   - Documents created / ä½œæˆæ–‡æ›¸æ•°: {knowledge_stats.total_documents_created}")
    print(f"   - Chunks created / ä½œæˆãƒãƒ£ãƒ³ã‚¯æ•°: {knowledge_stats.total_chunks_created}")
    print(f"   - Processing time / å‡¦ç†æ™‚é–“: {knowledge_stats.total_processing_time:.3f}s")
    print(f"   - Enhanced with / å¼·åŒ–æ©Ÿèƒ½: Dictionary + Knowledge Graph / è¾æ›¸ + çŸ¥è­˜ã‚°ãƒ©ãƒ•")


def demonstrate_stage_selection(temp_dir: Path, file_paths: List[str]):
    """
    Demonstrate stage selection approach
    ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    
    print("\n" + "="*60)
    print("ğŸ›ï¸  STAGE SELECTION DEMONSTRATION")
    print("ğŸ›ï¸  ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("="*60)
    
    doc_store = SQLiteDocumentStore(":memory:")
    vector_store = InMemoryVectorStore()
    corpus_manager = CorpusManager(doc_store, vector_store)
    
    print("\nğŸ“Œ Custom Stage Selection: Load â†’ Dictionary â†’ Chunk â†’ Vector")
    print("ğŸ“Œ ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠ: ãƒ­ãƒ¼ãƒ‰ â†’ è¾æ›¸ â†’ ãƒãƒ£ãƒ³ã‚¯ â†’ ãƒ™ã‚¯ãƒˆãƒ«")
    print("-" * 50)
    
    # Configure individual stages / å€‹ã€…ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’è¨­å®š
    stage_configs = {
        "loader_config": LoaderConfig(),
        "dictionary_config": DictionaryMakerConfig(
            dictionary_file_path=str(temp_dir / "tutorial_dictionary.md"),
            focus_on_technical_terms=True,
            extract_abbreviations=True,
            include_definitions=True
        ),
        "chunker_config": ChunkingConfig(
            chunk_size=256,
            overlap=32,
            split_by_sentence=True
        )
    }
    
    # Execute selected stages only / é¸æŠã—ãŸã‚¹ãƒ†ãƒ¼ã‚¸ã®ã¿ã‚’å®Ÿè¡Œ
    selected_stages = ["load", "dictionary", "chunk", "vector"]
    stage_stats = corpus_manager.build_corpus(
        file_paths=file_paths,
        stages=selected_stages,
        stage_configs=stage_configs
    )
    
    print(f"âœ… Stage Selection Results / ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠçµæœ:")
    print(f"   - Selected stages / é¸æŠã‚¹ãƒ†ãƒ¼ã‚¸: {selected_stages}")
    print(f"   - Files processed / å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stage_stats.total_files_processed}")
    print(f"   - Documents created / ä½œæˆæ–‡æ›¸æ•°: {stage_stats.total_documents_created}")
    print(f"   - Chunks created / ä½œæˆãƒãƒ£ãƒ³ã‚¯æ•°: {stage_stats.total_chunks_created}")
    print(f"   - Processing time / å‡¦ç†æ™‚é–“: {stage_stats.total_processing_time:.3f}s")
    print(f"   - Documents by stage / ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥æ–‡æ›¸æ•°: {stage_stats.documents_by_stage}")
    
    # Check generated dictionary / ç”Ÿæˆã•ã‚ŒãŸè¾æ›¸ã‚’ç¢ºèª
    dict_file = temp_dir / "tutorial_dictionary.md"
    if dict_file.exists():
        print(f"\nğŸ“– Generated Dictionary Preview / ç”Ÿæˆã•ã‚ŒãŸè¾æ›¸ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
        content = dict_file.read_text(encoding='utf-8')
        lines = content.split('\n')[:10]  # Show first 10 lines
        for line in lines:
            if line.strip():
                print(f"   {line}")
        total_lines = len(content.split('\n'))
        if total_lines > 10:
            print(f"   ... ({total_lines - 10} more lines / ã•ã‚‰ã«{total_lines - 10}è¡Œ)")


def demonstrate_file_format_support(temp_dir: Path):
    """
    Demonstrate support for different file formats
    ç•°ãªã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®ã‚µãƒãƒ¼ãƒˆã‚’ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    
    print("\n" + "="*60)
    print("ğŸ“ FILE FORMAT SUPPORT DEMONSTRATION")
    print("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚µãƒãƒ¼ãƒˆã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("="*60)
    
    # Create different file format samples / ç•°ãªã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ
    formats_dir = temp_dir / "formats"
    formats_dir.mkdir(exist_ok=True)
    
    # Text file
    (formats_dir / "sample.txt").write_text("This is a text file sample.", encoding='utf-8')
    
    # Markdown file
    (formats_dir / "sample.md").write_text("""
# Markdown Sample
This is a **markdown** file with formatting.
- List item 1
- List item 2
""", encoding='utf-8')
    
    # JSON file
    import json
    json_data = {
        "title": "Sample JSON",
        "content": "This is JSON format data",
        "metadata": {"type": "sample", "version": 1.0}
    }
    (formats_dir / "sample.json").write_text(json.dumps(json_data, ensure_ascii=False), encoding='utf-8')
    
    # CSV file
    (formats_dir / "sample.csv").write_text("""
name,description,category
AI,Artificial Intelligence,Technology
ML,Machine Learning,Technology
DL,Deep Learning,Technology
""", encoding='utf-8')
    
    # HTML file
    (formats_dir / "sample.html").write_text("""
<!DOCTYPE html>
<html>
<head><title>Sample HTML</title></head>
<body>
<h1>HTML Sample</h1>
<p>This is an HTML file example.</p>
</body>
</html>
""", encoding='utf-8')
    
    # Process all formats / ã™ã¹ã¦ã®å½¢å¼ã‚’å‡¦ç†
    doc_store = SQLiteDocumentStore(":memory:")
    vector_store = InMemoryVectorStore()
    manager = CorpusManager.create_simple_rag(doc_store, vector_store)
    
    format_files = list(formats_dir.glob("*"))
    stats = manager.build_corpus([str(f) for f in format_files])
    
    print(f"âœ… Multi-Format Processing Results / è¤‡æ•°å½¢å¼å‡¦ç†çµæœ:")
    print(f"   - Supported formats / ã‚µãƒãƒ¼ãƒˆå½¢å¼: TXT, MD, JSON, CSV, HTML")
    print(f"   - Files processed / å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats.total_files_processed}")
    print(f"   - Documents created / ä½œæˆæ–‡æ›¸æ•°: {stats.total_documents_created}")
    print(f"   - Chunks created / ä½œæˆãƒãƒ£ãƒ³ã‚¯æ•°: {stats.total_chunks_created}")
    
    print(f"\nğŸ“‹ File Details / ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°:")
    for file_path in format_files:
        print(f"   - {file_path.name}: {file_path.stat().st_size} bytes")


def demonstrate_incremental_loading(temp_dir: Path, initial_files: List[str]):
    """
    Demonstrate incremental loading capabilities
    å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ©Ÿèƒ½ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    
    print("\n" + "="*60)
    print("âš¡ INCREMENTAL LOADING DEMONSTRATION")
    print("âš¡ å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("="*60)
    
    # First: Load initial corpus / æœ€åˆï¼šåˆæœŸã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰
    doc_store = SQLiteDocumentStore(str(temp_dir / "incremental.db"))
    vector_store = InMemoryVectorStore()
    manager = CorpusManager.create_simple_rag(doc_store, vector_store)
    
    print("\nğŸ“Œ Step 1: Initial corpus loading / ã‚¹ãƒ†ãƒƒãƒ—1: åˆæœŸã‚³ãƒ¼ãƒ‘ã‚¹ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°")
    initial_stats = manager.build_corpus(initial_files)
    print(f"   - Initial files / åˆæœŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {initial_stats.total_files_processed}")
    print(f"   - Initial chunks / åˆæœŸãƒãƒ£ãƒ³ã‚¯æ•°: {initial_stats.total_chunks_created}")
    
    # Second: Add new files / æ¬¡ã«ï¼šæ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
    new_file = temp_dir / "new_document.txt"
    new_file.write_text("""
æ–°ã—ã„æ–‡æ›¸ã§ã™ã€‚å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
ã“ã®æ–‡æ›¸ã¯åˆæœŸã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆå¾Œã«è¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚

è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰ã®æœ€æ–°æŠ€è¡“ã«ã¤ã„ã¦ï¼š
- BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰
- GPTï¼ˆGenerative Pre-trained Transformerï¼‰
- T5ï¼ˆText-to-Text Transfer Transformerï¼‰
""", encoding='utf-8')
    
    # Third: Incremental update / ç¬¬ä¸‰ï¼šå¢—åˆ†æ›´æ–°
    print("\nğŸ“Œ Step 2: Incremental update / ã‚¹ãƒ†ãƒƒãƒ—2: å¢—åˆ†æ›´æ–°")
    all_files = initial_files + [str(new_file)]
    
    # Use incremental loading / å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
    incremental_stats = manager.build_corpus(
        file_paths=all_files,
        use_incremental=True
    )
    
    print(f"   - Total files after update / æ›´æ–°å¾Œç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {incremental_stats.total_files_processed}")
    print(f"   - New chunks added / æ–°è¦è¿½åŠ ãƒãƒ£ãƒ³ã‚¯æ•°: {incremental_stats.total_chunks_created}")
    print(f"   - Incremental processing time / å¢—åˆ†å‡¦ç†æ™‚é–“: {incremental_stats.total_processing_time:.3f}s")
    print(f"   - Only new/modified files processed / æ–°è¦ãƒ»å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†")


def demonstrate_monitoring_and_statistics(temp_dir: Path, file_paths: List[str]):
    """
    Demonstrate monitoring and statistics features
    ç›£è¦–ã¨çµ±è¨ˆæ©Ÿèƒ½ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    
    print("\n" + "="*60)
    print("ğŸ“Š MONITORING AND STATISTICS DEMONSTRATION")
    print("ğŸ“Š ç›£è¦–ã¨çµ±è¨ˆã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("="*60)
    
    doc_store = SQLiteDocumentStore(":memory:")
    vector_store = InMemoryVectorStore()
    manager = CorpusManager.create_semantic_rag(doc_store, vector_store)
    
    # Build corpus with detailed monitoring / è©³ç´°ãªç›£è¦–ã§ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
    stats = manager.build_corpus(file_paths)
    
    print(f"\nğŸ“ˆ Detailed Processing Statistics / è©³ç´°ãªå‡¦ç†çµ±è¨ˆ:")
    print(f"   - Total files processed / ç·å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats.total_files_processed}")
    print(f"   - Total documents created / ç·ä½œæˆæ–‡æ›¸æ•°: {stats.total_documents_created}")
    print(f"   - Total chunks created / ç·ä½œæˆãƒãƒ£ãƒ³ã‚¯æ•°: {stats.total_chunks_created}")
    print(f"   - Total processing time / ç·å‡¦ç†æ™‚é–“: {stats.total_processing_time:.3f}s")
    print(f"   - Pipeline stages executed / å®Ÿè¡Œãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¸: {stats.pipeline_stages_executed}")
    
    if hasattr(stats, 'documents_by_stage'):
        print(f"   - Documents by stage / ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥æ–‡æ›¸æ•°:")
        for stage, count in stats.documents_by_stage.items():
            print(f"     * {stage}: {count}")
    
    if hasattr(stats, 'errors_encountered'):
        print(f"   - Errors encountered / é­é‡ã‚¨ãƒ©ãƒ¼æ•°: {len(stats.errors_encountered)}")
        if stats.errors_encountered:
            print(f"   - Error details / ã‚¨ãƒ©ãƒ¼è©³ç´°:")
            for error in stats.errors_encountered[:3]:  # Show first 3 errors
                print(f"     * {error}")
    
    # Storage validation / ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æ¤œè¨¼
    print(f"\nğŸ’¾ Storage Validation / ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æ¤œè¨¼:")
    total_docs = doc_store.count_documents() if hasattr(doc_store, 'count_documents') else "N/A"
    total_vectors = vector_store.count() if hasattr(vector_store, 'count') else len(vector_store._vectors)
    
    print(f"   - Documents in DocumentStore / DocumentStoreå†…æ–‡æ›¸æ•°: {total_docs}")
    print(f"   - Vectors in VectorStore / VectorStoreå†…ãƒ™ã‚¯ãƒˆãƒ«æ•°: {total_vectors}")
    
    # Performance metrics / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
    if stats.total_files_processed > 0:
        avg_time_per_file = stats.total_processing_time / stats.total_files_processed
        avg_chunks_per_file = stats.total_chunks_created / stats.total_files_processed
        
        print(f"\nâš¡ Performance Metrics / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™:")
        print(f"   - Average processing time per file / ãƒ•ã‚¡ã‚¤ãƒ«å½“ãŸã‚Šå¹³å‡å‡¦ç†æ™‚é–“: {avg_time_per_file:.3f}s")
        print(f"   - Average chunks per file / ãƒ•ã‚¡ã‚¤ãƒ«å½“ãŸã‚Šå¹³å‡ãƒãƒ£ãƒ³ã‚¯æ•°: {avg_chunks_per_file:.1f}")
        print(f"   - Processing throughput / å‡¦ç†ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {stats.total_files_processed/stats.total_processing_time:.1f} files/sec")


def main():
    """
    Main demonstration function
    ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°
    """
    
    print("ğŸš€ Part 1: Corpus Creation Tutorial")
    print("ğŸš€ Part 1: ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«")
    print("="*60)
    print("Comprehensive demonstration of corpus creation with refinire-rag")
    print("refinire-ragã‚’ä½¿ç”¨ã—ãŸã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆã®åŒ…æ‹¬çš„ãªãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("")
    print("Features demonstrated / ãƒ‡ãƒ¢æ©Ÿèƒ½:")
    print("âœ“ Preset configurations / ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®š")
    print("âœ“ Stage selection / ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠ") 
    print("âœ“ File format support / ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚µãƒãƒ¼ãƒˆ")
    print("âœ“ Incremental loading / å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°")
    print("âœ“ Monitoring & statistics / ç›£è¦–ã¨çµ±è¨ˆ")
    
    # Create temporary directory / ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        # Create sample documents / ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ã‚’ä½œæˆ
        print(f"\nğŸ“ Setup: Creating sample documents in {temp_dir}")
        print(f"ğŸ“ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—: {temp_dir} ã«ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ã‚’ä½œæˆä¸­")
        file_paths = create_sample_documents(temp_dir)
        
        # Demonstration sequence / ãƒ‡ãƒ¢ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
        demonstrate_preset_configurations(temp_dir, file_paths)
        demonstrate_stage_selection(temp_dir, file_paths)
        demonstrate_file_format_support(temp_dir)
        demonstrate_incremental_loading(temp_dir, file_paths)
        demonstrate_monitoring_and_statistics(temp_dir, file_paths)
        
        print("\n" + "="*60)
        print("ğŸ‰ TUTORIAL COMPLETE / ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å®Œäº†")
        print("="*60)
        print("âœ… All corpus creation demonstrations completed successfully!")
        print("âœ… ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆãƒ‡ãƒ¢ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        print("")
        print("ğŸ“š What you learned / å­¦ç¿’å†…å®¹:")
        print("   â€¢ Preset configurations for quick setup / ã‚¯ã‚¤ãƒƒã‚¯ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç”¨ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®š")
        print("   â€¢ Custom stage selection for flexibility / æŸ”è»Ÿæ€§ã®ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠ")
        print("   â€¢ Multi-format file support / è¤‡æ•°å½¢å¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚µãƒãƒ¼ãƒˆ")
        print("   â€¢ Incremental loading for efficiency / åŠ¹ç‡æ€§ã®ãŸã‚ã®å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°")
        print("   â€¢ Comprehensive monitoring / åŒ…æ‹¬çš„ãªç›£è¦–")
        print("")
        print(f"ğŸ“ Generated files available in: {temp_dir}")
        print(f"ğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€: {temp_dir}")
        print("")
        print("Next step / æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("â†’ Part 2: Query Engine Tutorial (æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«)")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Tutorial failed / ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # Cleanup (comment out to inspect generated files)
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã™ã‚‹å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
        # shutil.rmtree(temp_dir)
        pass


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)