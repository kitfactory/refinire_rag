#!/usr/bin/env python3
"""
Performance Analysis Test
Âá¶ÁêÜÊôÇÈñì„ÅÆË©≥Á¥∞ÂàÜÊûê
"""

import os
import sys
import time
from pathlib import Path

# Add src to Python path
sys.path.insert(0, str(Path(__file__).parent / "src"))

# Disable ChromaDB telemetry
os.environ["CHROMA_TELEMETRY_DISABLED"] = "true"
os.environ["ANONYMIZED_TELEMETRY"] = "false"
os.environ["CHROMA_ANALYTICS_ENABLED"] = "false"

# Set environment variables (use in-memory storage to avoid disk I/O issues)
os.environ.setdefault("REFINIRE_RAG_DOCUMENT_STORES", "inmemory")
# os.environ.setdefault("REFINIRE_RAG_SQLITE_DB_PATH", "./business_rag.db")
os.environ.setdefault("REFINIRE_RAG_EMBEDDERS", "openai")
os.environ.setdefault("REFINIRE_RAG_VECTOR_STORES", "chroma")
os.environ.setdefault("REFINIRE_RAG_KEYWORD_STORES", "bm25s_keyword")
os.environ.setdefault("REFINIRE_RAG_RETRIEVERS", "simple,keyword")
os.environ.setdefault("REFINIRE_RAG_RERANKERS", "llm")
os.environ.setdefault("REFINIRE_RAG_LLM_RERANKER_BATCH_SIZE", "15")
os.environ.setdefault("REFINIRE_RAG_SYNTHESIZERS", "answer")
os.environ.setdefault("REFINIRE_RAG_LLM_MODEL", "gpt-4o-mini")

from refinire_rag.application import QueryEngine

def analyze_performance():
    print("üîç Performance Analysis Test")
    print("=" * 50)
    
    # Create QueryEngine (assumes corpus already exists)
    print("\nüìä Creating QueryEngine...")
    start_time = time.time()
    query_engine = QueryEngine()
    init_time = time.time() - start_time
    print(f"   ‚úÖ QueryEngine initialization: {init_time:.3f}s")
    
    # Test query
    test_query = "‰ºöÁ§æ„ÅÆ‰∏ª„Å™‰∫ãÊ•≠ÂÜÖÂÆπ„ÅØ‰Ωï„Åß„Åô„ÅãÔºü"
    print(f"\nüéØ Testing query: {test_query}")
    print("-" * 50)
    
    # Execute query
    result = query_engine.query(test_query, retriever_top_k=6)  # 6 documents to test batch processing
    
    # Display detailed timing
    if result.metadata and 'total_processing_time' in result.metadata:
        total_time = result.metadata['total_processing_time']
        retrieval_time = result.metadata.get('retrieval_time', 0)
        reranking_time = result.metadata.get('reranking_time', 0)
        synthesis_time = result.metadata.get('synthesis_time', 0)
        
        print(f"\nüìà Detailed Performance Breakdown:")
        print(f"   üîç Retrieval Time:  {retrieval_time:.3f}s ({retrieval_time/total_time*100:.1f}%)")
        print(f"   üéØ Reranking Time:  {reranking_time:.3f}s ({reranking_time/total_time*100:.1f}%)")
        print(f"   üí¨ Synthesis Time:  {synthesis_time:.3f}s ({synthesis_time/total_time*100:.1f}%)")
        print(f"   ‚è±Ô∏è  Total Time:     {total_time:.3f}s")
        print(f"   üìä Sources Found:   {len(result.sources)}")
        
        # Component info
        print(f"\nüîß Component Configuration:")
        print(f"   ‚Ä¢ Retrievers: {result.metadata.get('retrievers_used', [])}")
        print(f"   ‚Ä¢ Reranker: {result.metadata.get('reranker_used', 'None')}")
        print(f"   ‚Ä¢ Synthesizer: {result.metadata.get('synthesizer_used', 'None')}")
        
        # Analysis
        print(f"\nüí° Performance Analysis:")
        if reranking_time > total_time * 0.5:
            print(f"   ‚ö†Ô∏è  Reranking is the bottleneck ({reranking_time/total_time*100:.1f}% of total time)")
            print(f"   üí≠ Consider: Increase batch size or optimize LLM reranker")
        
        if synthesis_time > total_time * 0.3:
            print(f"   ‚ö†Ô∏è  Answer synthesis is significant ({synthesis_time/total_time*100:.1f}% of total time)")
            print(f"   üí≠ Consider: Optimize answer generation or reduce context")
        
        if retrieval_time > total_time * 0.3:
            print(f"   ‚ö†Ô∏è  Document retrieval is significant ({retrieval_time/total_time*100:.1f}% of total time)")
            print(f"   üí≠ Consider: Optimize vector/keyword search")
        
        # Recommendations
        print(f"\nüöÄ Optimization Recommendations:")
        
        if reranking_time > 10:  # If reranking takes more than 10 seconds
            print(f"   1. Increase LLM Reranker batch size from 15 to 20-25")
            print(f"   2. Consider using RRF reranker instead of LLM for faster processing")
            print(f"   3. Reduce retriever_top_k to limit documents sent to reranker")
        
        if synthesis_time > 5:  # If synthesis takes more than 5 seconds
            print(f"   4. Optimize answer synthesis prompt length")
            print(f"   5. Consider streaming responses for better perceived performance")
        
        # Batch size analysis
        sources_count = len(result.sources)
        batch_size = int(os.environ.get("REFINIRE_RAG_LLM_RERANKER_BATCH_SIZE", "15"))
        print(f"\nüî¢ Batch Processing Analysis:")
        print(f"   ‚Ä¢ Documents processed: {sources_count}")
        print(f"   ‚Ä¢ Current batch size: {batch_size}")
        
        if sources_count > batch_size:
            batches_needed = (sources_count + batch_size - 1) // batch_size
            print(f"   ‚Ä¢ Batches needed: {batches_needed}")
            print(f"   üí° Suggestion: Increase batch size to {sources_count} for single-batch processing")
    
    else:
        print("   ‚ö†Ô∏è  No detailed timing information available")
    
    print(f"\n‚úÖ Analysis completed")

if __name__ == "__main__":
    try:
        analyze_performance()
    except Exception as e:
        print(f"‚ùå Error during performance analysis: {e}")
        import traceback
        traceback.print_exc()