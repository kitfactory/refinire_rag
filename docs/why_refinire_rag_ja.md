# ãªãœrefinire-ragï¼Ÿæ´—ç·´ã•ã‚ŒãŸRAGãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

LangChainã‚„LlamaIndexãªã©ã®å¾“æ¥ã®RAGãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯å¼·åŠ›ã§ã™ãŒè¤‡é›‘ã§ã™ã€‚refinire-ragã¯æ ¹æœ¬çš„ãªã‚·ãƒ³ãƒ—ãƒ«ã•ã¨ä¼æ¥­ã‚°ãƒ¬ãƒ¼ãƒ‰ã®ç”Ÿç”£æ€§ã§RAGé–‹ç™ºä½“é¨“ã‚’æ´—ç·´ã—ã¾ã™ã€‚

## æ—¢å­˜RAGãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®å•é¡Œç‚¹

### è¤‡é›‘ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçµ„ã¿ç«‹ã¦
```python
# LangChain: åŸºæœ¬RAGæ§‹ç¯‰ã«50è¡Œä»¥ä¸Š
from langchain.document_loaders import PyPDFLoader, TextLoader, CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA

def build_rag_system():
    # æ–‡æ›¸ãƒ­ãƒ¼ãƒ‰
    pdf_loader = PyPDFLoader("docs/manual.pdf")
    text_loader = TextLoader("docs/readme.txt")
    pdf_docs = pdf_loader.load()
    text_docs = text_loader.load()
    all_docs = pdf_docs + text_docs
    
    # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    texts = text_splitter.split_documents(all_docs)
    
    # åŸ‹ã‚è¾¼ã¿ä½œæˆ
    embeddings = OpenAIEmbeddings(
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        model="text-embedding-ada-002"
    )
    
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ§‹ç¯‰
    vectorstore = Chroma.from_documents(
        documents=texts,
        embedding=embeddings,
        persist_directory="./chroma_db"
    )
    
    # QAãƒã‚§ãƒ¼ãƒ³ä½œæˆ
    qa_chain = RetrievalQA.from_chain_type(
        llm=OpenAI(temperature=0),
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
        return_source_documents=True
    )
    
    return qa_chain

# ã¾ã ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã€ç›£è¦–ã€å¢—åˆ†æ›´æ–°ãŒå¿…è¦...
```

### ä¸€è²«æ€§ã®ãªã„API
å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ç•°ãªã‚‹åˆæœŸåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼š
- ãƒ­ãƒ¼ãƒ€ãƒ¼: `PyPDFLoader("file.pdf").load()`
- åˆ†å‰²å™¨: `RecursiveCharacterTextSplitter(chunk_size=1000).split_documents(docs)`
- åŸ‹ã‚è¾¼ã¿: `OpenAIEmbeddings(openai_api_key="...").embed_documents(texts)`
- ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢: `Chroma.from_documents(documents, embeddings)`

### ä¼æ¥­æ©Ÿèƒ½ã®ä¸è¶³
- å¤§è¦æ¨¡æ–‡æ›¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å¢—åˆ†å‡¦ç†ãªã—
- ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ã¯æ‰‹å‹•å®Ÿè£…ãŒå¿…è¦
- ç›£è¦–ãƒ»ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ãŒé™å®šçš„
- æ—¥æœ¬èªæœ€é©åŒ–ã®æ©Ÿèƒ½ãŒå†…è”µã•ã‚Œã¦ã„ãªã„

## refinire-ragã®è§£æ±ºç­–ï¼šæ ¹æœ¬çš„ã‚·ãƒ³ãƒ—ãƒ«åŒ–

### 1. çµ±ä¸€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ = 10å€ã‚·ãƒ³ãƒ—ãƒ«

**ã™ã¹ã¦ã«çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹**
```python
# refinire-rag: åŒã˜RAGã‚·ã‚¹ãƒ†ãƒ ã‚’5è¡Œã§æ§‹ç¯‰
from refinire_rag.application import CorpusManager

# ãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼RAGã‚·ã‚¹ãƒ†ãƒ 
manager = CorpusManager.create_simple_rag(doc_store, vector_store)

# ã™ã¹ã¦ã®æ–‡æ›¸ã‚’å‡¦ç†
results = manager.process_corpus(["documents/"])

# å³åº§ã«ã‚¯ã‚¨ãƒª
answer = query_engine.answer("ã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®šæ–¹æ³•ã¯ï¼Ÿ")
```

**ä¸€è²«ã—ãŸDocumentProcessorãƒ‘ã‚¿ãƒ¼ãƒ³**
```python
# ã™ã¹ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒåŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¾“ã†
processor = SomeProcessor(config)
results = processor.process(document)  # å¸¸ã«åŒã˜ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

# ç°¡å˜ã«ãƒã‚§ãƒ¼ãƒ³åŒ–
pipeline = DocumentPipeline([
    Normalizer(config),      # process(doc) -> [doc]
    Chunker(config),         # process(doc) -> [chunks]  
    VectorStore(config)      # process(doc) -> [embedded_doc]
])
```

### 2. è¨­å®šãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆé–‹ç™º = 5å€é«˜é€Ÿ

**å®£è¨€çš„ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**
```python
# ä½œã‚Šæ–¹ã§ã¯ãªãã€ä½•ã‚’ä½œã‚ŠãŸã„ã‹ã‚’å®šç¾©
config = CorpusManagerConfig(
    document_store=SQLiteDocumentStore("corpus.db"),
    vector_store=InMemoryVectorStore(),
    embedder=TFIDFEmbedder(TFIDFEmbeddingConfig(min_df=1)),
    processors=[
        Normalizer(normalizer_config),
        TokenBasedChunker(chunking_config)
    ],
    enable_progress_reporting=True
)

# ã‚·ã‚¹ãƒ†ãƒ ãŒè‡ªå‹•æ§‹ç¯‰
corpus_manager = CorpusManager(config)
```

**ç’°å¢ƒåˆ¥è¨­å®š**
```yaml
# development.yaml
document_store:
  type: sqlite
  path: dev.db

# production.yaml  
document_store:
  type: postgresql
  connection_string: ${DATABASE_URL}
```

### 3. ãƒ—ãƒªã‚»ãƒƒãƒˆã«ã‚ˆã‚‹å³åº§ã®ç”Ÿç”£æ€§

**æ—¥ã§ã¯ãªãåˆ†ã§é–‹å§‹**
```python
# ç•°ãªã‚‹ç”¨é€”ã«å¿œã˜ãŸå³åº§ã®RAGã‚·ã‚¹ãƒ†ãƒ 
simple_rag = CorpusManager.create_simple_rag(doc_store, vector_store)
semantic_rag = CorpusManager.create_semantic_rag(doc_store, vector_store)  
knowledge_rag = CorpusManager.create_knowledge_rag(doc_store, vector_store)

# å³åº§ã®çµæœ
results = simple_rag.process_corpus(["documents/"])
```

### 4. ä¼æ¥­ã‚°ãƒ¬ãƒ¼ãƒ‰æ©Ÿèƒ½ãŒå†…è”µ

**å¢—åˆ†å‡¦ç†**
```python
# 10,000+æ–‡æ›¸ã‚’åŠ¹ç‡çš„ã«å‡¦ç†
incremental_loader = IncrementalLoader(document_store, cache_file=".cache.json")

# æ–°è¦ãƒ»å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†ï¼ˆ90%ä»¥ä¸Šã®æ™‚é–“å‰Šæ¸›ï¼‰
results = incremental_loader.process_incremental(["documents/"])
print(f"æ–°è¦: {len(results['new'])}, æ›´æ–°: {len(results['updated'])}, ã‚¹ã‚­ãƒƒãƒ—: {len(results['skipped'])}")
```

**éƒ¨ç½²ãƒ¬ãƒ™ãƒ«ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡**
```python
# ä¼æ¥­ãƒ‡ãƒ¼ã‚¿åˆ†é›¢ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«5ã®ä¾‹ï¼‰
# éƒ¨ç½²ã”ã¨ã«åˆ†é›¢ã•ã‚ŒãŸRAGã‚·ã‚¹ãƒ†ãƒ 
hr_rag = CorpusManager.create_simple_rag(hr_doc_store, hr_vector_store)
sales_rag = CorpusManager.create_simple_rag(sales_doc_store, sales_vector_store)

# éƒ¨ç½²å›ºæœ‰ã®æ–‡æ›¸ã‚’å‡¦ç†
hr_rag.process_corpus(["hr_documents/"])
sales_rag.process_corpus(["sales_documents/"])

# éƒ¨ç½²åˆ†é›¢ã•ã‚ŒãŸã‚¯ã‚¨ãƒª
hr_answer = hr_query_engine.answer("ä¼‘æš‡åˆ¶åº¦ã¯ã©ã†ãªã£ã¦ã„ã¾ã™ã‹ï¼Ÿ")
sales_answer = sales_query_engine.answer("å–¶æ¥­ãƒ—ãƒ­ã‚»ã‚¹ã¯ã©ã†ãªã£ã¦ã„ã¾ã™ã‹ï¼Ÿ")
```

**æœ¬ç•ªç›£è¦–**
```python
# åŒ…æ‹¬çš„ãªå¯è¦³æ¸¬æ€§ãŒå†…è”µ
stats = corpus_manager.get_corpus_stats()
print(f"å‡¦ç†æ¸ˆã¿æ–‡æ›¸: {stats['documents_processed']}")
print(f"å‡¦ç†æ™‚é–“: {stats['total_processing_time']:.2f}ç§’")
print(f"ã‚¨ãƒ©ãƒ¼ç‡: {stats['errors']}/{stats['total_documents']}")

# æ–‡æ›¸ç³»è­œè¿½è·¡
lineage = corpus_manager.get_document_lineage("doc_123")
# è¡¨ç¤º: å…ƒæ–‡æ›¸ â†’ æ­£è¦åŒ– â†’ ãƒãƒ£ãƒ³ã‚¯åŒ– â†’ åŸ‹ã‚è¾¼ã¿
```

## å®Ÿä¸–ç•Œã®é–‹ç™ºé€Ÿåº¦æ¯”è¼ƒ

### ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã‹ã‚‰æœ¬ç•ªã¾ã§ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³

| ãƒ•ã‚§ãƒ¼ã‚º | LangChain/LlamaIndex | refinire-rag | æ™‚é–“å‰Šæ¸› |
|----------|---------------------|---------------|----------|
| **ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—** | 2-3æ™‚é–“ | 15åˆ† | **90%** |
| **åŸºæœ¬RAGå®Ÿè£…** | 1-2æ—¥ | 2-3æ™‚é–“ | **85%** |
| **æ—¥æœ¬èªå‡¦ç†** | 3-5æ—¥ï¼ˆã‚«ã‚¹ã‚¿ãƒ ï¼‰ | 1æ™‚é–“ï¼ˆå†…è”µï¼‰ | **95%** |
| **å¢—åˆ†æ›´æ–°** | 1-2é€±é–“ï¼ˆã‚«ã‚¹ã‚¿ãƒ ï¼‰ | 1æ—¥ï¼ˆå†…è”µï¼‰ | **90%** |
| **ä¼æ¥­æ©Ÿèƒ½** | 2-3é€±é–“ï¼ˆã‚«ã‚¹ã‚¿ãƒ ï¼‰ | 2-3æ—¥ï¼ˆè¨­å®šï¼‰ | **85%** |
| **æœ¬ç•ªç›£è¦–** | 1é€±é–“ï¼ˆã‚«ã‚¹ã‚¿ãƒ ï¼‰ | 1æ™‚é–“ï¼ˆå†…è”µï¼‰ | **95%** |
| **ãƒãƒ¼ãƒ çµ±åˆ** | 3-5æ—¥ | 1æ—¥ | **80%** |

**ç·é–‹ç™ºæ™‚é–“: 3ãƒ¶æœˆ â†’ 1.5ãƒ¶æœˆï¼ˆ50%å‰Šæ¸›ï¼‰**

### ã‚³ãƒ¼ãƒ‰è¤‡é›‘æ€§æ¯”è¼ƒ

| æ©Ÿèƒ½ | LangChainè¡Œæ•° | refinire-ragè¡Œæ•° | å‰Šæ¸›ç‡ |
|------|-------------|-----------------|-------|
| åŸºæœ¬RAGã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— | 50+è¡Œ | 5è¡Œ | **90%** |
| æ–‡æ›¸å‡¦ç† | 30+è¡Œ | 3è¡Œ | **90%** |
| è¨­å®šç®¡ç† | 100+è¡Œ | 10è¡Œ | **90%** |
| ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚° | 50+è¡Œ | å†…è”µ | **100%** |
| ç›£è¦– | 200+è¡Œ | å†…è”µ | **100%** |

## æ´—ç·´ã®å“²å­¦

### ã‚ˆã‚Šå°‘ãªã„ã‚³ãƒ¼ãƒ‰ã€ã‚ˆã‚Šå¤šãã®ä¾¡å€¤
```python
# å¾“æ¥: å‘½ä»¤çš„è¤‡é›‘ã•
loader = PyPDFLoader("file.pdf")
docs = loader.load()
splitter = RecursiveCharacterTextSplitter(chunk_size=1000)
chunks = splitter.split_documents(docs)
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings)
retriever = vectorstore.as_retriever()
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

# refinire-rag: å®£è¨€çš„ã‚·ãƒ³ãƒ—ãƒ«ã•
manager = CorpusManager.create_simple_rag(doc_store, vector_store)
manager.process_corpus(["documents/"])
```

### ã‚³ãƒ¼ãƒ‰ã‚ˆã‚Šè¨­å®š
```python
# ã‚³ãƒ¼ãƒ‰æ›¸ãç›´ã—ã§ã¯ãªãè¨­å®šã§å‹•ä½œå¤‰æ›´
config.embedder = OpenAIEmbedder(openai_config)      # OpenAIã«åˆ‡ã‚Šæ›¿ãˆ
config.processors.append(Normalizer(norm_config))    # æ­£è¦åŒ–è¿½åŠ 
config.chunking_config.chunk_size = 300              # ãƒãƒ£ãƒ³ã‚¯èª¿æ•´
```

### ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ãŒå†…è”µ
```python
# ä¼æ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
- å¢—åˆ†å‡¦ç†: æ¨™æº–æ©Ÿèƒ½
- ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡: éƒ¨ç½²åˆ†é›¢ãŒå†…è”µ  
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§çµ±ä¸€
- ç›£è¦–: åŒ…æ‹¬çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä»˜å±
- æ—¥æœ¬èªã‚µãƒãƒ¼ãƒˆ: æœ€é©åŒ–ã•ã‚ŒãŸæ­£è¦åŒ–ãŒä»˜å±
```

## é–‹ç™ºè€…ä½“é¨“ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ

### âš¡ å³åº§ã®æº€è¶³æ„Ÿ
```python
# 30ç§’ã§å‹•ä½œã™ã‚‹RAG
from refinire_rag import create_simple_rag
rag = create_simple_rag("documents/")
answer = rag.query("ã“ã‚Œã¯ä½•ã«ã¤ã„ã¦ï¼Ÿ")
```

### ğŸ”§ ç°¡å˜ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
```python
# è¤‡é›‘ã•ãªã—ã«æ©Ÿèƒ½æ‹¡å¼µ
config.processors.insert(0, CustomProcessor(my_config))
```

### ğŸ› æ¥½ã€…ãƒ‡ãƒãƒƒã‚°
```python
# è±Šå¯Œãªãƒ‡ãƒãƒƒã‚°æƒ…å ±ãŒæ¨™æº–è£…å‚™
pipeline_stats = manager.get_pipeline_stats()
document_lineage = manager.get_document_lineage("doc_id")
error_details = manager.get_error_details()
```

### ğŸš€ ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
```python
# ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã‹ã‚‰æœ¬ç•ªã¾ã§åŒã˜ã‚³ãƒ¼ãƒ‰
if production:
    config.document_store = PostgreSQLDocumentStore(db_url)
    config.vector_store = ChromaVectorStore(persist_dir)
    config.parallel_processing = True
```

## refinire-ragã‚’é¸ã¶ã¹ãå ´åˆ

### âœ… æœ€é©ãªç”¨é€”:
- **ä¼æ¥­RAGã‚·ã‚¹ãƒ†ãƒ ** æœ¬ç•ªå¯¾å¿œãŒå¿…è¦
- **æ—¥æœ¬èªæ–‡æ›¸å‡¦ç†** è¨€èªæœ€é©åŒ–ä»˜ã
- **å¤§è¦æ¨¡ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ** é »ç¹ãªæ–‡æ›¸æ›´æ–°
- **ãƒãƒ¼ãƒ é–‹ç™º** ä¸€è²«ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå¿…è¦
- **è¿…é€Ÿãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°** ã‹ã‚‰æœ¬ç•ªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¾ã§
- **Refinireã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ** çµ±åˆ

### âš ï¸ ä»£æ›¿ã‚’æ¤œè¨ã™ã‚‹å ´åˆ:
- è±Šå¯Œãªã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ä¾‹ãŒã‚ã‚‹ç°¡å˜ãªå®Ÿé¨“
- æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«çµ±åˆãŒå¿…è¦ãªç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
- LangChain/LlamaIndexã®æ·±ã„å°‚é–€çŸ¥è­˜ãŒã‚ã‚‹ãƒãƒ¼ãƒ 

## æ•°åˆ†ã§é–‹å§‹

```bash
pip install refinire-rag
```

```python
from refinire_rag import create_simple_rag

# RAGã‚·ã‚¹ãƒ†ãƒ ã®æº–å‚™å®Œäº†
rag = create_simple_rag("your_documents/")
answer = rag.query("ã“ã‚Œã¯ã©ã†å‹•ãã¾ã™ã‹ï¼Ÿ")
print(answer)
```

**refinire-rag: ä¼æ¥­RAGé–‹ç™ºãŒæ¥½ã€…ã«ãªã‚‹å ´æ‰€ã€‚**

---

*æ´—ç·´ã•ã‚ŒãŸRAGã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰æ–¹æ³•ã‚’ä½“é¨“ã™ã‚‹æº–å‚™ã¯ã§ãã¾ã—ãŸã‹ï¼Ÿ[ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](./tutorials/)ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€ãã®é•ã„ã‚’å®Ÿæ„Ÿã—ã¦ãã ã•ã„ã€‚*