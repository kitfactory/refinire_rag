# KeywordSearch & VectorStore Interface Documentation

## Overview / æ¦‚è¦

This document provides comprehensive documentation for the KeywordSearch and VectorStore interfaces in the refinire-rag system. These interfaces provide unified APIs for document indexing, storage, and retrieval across different search implementations.

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€refinire-ragã‚·ã‚¹ãƒ†ãƒ ã®KeywordSearchã¨VectorStoreã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®åŒ…æ‹¬çš„ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¯ã€ç•°ãªã‚‹æ¤œç´¢å®Ÿè£…ã§ã®æ–‡æ›¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã€æ¤œç´¢ã®ãŸã‚ã®çµ±ä¸€APIã‚’æä¾›ã—ã¾ã™ã€‚

## Architecture Overview / ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

### Interface Hierarchy / ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹éšå±¤

```mermaid
classDiagram
    class DocumentProcessor {
        <<abstract>>
        +process(documents) Iterator[Document]
        +get_processing_stats() Dict
    }
    
    class Indexer {
        <<abstract>>
        +index_document(document) None
        +index_documents(documents) None
        +remove_document(id) bool
        +update_document(document) bool
        +clear_index() None
        +get_document_count() int
    }
    
    class Retriever {
        <<abstract>>
        +retrieve(query, limit, metadata_filter) List[SearchResult]
    }
    
    class KeywordSearch {
        <<abstract>>
        +add_document(document) None
        +search(query, limit) List[SearchResult]
    }
    
    class VectorStore {
        <<abstract>>
        +add_vector(entry) str
        +search_similar(query_vector, limit) List[VectorSearchResult]
        +search_with_text(query_text, limit) List[VectorSearchResult]
    }
    
    DocumentProcessor <|-- KeywordSearch
    DocumentProcessor <|-- VectorStore
    Indexer <|-- KeywordSearch
    Indexer <|-- VectorStore
    Retriever <|-- KeywordSearch
    Retriever <|-- VectorStore
    
    KeywordSearch <|-- TFIDFKeywordStore
    VectorStore <|-- InMemoryVectorStore
    VectorStore <|-- PickleVectorStore
```

## KeywordSearch Interface / ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

### Overview / æ¦‚è¦

The `KeywordSearch` interface provides unified access to keyword-based document retrieval systems. It combines `DocumentProcessor`, `Indexer`, and `Retriever` capabilities for complete document processing and search functionality.

`KeywordSearch`ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¯ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æ–‡æ›¸æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã¸ã®çµ±ä¸€ã‚¢ã‚¯ã‚»ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚å®Œå…¨ãªæ–‡æ›¸å‡¦ç†ã¨æ¤œç´¢æ©Ÿèƒ½ã®ãŸã‚ã«`DocumentProcessor`ã€`Indexer`ã€`Retriever`ã®æ©Ÿèƒ½ã‚’çµ„ã¿åˆã‚ã›ã¦ã„ã¾ã™ã€‚

### Class Definition / ã‚¯ãƒ©ã‚¹å®šç¾©

```python
class KeywordSearch(DocumentProcessor, Indexer, Retriever):
    """Base class for keyword-based document search with DocumentProcessor integration"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """Initialize KeywordSearch with DocumentProcessor integration"""
        
    @classmethod
    @abstractmethod
    def get_config_class(cls) -> Type[Dict]:
        """Get the configuration class for this keyword search"""
        
    @abstractmethod
    def add_document(self, document: Document) -> None:
        """Add a document to the store"""
        
    @abstractmethod
    def search(self, query: str, limit: int = 10) -> List[SearchResult]:
        """Search for documents using keyword matching"""
```

### Core Methods / ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰

| Method | Purpose | Input | Output |
|--------|---------|--------|--------|
| `add_document()` | ã‚¹ãƒˆã‚¢ã«æ–‡æ›¸ã‚’è¿½åŠ  | `Document` | `None` |
| `search()` | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã§æ–‡æ›¸æ¤œç´¢ | `query: str, limit: int` | `List[SearchResult]` |
| `retrieve()` | è©³ç´°ãªæ¤œç´¢ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ä»˜ãï¼‰ | `query: str, limit: int, metadata_filter: Dict` | `List[SearchResult]` |
| `index_document()` | å˜ä¸€æ–‡æ›¸ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ | `Document` | `None` |
| `index_documents()` | è¤‡æ•°æ–‡æ›¸ã‚’åŠ¹ç‡çš„ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ | `List[Document]` | `None` |
| `remove_document()` | ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰æ–‡æ›¸ã‚’å‰Šé™¤ | `document_id: str` | `bool` |
| `update_document()` | ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ—¢å­˜æ–‡æ›¸ã‚’æ›´æ–° | `Document` | `bool` |
| `clear_index()` | ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰ã™ã¹ã¦ã®æ–‡æ›¸ã‚’å‰Šé™¤ | - | `None` |
| `get_document_count()` | ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†…ã®æ–‡æ›¸æ•°ã‚’å–å¾— | - | `int` |

### Usage Example / ä½¿ç”¨ä¾‹

```python
from refinire_rag.keywordstore import TFIDFKeywordStore
from refinire_rag.models.document import Document

# Initialize keyword store
keyword_store = TFIDFKeywordStore()

# Add documents
documents = [
    Document(id="doc1", content="Machine learning is a subset of artificial intelligence"),
    Document(id="doc2", content="Deep learning uses neural networks with multiple layers"),
    Document(id="doc3", content="Natural language processing deals with text understanding")
]

# Index documents
for doc in documents:
    keyword_store.add_document(doc)

# Search documents
results = keyword_store.search("machine learning", limit=5)
for result in results:
    print(f"ID: {result.document_id}, Score: {result.score}")
    print(f"Content: {result.document.content}")
```

### TFIDFKeywordStore Implementation / TFIDFå®Ÿè£…

The default implementation uses TF-IDF (Term Frequency-Inverse Document Frequency) algorithm:

```python
class TFIDFKeywordStore(KeywordSearch):
    """TF-IDF based keyword search implementation"""
    
    def __init__(self, config: Optional[PluginConfig] = None):
        """Initialize TF-IDF keyword store"""
        
    def retrieve(self, query: str, limit: Optional[int] = None,
                metadata_filter: Optional[Dict[str, Any]] = None) -> List[SearchResult]:
        """Retrieve relevant documents using TF-IDF keyword search"""
        
    def index_document(self, document: Document) -> None:
        """Index a single document for TF-IDF search"""
```

**Features:**
- scikit-learn's TfidfVectorizer integration
- Cosine similarity scoring
- Metadata filtering support
- Configurable similarity thresholds
- Automatic index rebuilding

## VectorStore Interface / ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

### Overview / æ¦‚è¦

The `VectorStore` interface provides unified access to vector-based document storage and similarity search systems. It combines embedding generation, vector storage, and semantic search capabilities.

`VectorStore`ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¯ã€ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã®æ–‡æ›¸ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã¨é¡ä¼¼æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã¸ã®çµ±ä¸€ã‚¢ã‚¯ã‚»ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã€ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢æ©Ÿèƒ½ã‚’çµ„ã¿åˆã‚ã›ã¦ã„ã¾ã™ã€‚

### Class Definition / ã‚¯ãƒ©ã‚¹å®šç¾©

```python
class VectorStore(DocumentProcessor):
    """Abstract base class for vector storage, retrieval, and indexing"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """Initialize VectorStore with DocumentProcessor integration"""
        
    def set_embedder(self, embedder):
        """Set the embedder for this vector store"""
        
    @abstractmethod
    def add_vector(self, entry: VectorEntry) -> str:
        """Add a vector entry to the store"""
        
    @abstractmethod
    def search_similar(self, query_vector: np.ndarray, limit: int = 10,
                      threshold: Optional[float] = None,
                      filters: Optional[Dict[str, Any]] = None) -> List[VectorSearchResult]:
        """Search for similar vectors"""
```

### Core Data Types / ä¸»è¦ãƒ‡ãƒ¼ã‚¿å‹

#### VectorEntry

```python
@dataclass
class VectorEntry:
    """Represents a document with its embedding vector"""
    document_id: str
    content: str
    embedding: np.ndarray
    metadata: Dict[str, Any]
```

#### VectorSearchResult

```python
@dataclass
class VectorSearchResult:
    """Result from vector similarity search"""
    document_id: str
    content: str
    metadata: Dict[str, Any]
    score: float
    embedding: Optional[np.ndarray] = None
```

#### VectorStoreStats

```python
@dataclass
class VectorStoreStats:
    """Statistics for vector store"""
    total_vectors: int
    vector_dimension: int
    storage_size_bytes: int
    index_type: str = "exact"
```

### Core Methods / ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰

| Method | Purpose | Input | Output |
|--------|---------|--------|--------|
| `add_vector()` | ãƒ™ã‚¯ãƒˆãƒ«ã‚¨ãƒ³ãƒˆãƒªã‚’ã‚¹ãƒˆã‚¢ã«è¿½åŠ  | `VectorEntry` | `str` |
| `add_vectors()` | è¤‡æ•°ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ  | `List[VectorEntry]` | `List[str]` |
| `get_vector()` | æ–‡æ›¸IDã§ãƒ™ã‚¯ãƒˆãƒ«ã‚¨ãƒ³ãƒˆãƒªã‚’å–å¾— | `document_id: str` | `Optional[VectorEntry]` |
| `update_vector()` | æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¨ãƒ³ãƒˆãƒªã‚’æ›´æ–° | `VectorEntry` | `bool` |
| `delete_vector()` | æ–‡æ›¸IDã§ãƒ™ã‚¯ãƒˆãƒ«ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤ | `document_id: str` | `bool` |
| `search_similar()` | é¡ä¼¼ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ¤œç´¢ | `query_vector, limit, threshold, filters` | `List[VectorSearchResult]` |
| `search_with_text()` | ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã§æ–‡æ›¸ã‚’æ¤œç´¢ | `query_text, limit, threshold, filters` | `List[VectorSearchResult]` |
| `search_by_metadata()` | ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ã§ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ¤œç´¢ | `filters, limit` | `List[VectorSearchResult]` |
| `count_vectors()` | ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ•ã‚£ãƒ«ã‚¿ã«ä¸€è‡´ã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«æ•°ã‚’å–å¾— | `Optional[filters]` | `int` |
| `get_stats()` | ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢çµ±è¨ˆã‚’å–å¾— | - | `VectorStoreStats` |
| `clear()` | ã‚¹ãƒˆã‚¢ã‹ã‚‰ã™ã¹ã¦ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã‚¯ãƒªã‚¢ | - | `bool` |

### Convenience Methods / ä¾¿åˆ©ãƒ¡ã‚½ãƒƒãƒ‰

| Method | Purpose | Input | Output |
|--------|---------|--------|--------|
| `add_documents_with_embeddings()` | æ–‡æ›¸ã¨åŸ‹ã‚è¾¼ã¿ã‚’ä¸€ç·’ã«è¿½åŠ  | `documents, embeddings` | `List[str]` |
| `search_similar_to_document()` | æŒ‡å®šæ–‡æ›¸ã«é¡ä¼¼ã™ã‚‹æ–‡æ›¸ã‚’æ¤œç´¢ | `document_id, limit, exclude_self` | `List[VectorSearchResult]` |
| `get_vector_dimension()` | ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°ã‚’å–å¾— | - | `Optional[int]` |

### Usage Example / ä½¿ç”¨ä¾‹

```python
from refinire_rag.storage.vector_store import VectorEntry
from refinire_rag.storage.in_memory_vector_store import InMemoryVectorStore
from refinire_rag.embedding.openai_embedder import OpenAIEmbedder
import numpy as np

# Initialize vector store and embedder
vector_store = InMemoryVectorStore()
embedder = OpenAIEmbedder()
vector_store.set_embedder(embedder)

# Create vector entries
entry1 = VectorEntry(
    document_id="doc1",
    content="Artificial intelligence and machine learning",
    embedding=embedder.embed_text("Artificial intelligence and machine learning"),
    metadata={"category": "AI", "year": 2024}
)

# Add vectors
vector_store.add_vector(entry1)

# Search with text
results = vector_store.search_with_text("deep learning neural networks", limit=5)
for result in results:
    print(f"ID: {result.document_id}, Score: {result.score}")
    print(f"Content: {result.content}")

# Search with vector
query_embedding = embedder.embed_text("machine learning algorithms")
results = vector_store.search_similar(query_embedding, limit=3)

# Search with metadata filters
filtered_results = vector_store.search_with_text(
    "AI research",
    limit=10,
    filters={"category": "AI", "year": 2024}
)
```

### Available Implementations / åˆ©ç”¨å¯èƒ½ãªå®Ÿè£…

#### InMemoryVectorStore

Fast in-memory vector storage with exact similarity search:

```python
class InMemoryVectorStore(VectorStore):
    """In-memory vector storage with exact similarity search"""
    
    def __init__(self, similarity_metric: str = "cosine", config: Optional[Dict] = None):
        """Initialize with cosine, euclidean, or dot product similarity"""
```

**Features:**
- Fast exact similarity search
- Multiple similarity metrics (cosine, euclidean, dot product)
- Good for development and small datasets
- No persistence (data lost on restart)

#### PickleVectorStore

Persistent file-based vector storage:

```python
class PickleVectorStore(VectorStore):
    """File-based vector storage using pickle serialization"""
    
    def __init__(self, file_path: str = "./vectors.pkl", config: Optional[Dict] = None):
        """Initialize with file-based persistence"""
```

**Features:**
- Persistent storage using pickle serialization
- Automatic save/load on startup
- Good for development and medium datasets
- Simple file-based backup and restore

## DocumentProcessor Integration / DocumentProcessorçµ±åˆ

Both KeywordSearch and VectorStore implement the `DocumentProcessor` interface, allowing them to be used in processing pipelines:

```python
from refinire_rag.processing.document_pipeline import DocumentPipeline
from refinire_rag.keywordstore import TFIDFKeywordStore
from refinire_rag.storage.in_memory_vector_store import InMemoryVectorStore

# Create processing pipeline
pipeline = DocumentPipeline()

# Add keyword indexing
keyword_store = TFIDFKeywordStore()
pipeline.add_processor(keyword_store)

# Add vector indexing
vector_store = InMemoryVectorStore()
vector_store.set_embedder(embedder)
pipeline.add_processor(vector_store)

# Process documents through both indexers
documents = [...]
processed_docs = pipeline.process(documents)

# Both stores now contain indexed documents
keyword_results = keyword_store.search("query text")
vector_results = vector_store.search_with_text("query text")
```

## Configuration / è¨­å®š

### Environment Variables / ç’°å¢ƒå¤‰æ•°

| Variable | Description | Default | Importance |
|----------|-------------|---------|-----------|
| `REFINIRE_RAG_TFIDF_KEYWORD_STORE_TOP_K` | Default top-K for TF-IDF search | "10" | ğŸŸ¡ Important |
| `REFINIRE_RAG_TFIDF_KEYWORD_STORE_SIMILARITY_THRESHOLD` | Minimum similarity threshold | "0.0" | ğŸŸ¢ Optional |
| `REFINIRE_RAG_VECTOR_STORE_SIMILARITY_METRIC` | Similarity metric for vector search | "cosine" | ğŸŸ¢ Optional |
| `REFINIRE_RAG_VECTOR_STORE_DEFAULT_LIMIT` | Default search result limit | "10" | ğŸŸ¢ Optional |
| `REFINIRE_RAG_VECTOR_STORE_CACHE_SIZE` | Vector cache size | "1000" | ğŸŸ¢ Optional |

### Configuration Examples / è¨­å®šä¾‹

```python
# TF-IDF Configuration
tfidf_config = {
    "top_k": 20,
    "similarity_threshold": 0.1,
    "enable_filtering": True,
    "max_features": 10000,
    "ngram_range": (1, 2)
}
keyword_store = TFIDFKeywordStore(PluginConfig().for_plugin_type("keyword_store", tfidf_config))

# Vector Store Configuration
vector_config = {
    "similarity_metric": "cosine",
    "default_limit": 15,
    "auto_normalize": True
}
vector_store = InMemoryVectorStore(config=vector_config)
```

## Performance Characteristics / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰¹æ€§

### KeywordSearch Performance / ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

| Aspect | TFIDFKeywordStore | Characteristics |
|--------|-------------------|-----------------|
| **Indexing Speed** | Medium | Depends on vocabulary size |
| **Search Speed** | Fast | O(log n) with sparse matrices |
| **Memory Usage** | Medium | Stores TF-IDF matrix |
| **Accuracy** | Good | Exact keyword matching |
| **Best Use Cases** | Keyword search, filtering | Document categorization |

### VectorStore Performance / ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

| Aspect | InMemoryVectorStore | PickleVectorStore |
|--------|-------------------|-------------------|
| **Indexing Speed** | Very Fast | Medium (I/O bound) |
| **Search Speed** | Very Fast | Fast |
| **Memory Usage** | High | Medium |
| **Persistence** | None | File-based |
| **Scalability** | Limited by RAM | Limited by disk |
| **Best Use Cases** | Development, small datasets | Medium datasets, persistence |

## Common Patterns / ä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³

### Hybrid Search / ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢

Combine keyword and vector search for better results:

```python
def hybrid_search(query: str, keyword_store: KeywordSearch, 
                 vector_store: VectorStore, limit: int = 10):
    """Combine keyword and vector search results"""
    
    # Get keyword results
    keyword_results = keyword_store.search(query, limit=limit)
    
    # Get vector results
    vector_results = vector_store.search_with_text(query, limit=limit)
    
    # Combine and re-rank results
    combined_results = []
    seen_docs = set()
    
    # Add keyword results with boost
    for result in keyword_results:
        if result.document_id not in seen_docs:
            result.score = result.score * 1.2  # Boost keyword matches
            combined_results.append(result)
            seen_docs.add(result.document_id)
    
    # Add vector results
    for result in vector_results:
        if result.document_id not in seen_docs:
            combined_results.append(result)
            seen_docs.add(result.document_id)
    
    # Sort by score and return top results
    combined_results.sort(key=lambda x: x.score, reverse=True)
    return combined_results[:limit]
```

### Batch Processing / ãƒãƒƒãƒå‡¦ç†

Efficiently process large document collections:

```python
def batch_index_documents(documents: List[Document], 
                         keyword_store: KeywordSearch,
                         vector_store: VectorStore,
                         batch_size: int = 100):
    """Index documents in batches for better performance"""
    
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]
        
        # Index in keyword store
        keyword_store.index_documents(batch)
        
        # Index in vector store
        vector_store.index_documents(batch)
        
        print(f"Processed batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}")
```

### Filtered Search / ãƒ•ã‚£ãƒ«ã‚¿æ¤œç´¢

Use metadata filters for constrained search:

```python
def search_with_filters(query: str, store: Union[KeywordSearch, VectorStore],
                       category: str = None, date_range: tuple = None):
    """Search with metadata filters"""
    
    filters = {}
    if category:
        filters["category"] = category
    if date_range:
        filters["date"] = {"$gte": date_range[0], "$lte": date_range[1]}
    
    if isinstance(store, KeywordSearch):
        return store.retrieve(query, metadata_filter=filters)
    else:
        return store.search_with_text(query, filters=filters)
```

## Error Handling / ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### Common Exceptions / ä¸€èˆ¬çš„ãªä¾‹å¤–

```python
from refinire_rag.exceptions import StorageError

try:
    # Index documents
    keyword_store.index_documents(documents)
    vector_store.index_documents(documents)
    
except StorageError as e:
    logger.error(f"Storage error during indexing: {e}")
    # Handle storage-specific errors
    
except ValueError as e:
    logger.error(f"Invalid data during indexing: {e}")
    # Handle data validation errors
    
except Exception as e:
    logger.error(f"Unexpected error during indexing: {e}")
    # Handle unexpected errors
```

### Retry Logic / ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯

```python
import time
from typing import Callable, Any

def retry_operation(operation: Callable, max_retries: int = 3, 
                   delay: float = 1.0) -> Any:
    """Retry operation with exponential backoff"""
    
    for attempt in range(max_retries):
        try:
            return operation()
        except Exception as e:
            if attempt == max_retries - 1:
                raise e
            
            wait_time = delay * (2 ** attempt)
            logger.warning(f"Operation failed (attempt {attempt + 1}), "
                          f"retrying in {wait_time}s: {e}")
            time.sleep(wait_time)
```

## Next: Plugin Development Guide / æ¬¡ï¼šãƒ—ãƒ©ã‚°ã‚¤ãƒ³é–‹ç™ºã‚¬ã‚¤ãƒ‰

For information on creating custom KeywordSearch and VectorStore implementations, see:
- [Plugin Development Guide](./plugin_development_guide.md)
- [Custom Keyword Store Implementation](./custom_keyword_store_guide.md)
- [Custom Vector Store Implementation](./custom_vector_store_guide.md)