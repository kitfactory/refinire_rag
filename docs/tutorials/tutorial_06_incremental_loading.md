# Tutorial 6: Incremental Document Loading

This tutorial demonstrates how to efficiently manage large document collections using incremental loading capabilities, building on the enterprise RAG system from Tutorial 5.

## Overview

Incremental loading allows you to:
- Process only new and updated documents
- Skip unchanged files for efficient updates
- Handle large document repositories efficiently
- Maintain document lineage and history

## Prerequisites

Complete Tutorial 5 (Enterprise RAG Usage) as we'll extend that enterprise system with incremental loading capabilities.

## Implementation

### Step 1: Enhanced Enterprise RAG with Incremental Loading

```python
#!/usr/bin/env python3
"""
Tutorial 6: Incremental Document Loading
ä¼æ¥­ç’°å¢ƒã§ã®RAGã‚·ã‚¹ãƒ†ãƒ ã«å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ©Ÿèƒ½ã‚’è¿½åŠ 
"""

import sys
import os
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from refinire_rag.loaders.incremental_loader import IncrementalLoader
from refinire_rag.application.corpus_manager import CorpusManager, CorpusManagerConfig
from refinire_rag.application.query_engine import QueryEngine, QueryEngineConfig
from refinire_rag.storage import SQLiteDocumentStore, InMemoryVectorStore
from refinire_rag.embedding import TFIDFEmbedder, TFIDFEmbeddingConfig
from refinire_rag.retrieval import SimpleRetriever, SimpleReranker, SimpleReader
from refinire_rag.processing import Normalizer, NormalizerConfig, TokenBasedChunker, ChunkingConfig
from refinire_rag.models.document import Document


class IncrementalEnterpriseRAG:
    """
    Enterprise RAG system with incremental loading capabilities
    å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œã®ä¼æ¥­RAGã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self, department: str, base_dir: Path):
        self.department = department
        self.base_dir = base_dir
        self.docs_dir = base_dir / f"{department}_docs"
        self.db_path = str(base_dir / f"{department}_rag.db")
        self.cache_path = str(base_dir / f".{department}_cache.json")
        
        # Create department directory
        self.docs_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize components
        self._setup_components()
        
        # Statistics
        self.stats = {
            "total_documents": 0,
            "last_update": None,
            "incremental_runs": 0,
            "total_processing_time": 0.0
        }
    
    def _setup_components(self):
        """Initialize RAG components"""
        
        # Storage
        self.document_store = SQLiteDocumentStore(self.db_path)
        self.vector_store = InMemoryVectorStore(similarity_metric="cosine")
        
        # Embedding
        embedder_config = TFIDFEmbeddingConfig(min_df=1, max_df=1.0, max_features=5000)
        self.embedder = TFIDFEmbedder(config=embedder_config)
        
        # Processing components
        normalizer_config = NormalizerConfig(
            dictionary_file_path=str(self.base_dir / f"{self.department}_dictionary.md"),
            whole_word_only=False
        )
        
        chunking_config = ChunkingConfig(
            chunk_size=300,
            overlap=30,
            split_by_sentence=True
        )
        
        # Corpus manager with processors
        corpus_config = CorpusManagerConfig(
            document_store=self.document_store,
            vector_store=self.vector_store,
            embedder=self.embedder,
            processors=[
                Normalizer(normalizer_config),
                TokenBasedChunker(chunking_config)
            ],
            enable_progress_reporting=True
        )
        
        self.corpus_manager = CorpusManager(corpus_config)
        
        # Query engine
        retriever = SimpleRetriever(self.vector_store, self.embedder)
        reranker = SimpleReranker()
        reader = SimpleReader()
        
        query_config = QueryEngineConfig(
            retriever=retriever,
            reranker=reranker,
            reader=reader,
            normalizer=Normalizer(normalizer_config)
        )
        
        self.query_engine = QueryEngine(query_config)
        
        # Incremental loader
        self.incremental_loader = IncrementalLoader(
            document_store=self.document_store,
            base_loader=self.corpus_manager._loader,
            cache_file=self.cache_path
        )
    
    def add_documents(self, documents: Dict[str, str], force_update: bool = False):
        """
        Add or update documents in the department's collection
        
        Args:
            documents: Dict of filename -> content
            force_update: Force update even if file hasn't changed
        """
        print(f"\\nğŸ“ Adding documents to {self.department} department...")
        
        # Write documents to files
        for filename, content in documents.items():
            file_path = self.docs_dir / filename
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"   Created: {filename}")
        
        # Process incrementally
        start_time = time.time()
        
        force_reload = set()
        if force_update:
            force_reload = {str(self.docs_dir / filename) for filename in documents.keys()}
        
        results = self.incremental_loader.process_incremental(
            sources=[self.docs_dir],
            force_reload=force_reload
        )
        
        processing_time = time.time() - start_time
        
        # Fit embedder if we have new documents
        all_docs = results['new'] + results['updated']
        if all_docs and not self.embedder.is_fitted():
            print(f"   Fitting embedder on {len(all_docs)} documents...")
            texts = [doc.content for doc in all_docs if doc.content.strip()]
            if texts:
                self.embedder.fit(texts)
        
        # Process through corpus manager pipeline
        if all_docs:
            print(f"   Processing {len(all_docs)} documents through RAG pipeline...")
            corpus_results = self.corpus_manager.process_documents(all_docs)
            
            # Generate embeddings and store
            embedded_docs = self.corpus_manager.embed_documents(corpus_results)
            print(f"   Generated embeddings for {len(embedded_docs)} documents")
        
        # Update statistics
        self.stats["total_documents"] = self.document_store.get_stats().total_documents
        self.stats["last_update"] = datetime.now().isoformat()
        self.stats["incremental_runs"] += 1
        self.stats["total_processing_time"] += processing_time
        
        # Report results
        print(f"\\nğŸ“Š Processing Results:")
        print(f"   New documents: {len(results['new'])}")
        print(f"   Updated documents: {len(results['updated'])}")
        print(f"   Skipped documents: {len(results['skipped'])}")
        print(f"   Processing time: {processing_time:.2f}s")
        
        return results
    
    def update_document(self, filename: str, new_content: str):
        """Update a specific document"""
        print(f"\\nğŸ“ Updating document: {filename}")
        
        file_path = self.docs_dir / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        # Process the update
        results = self.add_documents({filename: new_content})
        return results
    
    def delete_document(self, filename: str):
        """Delete a document and clean up"""
        print(f"\\nğŸ—‘ï¸ Deleting document: {filename}")
        
        file_path = self.docs_dir / filename
        if file_path.exists():
            file_path.unlink()
            print(f"   Deleted file: {filename}")
            
            # Clean up from stores
            deleted_docs = self.incremental_loader.cleanup_deleted_files([self.docs_dir])
            print(f"   Cleaned up {len(deleted_docs)} documents from stores")
            
            return deleted_docs
        else:
            print(f"   File not found: {filename}")
            return []
    
    def query(self, question: str) -> Dict[str, Any]:
        """Query the department's RAG system"""
        return self.query_engine.answer(question)
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get comprehensive system statistics"""
        cache_stats = self.incremental_loader.get_cache_stats()
        store_stats = self.document_store.get_stats()
        
        return {
            **self.stats,
            "cache_statistics": cache_stats,
            "document_store": {
                "total_documents": store_stats.total_documents,
                "db_path": self.db_path
            },
            "vector_store": {
                "total_vectors": len(self.vector_store.vectors),
                "dimension": getattr(self.vector_store, 'dimension', 'Unknown')
            }
        }


def demo_incremental_enterprise_rag():
    """Demonstrate incremental loading in enterprise RAG"""
    
    print("ğŸ¢ Tutorial 6: Incremental Document Loading")
    print("=" * 60)
    
    # Setup demo environment
    demo_dir = Path("enterprise_incremental_demo")
    demo_dir.mkdir(exist_ok=True)
    
    try:
        # Create HR department RAG system
        hr_rag = IncrementalEnterpriseRAG("hr", demo_dir)
        
        print("\\nğŸ¯ Step 1: Initial Document Loading")
        print("-" * 40)
        
        # Initial documents
        initial_docs = {
            "employee_handbook.md": '''
# å¾“æ¥­å“¡ãƒãƒ³ãƒ‰ãƒ–ãƒƒã‚¯ v1.0

## å‹¤å‹™æ™‚é–“
- å¹³æ—¥: 9:00-18:00
- æ˜¼ä¼‘ã¿: 12:00-13:00

## ä¼‘æš‡åˆ¶åº¦
- å¹´æ¬¡æœ‰çµ¦ä¼‘æš‡: 20æ—¥
- å¤å­£ä¼‘æš‡: 3æ—¥
- å¹´æœ«å¹´å§‹: 5æ—¥

## ç¦åˆ©åšç”Ÿ
- å¥åº·ä¿é™º
- åšç”Ÿå¹´é‡‘
- é›‡ç”¨ä¿é™º
            ''',
            
            "remote_work_policy.md": '''
# ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯è¦å®š

## å¯¾è±¡è€…
æ­£ç¤¾å“¡ãŠã‚ˆã³å¥‘ç´„ç¤¾å“¡

## ç”³è«‹æ‰‹ç¶šã
1. ä¸Šå¸ã«äº‹å‰ç”³è«‹
2. äººäº‹éƒ¨æ‰¿èª
3. å®Ÿæ–½å ±å‘Š

## å‹¤å‹™ç’°å¢ƒ
- å®‰å®šã—ãŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š
- é©åˆ‡ãªä½œæ¥­ã‚¹ãƒšãƒ¼ã‚¹
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–
            '''
        }
        
        # Add initial documents
        results1 = hr_rag.add_documents(initial_docs)
        
        print("\\nğŸ” Step 2: Test Initial Query")
        print("-" * 40)
        
        result = hr_rag.query("ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã®ç”³è«‹æ‰‹ç¶šãã¯ï¼Ÿ")
        print(f"è³ªå•: ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã®ç”³è«‹æ‰‹ç¶šãã¯ï¼Ÿ")
        print(f"å›ç­”: {result['answer']}")
        
        print("\\nâ±ï¸ Step 3: Incremental Update (No Changes)")
        print("-" * 40)
        
        # Run again with no changes - should skip all
        results2 = hr_rag.add_documents({})
        
        print("\\nğŸ“ Step 4: Update Existing Document")
        print("-" * 40)
        
        # Update employee handbook
        updated_handbook = '''
# å¾“æ¥­å“¡ãƒãƒ³ãƒ‰ãƒ–ãƒƒã‚¯ v2.0

## å‹¤å‹™æ™‚é–“
- å¹³æ—¥: 9:00-17:30 (å¤‰æ›´)
- æ˜¼ä¼‘ã¿: 12:00-13:00

## ä¼‘æš‡åˆ¶åº¦
- å¹´æ¬¡æœ‰çµ¦ä¼‘æš‡: 25æ—¥ (æ”¹å–„)
- å¤å­£ä¼‘æš‡: 5æ—¥ (æ”¹å–„)
- å¹´æœ«å¹´å§‹: 5æ—¥
- ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ä¼‘æš‡: 3æ—¥ (æ–°è¦)

## ç¦åˆ©åšç”Ÿ
- å¥åº·ä¿é™º
- åšç”Ÿå¹´é‡‘
- é›‡ç”¨ä¿é™º
- ä½å®…æ‰‹å½“ (æ–°è¦)

æœ€çµ‚æ›´æ–°: 2024å¹´6æœˆ
        '''
        
        results3 = hr_rag.update_document("employee_handbook.md", updated_handbook)
        
        print("\\nğŸ“„ Step 5: Add New Document")
        print("-" * 40)
        
        new_docs = {
            "performance_review.md": '''
# äººäº‹è©•ä¾¡åˆ¶åº¦

## è©•ä¾¡æœŸé–“
- ä¸ŠåŠæœŸ: 4æœˆ-9æœˆ
- ä¸‹åŠæœŸ: 10æœˆ-3æœˆ

## è©•ä¾¡é …ç›®
1. æ¥­å‹™æˆæœ
2. ãƒ—ãƒ­ã‚»ã‚¹
3. è¡Œå‹•ç‰¹æ€§

## è©•ä¾¡ãƒ—ãƒ­ã‚»ã‚¹
1. è‡ªå·±è©•ä¾¡
2. ä¸Šå¸è©•ä¾¡
3. ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é¢è«‡
4. è©•ä¾¡ç¢ºå®š

## æ˜‡é€²ãƒ»æ˜‡æ ¼
è©•ä¾¡çµæœã«åŸºã¥ãå¹´2å›æ¤œè¨
            '''
        }
        
        results4 = hr_rag.add_documents(new_docs)
        
        print("\\nğŸ” Step 6: Test Updated Knowledge")
        print("-" * 40)
        
        # Test updated information
        questions = [
            "å‹¤å‹™æ™‚é–“ã¯ä½•æ™‚ã¾ã§ã§ã™ã‹ï¼Ÿ",
            "å¹´æ¬¡æœ‰çµ¦ä¼‘æš‡ã¯ä½•æ—¥ã‚‚ã‚‰ãˆã¾ã™ã‹ï¼Ÿ",
            "äººäº‹è©•ä¾¡ã¯ã„ã¤è¡Œã‚ã‚Œã¾ã™ã‹ï¼Ÿ"
        ]
        
        for question in questions:
            result = hr_rag.query(question)
            print(f"\\nè³ªå•: {question}")
            print(f"å›ç­”: {result['answer']}")
        
        print("\\nğŸ—‘ï¸ Step 7: Document Deletion")
        print("-" * 40)
        
        # Delete a document
        deleted = hr_rag.delete_document("remote_work_policy.md")
        
        print("\\nğŸ“Š Step 8: Final Statistics")
        print("-" * 40)
        
        stats = hr_rag.get_statistics()
        print(f"ğŸ“ˆ System Statistics:")
        print(f"   Total documents: {stats['total_documents']}")
        print(f"   Incremental runs: {stats['incremental_runs']}")
        print(f"   Total processing time: {stats['total_processing_time']:.2f}s")
        print(f"   Last update: {stats['last_update']}")
        print(f"   Cache files: {stats['cache_statistics']['total_files']}")
        print(f"   Vector store size: {stats['vector_store']['total_vectors']}")
        
        print("\\nğŸ¯ Benefits of Incremental Loading")
        print("-" * 40)
        print("âœ… Efficient updates - only processes changed files")
        print("âœ… Fast reprocessing - skips unchanged documents")
        print("âœ… Large-scale support - handles thousands of documents")
        print("âœ… Automatic cleanup - removes deleted documents")
        print("âœ… Change detection - multiple verification methods")
        print("âœ… Enterprise ready - production deployment suitable")
        
        print("\\nğŸ’¡ Production Usage Tips")
        print("-" * 40)
        print("1. Run incremental updates on schedule (nightly/hourly)")
        print("2. Monitor cache statistics for optimization")
        print("3. Use force_reload for systematic updates")
        print("4. Implement file watching for real-time updates")
        print("5. Backup cache files for disaster recovery")
        
    except Exception as e:
        print(f"âŒ Error in demo: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # Cleanup option
        cleanup = input("\\nClean up demo files? (y/N): ").lower().strip()
        if cleanup == 'y':
            import shutil
            if demo_dir.exists():
                shutil.rmtree(demo_dir)
            print("âœ… Demo files cleaned up")
        else:
            print(f"ğŸ“ Demo files preserved in: {demo_dir}")
    
    print("\\nğŸ‰ Tutorial 6 Complete!")
    print("\\nNext: Tutorial 7 will cover advanced query optimization and caching")


if __name__ == "__main__":
    demo_incremental_enterprise_rag()