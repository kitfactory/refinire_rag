# ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«3: ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã¨å›ç­”ç”Ÿæˆ

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€QueryEngineã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªã‚¯ã‚¨ãƒªå‡¦ç†ã¨å›ç­”ç”Ÿæˆã‚’å­¦ç¿’ã—ã¾ã™ã€‚

## å­¦ç¿’ç›®æ¨™

- QueryEngineã®åŸºæœ¬æ§‹é€ ã‚’ç†è§£ã™ã‚‹
- ã‚¯ã‚¨ãƒªæ­£è¦åŒ–ã®åŠ¹æœã‚’ä½“é¨“ã™ã‚‹
- Retrieverã€Rerankerã€Readerã®å½¹å‰²ã‚’å­¦ã¶
- å›ç­”å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹è¨­å®šã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹

## QueryEngineã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

QueryEngineã¯ä»¥ä¸‹ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§æ§‹æˆã•ã‚Œã¾ã™ï¼š

```
ã‚¯ã‚¨ãƒª â†’ [æ­£è¦åŒ–] â†’ [æ¤œç´¢] â†’ [å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°] â†’ [å›ç­”ç”Ÿæˆ] â†’ å›ç­”
          â†“        â†“         â†“           â†“
       Normalizer â†’ Retriever â†’ Reranker â†’ Reader
```

### ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å½¹å‰²

1. **Normalizer**: ã‚¯ã‚¨ãƒªã®è¡¨ç¾æºã‚‰ãã‚’çµ±ä¸€
2. **Retriever**: ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦ã«ã‚ˆã‚‹æ–‡æ›¸æ¤œç´¢
3. **Reranker**: æ¤œç´¢çµæœã®é–¢é€£æ€§ã«ã‚ˆã‚‹å†é †åºä»˜ã‘
4. **Reader**: LLMã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆ

## ã‚¹ãƒ†ãƒƒãƒ—1: æ­£è¦åŒ–ã‚³ãƒ¼ãƒ‘ã‚¹ã®æº–å‚™

ã¾ãšã€æ­£è¦åŒ–å‡¦ç†ã‚’å«ã‚€ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ§‹ç¯‰ã—ã¾ã™ï¼š

```python
import tempfile
from pathlib import Path

def setup_normalized_corpus():
    """æ­£è¦åŒ–ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ§‹ç¯‰"""
    
    from refinire_rag.use_cases.corpus_manager_new import CorpusManager
    from refinire_rag.storage.sqlite_store import SQLiteDocumentStore
    from refinire_rag.storage.in_memory_vector_store import InMemoryVectorStore
    from refinire_rag.models.document import Document
    from refinire_rag.embedding import TFIDFEmbedder, TFIDFEmbeddingConfig
    from refinire_rag.storage.vector_store import VectorEntry
    
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    temp_dir = Path(tempfile.mkdtemp())
    
    # è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    dict_file = temp_dir / "query_dictionary.md"
    dict_file.write_text("""# ã‚¯ã‚¨ãƒªæ­£è¦åŒ–è¾æ›¸

## AIæŠ€è¡“ç”¨èª

- **RAG** (Retrieval-Augmented Generation): æ¤œç´¢æ‹¡å¼µç”Ÿæˆ
  - è¡¨ç¾æºã‚‰ã: æ¤œç´¢æ‹¡å¼µç”Ÿæˆ, æ¤œç´¢å¼·åŒ–ç”Ÿæˆ, RAGã‚·ã‚¹ãƒ†ãƒ , æ¤œç´¢æ‹¡å¼µæŠ€è¡“

- **ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢** (Vector Search): ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
  - è¡¨ç¾æºã‚‰ã: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢, æ„å‘³æ¤œç´¢, ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢, æ„å‘³çš„æ¤œç´¢

- **LLM** (Large Language Model): å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«
  - è¡¨ç¾æºã‚‰ã: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«, è¨€èªãƒ¢ãƒ‡ãƒ«, LLMãƒ¢ãƒ‡ãƒ«, å¤§è¦æ¨¡LM

- **ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°** (Chunking): ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
  - è¡¨ç¾æºã‚‰ã: ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°, æ–‡æ›¸åˆ†å‰², ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰², ãƒãƒ£ãƒ³ã‚¯åŒ–
""", encoding='utf-8')
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    documents = [
        Document(
            id="doc1",
            content="""
            æ¤œç´¢æ‹¡å¼µç”Ÿæˆï¼ˆRAGï¼‰ã¯ã€æƒ…å ±æ¤œç´¢ã¨è¨€èªç”Ÿæˆã‚’çµ„ã¿åˆã‚ã›ãŸ
            é©æ–°çš„ãªAIæŠ€è¡“ã§ã™ã€‚å¾“æ¥ã®LLMã®çŸ¥è­˜åˆ¶é™ã‚’å…‹æœã—ã€
            å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ã—ã¦ã€
            ã‚ˆã‚Šæ­£ç¢ºã§æœ€æ–°ã®å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚
            RAGã‚·ã‚¹ãƒ†ãƒ ã¯ä¼æ¥­ã®è³ªç–‘å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ã‚„
            çŸ¥è­˜ç®¡ç†ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§åºƒãæ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
            """,
            metadata={"title": "RAGæŠ€è¡“æ¦‚è¦", "category": "æŠ€è¡“è§£èª¬"}
        ),
        
        Document(
            id="doc2", 
            content="""
            ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯ã€æ–‡æ›¸ã¨ã‚¯ã‚¨ãƒªã‚’é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã§
            è¡¨ç¾ã—ã€æ•°å­¦çš„é¡ä¼¼åº¦ã§é–¢é€£æ€§ã‚’è¨ˆç®—ã™ã‚‹æ¤œç´¢æ‰‹æ³•ã§ã™ã€‚
            å¾“æ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã¨ç•°ãªã‚Šã€
            æ–‡è„ˆã‚„æ„å‘³ã‚’ç†è§£ã—ãŸæ¤œç´¢ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
            ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã€ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã€å†…ç©ãªã©ã®
            è¨ˆç®—æ–¹æ³•ãŒä½¿ç”¨ã•ã‚Œã€è¿‘ä¼¼æœ€è¿‘å‚æ¢ç´¢ï¼ˆANNï¼‰ã«ã‚ˆã‚Š
            é«˜é€Ÿãªæ¤œç´¢ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
            """,
            metadata={"title": "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æŠ€è¡“", "category": "æŠ€è¡“è§£èª¬"}
        ),
        
        Document(
            id="doc3",
            content="""
            ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã¯ã€é•·ã„æ–‡æ›¸ã‚’æ¤œç´¢ãƒ»å‡¦ç†ã—ã‚„ã™ã„
            å°ã•ãªå˜ä½ã«åˆ†å‰²ã™ã‚‹é‡è¦ãªå‰å‡¦ç†æŠ€è¡“ã§ã™ã€‚
            é©åˆ‡ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¨ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®è¨­å®šã«ã‚ˆã‚Šã€
            æ–‡è„ˆã‚’ä¿æŒã—ãªãŒã‚‰åŠ¹ç‡çš„ãªæ¤œç´¢ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
            æ–‡ã®å¢ƒç•Œã€æ®µè½ã€æ„å‘³çš„ãªã¾ã¨ã¾ã‚Šã‚’è€ƒæ…®ã—ãŸ
            åˆ†å‰²æ‰‹æ³•ãŒå­˜åœ¨ã—ã€RAGã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½ã«
            å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã¾ã™ã€‚
            """,
            metadata={"title": "ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æŠ€è¡“", "category": "æŠ€è¡“è§£èª¬"}
        ),
        
        Document(
            id="doc4",
            content="""
            å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®è©•ä¾¡ã«ã¯ã€
            å¤šé¢çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦ã§ã™ã€‚
            BLEUã€ROUGEã€BERTScoreãªã©ã®è‡ªå‹•è©•ä¾¡æŒ‡æ¨™ã«åŠ ãˆã€
            äººæ‰‹è©•ä¾¡ã«ã‚ˆã‚‹æµæš¢ã•ã€æ­£ç¢ºæ€§ã€æœ‰ç”¨æ€§ã®è©•ä¾¡ãŒé‡è¦ã§ã™ã€‚
            RAGã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€æ¤œç´¢ç²¾åº¦ã¨ç”Ÿæˆå“è³ªã®
            ä¸¡æ–¹ã‚’è€ƒæ…®ã—ãŸç·åˆçš„ãªè©•ä¾¡ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚
            """,
            metadata={"title": "LLMè©•ä¾¡æ‰‹æ³•", "category": "è©•ä¾¡"}
        )
    ]
    
    # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åˆæœŸåŒ–
    document_store = SQLiteDocumentStore(":memory:")
    vector_store = InMemoryVectorStore()
    
    # æ‰‹å‹•ã§ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰ï¼ˆæ­£è¦åŒ–ã‚’æ¨¡æ“¬ï¼‰
    print("ğŸ“š æ­£è¦åŒ–ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ§‹ç¯‰ä¸­...")
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¹ãƒˆã‚¢ã«ä¿å­˜
    for doc in documents:
        # å…ƒæ–‡æ›¸ã¨ã—ã¦ä¿å­˜
        doc.metadata["processing_stage"] = "original"
        document_store.store_document(doc)
        
        # æ­£è¦åŒ–ç‰ˆã‚‚ä½œæˆï¼ˆå®Ÿéš›ã®æ­£è¦åŒ–å‡¦ç†ã¯çœç•¥ï¼‰
        normalized_doc = Document(
            id=doc.id,
            content=doc.content,
            metadata={
                **doc.metadata,
                "processing_stage": "normalized",
                "normalization_stats": {
                    "dictionary_file_used": str(dict_file),
                    "total_replacements": 2,
                    "variations_normalized": 1
                }
            }
        )
        document_store.store_document(normalized_doc)
    
    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    embedder_config = TFIDFEmbeddingConfig(min_df=1, max_df=1.0)
    embedder = TFIDFEmbedder(config=embedder_config)
    
    corpus_texts = [doc.content for doc in documents]
    embedder.fit(corpus_texts)
    
    for doc in documents:
        embedding_result = embedder.embed_text(doc.content)
        vector_entry = VectorEntry(
            document_id=doc.id,
            content=doc.content[:200] + "..." if len(doc.content) > 200 else doc.content,
            embedding=embedding_result.vector.tolist(),
            metadata=doc.metadata
        )
        vector_store.add_vector(vector_entry)
    
    print(f"âœ… {len(documents)}ä»¶ã®æ–‡æ›¸ã§ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ§‹ç¯‰")
    return document_store, vector_store, embedder, str(dict_file), temp_dir
```

## ã‚¹ãƒ†ãƒƒãƒ—2: åŸºæœ¬çš„ãªQueryEngineä½œæˆ

åŸºæœ¬è¨­å®šã§QueryEngineã‚’ä½œæˆã—ã€å‹•ä½œã‚’ç¢ºèªï¼š

```python
def create_basic_query_engine(document_store, vector_store, embedder):
    """åŸºæœ¬çš„ãªQueryEngineã‚’ä½œæˆ"""
    
    from refinire_rag.use_cases.query_engine import QueryEngine, QueryEngineConfig
    from refinire_rag.retrieval import SimpleRetriever, SimpleReranker, SimpleReader
    
    print("ğŸ¤– åŸºæœ¬QueryEngineã‚’ä½œæˆä¸­...")
    
    # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆä½œæˆ
    retriever = SimpleRetriever(vector_store, embedder=embedder)
    reranker = SimpleReranker()
    reader = SimpleReader()
    
    # åŸºæœ¬è¨­å®š
    config = QueryEngineConfig(
        enable_query_normalization=True,
        auto_detect_corpus_state=True,
        retriever_top_k=10,
        reranker_top_k=3,
        include_sources=True,
        include_confidence=True
    )
    
    # QueryEngineä½œæˆ
    query_engine = QueryEngine(
        document_store=document_store,
        vector_store=vector_store,
        retriever=retriever,
        reader=reader,
        reranker=reranker,
        config=config
    )
    
    print("âœ… åŸºæœ¬QueryEngineã‚’ä½œæˆ")
    return query_engine
```

## ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¯ã‚¨ãƒªæ­£è¦åŒ–ã®ãƒ†ã‚¹ãƒˆ

è¡¨ç¾æºã‚‰ãã‚’å«ã‚€ã‚¯ã‚¨ãƒªã§æ­£è¦åŒ–åŠ¹æœã‚’ç¢ºèªï¼š

```python
def test_query_normalization(query_engine, dict_path):
    """ã‚¯ã‚¨ãƒªæ­£è¦åŒ–åŠ¹æœã‚’ãƒ†ã‚¹ãƒˆ"""
    
    from refinire_rag.processing.normalizer import Normalizer, NormalizerConfig
    
    print("\\n" + "="*60)
    print("ğŸ”„ ã‚¯ã‚¨ãƒªæ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ")
    print("="*60)
    
    # æ‰‹å‹•ã§æ­£è¦åŒ–æ©Ÿèƒ½ã‚’è¨­å®šï¼ˆè‡ªå‹•æ¤œå‡ºãŒå‹•ä½œã—ãªã„å ´åˆã®å¯¾å¿œï¼‰
    normalizer_config = NormalizerConfig(
        dictionary_file_path=dict_path,
        normalize_variations=True,
        expand_abbreviations=True,
        whole_word_only=False  # æ—¥æœ¬èªå¯¾å¿œ
    )
    query_engine.normalizer = Normalizer(normalizer_config)
    query_engine.corpus_state = {
        "has_normalization": True,
        "dictionary_path": dict_path
    }
    
    # è¡¨ç¾æºã‚‰ãã‚’å«ã‚€ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
    test_queries = [
        {
            "query": "æ¤œç´¢å¼·åŒ–ç”Ÿæˆã«ã¤ã„ã¦æ•™ãˆã¦",
            "expected": "æ¤œç´¢æ‹¡å¼µç”Ÿæˆã«ã¤ã„ã¦æ•™ãˆã¦",
            "description": "æ¤œç´¢å¼·åŒ–ç”Ÿæˆ â†’ æ¤œç´¢æ‹¡å¼µç”Ÿæˆ"
        },
        {
            "query": "æ„å‘³æ¤œç´¢ã®ä»•çµ„ã¿ã¯ï¼Ÿ",
            "expected": "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ä»•çµ„ã¿ã¯ï¼Ÿ",
            "description": "æ„å‘³æ¤œç´¢ â†’ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"
        },
        {
            "query": "æ–‡æ›¸åˆ†å‰²ã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¦",
            "expected": "ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¦",
            "description": "æ–‡æ›¸åˆ†å‰² â†’ ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°"
        },
        {
            "query": "LLMãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™ã¯ï¼Ÿ",
            "expected": "å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™ã¯ï¼Ÿ",
            "description": "LLMãƒ¢ãƒ‡ãƒ« â†’ å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«"
        }
    ]
    
    print("ğŸ“ æ­£è¦åŒ–å‰å¾Œã®æ¯”è¼ƒ:")
    
    for i, test_case in enumerate(test_queries, 1):
        query = test_case["query"]
        description = test_case["description"]
        
        print(f"\\nğŸ“Œ ãƒ†ã‚¹ãƒˆ {i}: {description}")
        print(f"   å…ƒã‚¯ã‚¨ãƒª: ã€Œ{query}ã€")
        
        try:
            result = query_engine.answer(query)
            
            normalized = result.metadata.get("query_normalized", False)
            normalized_query = result.normalized_query
            
            if normalized and normalized_query:
                print(f"   âœ… æ­£è¦åŒ–å¾Œ: ã€Œ{normalized_query}ã€")
                print(f"   ğŸ”„ é©ç”¨: æˆåŠŸ")
            else:
                print(f"   âŒ æ­£è¦åŒ–: æœªé©ç”¨")
            
            print(f"   ğŸ“Š æ¤œç´¢çµæœ: {result.metadata.get('source_count', 0)}ä»¶")
            print(f"   â±ï¸ å‡¦ç†æ™‚é–“: {result.metadata.get('processing_time', 0):.3f}ç§’")
            
        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
```

## ã‚¹ãƒ†ãƒƒãƒ—4: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ¥è¨­å®šã®æœ€é©åŒ–

å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®è¨­å®šã‚’èª¿æ•´ã—ã¦æ€§èƒ½ã‚’å‘ä¸Šï¼š

```python
def demo_component_optimization(document_store, vector_store, embedder):
    """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæœ€é©åŒ–ã®ãƒ‡ãƒ¢"""
    
    from refinire_rag.retrieval import (
        SimpleRetrieverConfig, SimpleRerankerConfig, SimpleReaderConfig
    )
    
    print("\\n" + "="*60)
    print("âš™ï¸ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæœ€é©åŒ–ãƒ‡ãƒ¢")
    print("="*60)
    
    # æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®š
    print("ğŸ”§ æœ€é©åŒ–è¨­å®šã‚’é©ç”¨ä¸­...")
    
    # Retrieverè¨­å®š
    retriever_config = SimpleRetrieverConfig(
        top_k=15,  # ã‚ˆã‚Šå¤šãã®å€™è£œã‚’æ¤œç´¢
        similarity_threshold=0.1,  # ä½ã„é–¾å€¤ã§å¹…åºƒãæ¤œç´¢
        embedding_model="tfidf-optimized"
    )
    
    # Rerankerè¨­å®š
    reranker_config = SimpleRerankerConfig(
        top_k=5,  # ä¸Šä½5ä»¶ã«çµã‚Šè¾¼ã¿
        boost_exact_matches=True,  # å®Œå…¨ä¸€è‡´ã«ãƒœãƒ¼ãƒŠã‚¹
        length_penalty_factor=0.1  # é©åˆ‡ãªé•·ã•ã‚’å„ªé‡
    )
    
    # Readerè¨­å®š
    reader_config = SimpleReaderConfig(
        max_context_length=2500,  # ã‚ˆã‚Šå¤šãã®æ–‡è„ˆã‚’ä½¿ç”¨
        llm_model="gpt-4o-mini",
        temperature=0.1,  # ä¸€è²«æ€§ã‚’é‡è¦–
        max_tokens=600,  # ã‚ˆã‚Šè©³ç´°ãªå›ç­”
        include_sources=True
    )
    
    # æœ€é©åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆä½œæˆ
    optimized_retriever = SimpleRetriever(vector_store, embedder=embedder, config=retriever_config)
    optimized_reranker = SimpleReranker(config=reranker_config)
    optimized_reader = SimpleReader(config=reader_config)
    
    # QueryEngineè¨­å®š
    engine_config = QueryEngineConfig(
        enable_query_normalization=True,
        auto_detect_corpus_state=True,
        retriever_top_k=15,
        reranker_top_k=5,
        include_sources=True,
        include_confidence=True,
        include_processing_metadata=True
    )
    
    # æœ€é©åŒ–QueryEngineä½œæˆ
    optimized_engine = QueryEngine(
        document_store=document_store,
        vector_store=vector_store,
        retriever=optimized_retriever,
        reader=optimized_reader,
        reranker=optimized_reranker,
        config=engine_config
    )
    
    print("âœ… æœ€é©åŒ–QueryEngineã‚’ä½œæˆ")
    
    # æ€§èƒ½æ¯”è¼ƒãƒ†ã‚¹ãƒˆ
    test_queries = [
        "RAGã‚·ã‚¹ãƒ†ãƒ ã®æŠ€è¡“çš„ãªä»•çµ„ã¿ã‚’è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„",
        "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "åŠ¹æœçš„ãªãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥ã«ã¤ã„ã¦æ•™ãˆã¦",
        "LLMã®è©•ä¾¡æ–¹æ³•ã¨ãã®èª²é¡Œã¯ï¼Ÿ"
    ]
    
    print("\\nğŸ“Š æœ€é©åŒ–åŠ¹æœã®æ¯”è¼ƒ:")
    
    for i, query in enumerate(test_queries, 1):
        print(f"\\nğŸ“Œ ã‚¯ã‚¨ãƒª {i}: {query}")
        print("-" * 50)
        
        try:
            result = optimized_engine.answer(query)
            
            print(f"ğŸ¤– å›ç­”:")
            # å›ç­”ã‚’è¦‹ã‚„ã™ãè¡¨ç¤º
            answer_lines = result.answer.split('\\n')
            for line in answer_lines:
                if line.strip():
                    print(f"   {line.strip()}")
            
            print(f"\\nğŸ“Š è©³ç´°çµ±è¨ˆ:")
            print(f"   - æ¤œç´¢æ–‡æ›¸æ•°: {result.metadata.get('source_count', 0)}")
            print(f"   - ä¿¡é ¼åº¦: {result.confidence:.3f}")
            print(f"   - å‡¦ç†æ™‚é–“: {result.metadata.get('processing_time', 0):.3f}ç§’")
            
            # ã‚½ãƒ¼ã‚¹æƒ…å ±
            if result.sources:
                print(f"   - ä¸»è¦ã‚½ãƒ¼ã‚¹:")
                for j, source in enumerate(result.sources[:3], 1):
                    title = source.metadata.get('title', f'Document {source.document_id}')
                    print(f"     {j}. {title} (ã‚¹ã‚³ã‚¢: {source.score:.3f})")
            
            # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçµ±è¨ˆ
            metadata = result.metadata
            if metadata.get('include_processing_metadata'):
                print(f"   - ãƒªãƒ©ãƒ³ã‚«ãƒ¼ä½¿ç”¨: {'Yes' if metadata.get('reranker_used') else 'No'}")
                if 'retrieval_stats' in metadata:
                    ret_stats = metadata['retrieval_stats']
                    print(f"   - æ¤œç´¢çµ±è¨ˆ: {ret_stats.get('queries_processed', 0)}ã‚¯ã‚¨ãƒªå‡¦ç†æ¸ˆã¿")
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
    
    return optimized_engine
```

## ã‚¹ãƒ†ãƒƒãƒ—5: å›ç­”å“è³ªã®è©•ä¾¡

ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã®å“è³ªã‚’è©•ä¾¡ï¼š

```python
def evaluate_answer_quality(query_engine):
    """å›ç­”å“è³ªã®è©•ä¾¡"""
    
    print("\\n" + "="*60)
    print("ğŸ“ˆ å›ç­”å“è³ªè©•ä¾¡")
    print("="*60)
    
    # è©•ä¾¡ç”¨ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆ
    evaluation_queries = [
        {
            "query": "RAGã®ä¸»ãªåˆ©ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "category": "åŸºæœ¬çŸ¥è­˜",
            "expected_keywords": ["ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³", "çŸ¥è­˜æ›´æ–°", "å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿"]
        },
        {
            "query": "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®è¨ˆç®—æ–¹æ³•ã‚’æ•™ãˆã¦",
            "category": "æŠ€è¡“è©³ç´°", 
            "expected_keywords": ["ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦", "é«˜æ¬¡å…ƒ", "ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“"]
        },
        {
            "query": "åŠ¹æœçš„ãªãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥ã¨ã¯ï¼Ÿ",
            "category": "å®Ÿè·µå¿œç”¨",
            "expected_keywords": ["ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", "ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—", "æ–‡è„ˆ"]
        }
    ]
    
    print("ğŸ“ è©•ä¾¡çµæœ:")
    
    total_score = 0
    for i, eval_case in enumerate(evaluation_queries, 1):
        query = eval_case["query"]
        category = eval_case["category"]
        expected_keywords = eval_case["expected_keywords"]
        
        print(f"\\nğŸ“Œ è©•ä¾¡ {i}: {category}")
        print(f"   ã‚¯ã‚¨ãƒª: {query}")
        
        try:
            result = query_engine.answer(query)
            answer = result.answer
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ç‡ãƒã‚§ãƒƒã‚¯
            keyword_score = 0
            found_keywords = []
            
            for keyword in expected_keywords:
                if keyword in answer:
                    keyword_score += 1
                    found_keywords.append(keyword)
            
            keyword_ratio = keyword_score / len(expected_keywords)
            
            # å›ç­”é•·ãƒã‚§ãƒƒã‚¯
            answer_length = len(answer.strip())
            length_score = 1.0 if 50 <= answer_length <= 500 else 0.5
            
            # ã‚½ãƒ¼ã‚¹æ´»ç”¨åº¦ãƒã‚§ãƒƒã‚¯
            source_score = min(result.metadata.get('source_count', 0) / 3.0, 1.0)
            
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            overall_score = (keyword_ratio * 0.4 + length_score * 0.3 + source_score * 0.3)
            total_score += overall_score
            
            print(f"   ğŸ¤– å›ç­”: {answer[:100]}{'...' if len(answer) > 100 else ''}")
            print(f"   ğŸ“Š è©•ä¾¡:")
            print(f"     - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰: {keyword_score}/{len(expected_keywords)} ({keyword_ratio:.1%})")
            print(f"     - å›ç­”é•·: {answer_length}æ–‡å­— (é©åˆ‡: {'Yes' if length_score == 1.0 else 'No'})")
            print(f"     - ã‚½ãƒ¼ã‚¹æ´»ç”¨: {result.metadata.get('source_count', 0)}ä»¶")
            print(f"     - ç·åˆã‚¹ã‚³ã‚¢: {overall_score:.2f}/1.00")
            
            if found_keywords:
                print(f"     - ç™ºè¦‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(found_keywords)}")
        
        except Exception as e:
            print(f"   âŒ è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
    
    # å…¨ä½“è©•ä¾¡
    average_score = total_score / len(evaluation_queries)
    print(f"\\nğŸ† å…¨ä½“è©•ä¾¡çµæœ:")
    print(f"   - å¹³å‡ã‚¹ã‚³ã‚¢: {average_score:.2f}/1.00")
    
    if average_score >= 0.8:
        print(f"   - è©•ä¾¡: å„ªç§€ ğŸŒŸ")
    elif average_score >= 0.6:
        print(f"   - è©•ä¾¡: è‰¯å¥½ ğŸ‘")
    elif average_score >= 0.4:
        print(f"   - è©•ä¾¡: æ”¹å–„å¿…è¦ ğŸ“ˆ")
    else:
        print(f"   - è©•ä¾¡: è¦å¤§å¹…æ”¹å–„ ğŸ”§")
```

## ã‚¹ãƒ†ãƒƒãƒ—6: çµ±è¨ˆæƒ…å ±ã®åˆ†æ

QueryEngineã®è©³ç´°çµ±è¨ˆã‚’åˆ†æï¼š

```python
def analyze_engine_statistics(query_engine):
    """ã‚¨ãƒ³ã‚¸ãƒ³çµ±è¨ˆã®åˆ†æ"""
    
    print("\\n" + "="*60)
    print("ğŸ“Š ã‚¨ãƒ³ã‚¸ãƒ³çµ±è¨ˆåˆ†æ")
    print("="*60)
    
    # çµ±è¨ˆæƒ…å ±å–å¾—
    stats = query_engine.get_engine_stats()
    
    # åŸºæœ¬çµ±è¨ˆ
    print("ğŸ“ˆ åŸºæœ¬çµ±è¨ˆ:")
    print(f"   - å‡¦ç†ã‚¯ã‚¨ãƒªæ•°: {stats.get('queries_processed', 0)}")
    print(f"   - æ­£è¦åŒ–ã‚¯ã‚¨ãƒªæ•°: {stats.get('queries_normalized', 0)}")
    print(f"   - å¹³å‡å¿œç­”æ™‚é–“: {stats.get('average_response_time', 0):.3f}ç§’")
    print(f"   - å¹³å‡æ¤œç´¢ä»¶æ•°: {stats.get('average_retrieval_count', 0):.1f}")
    
    # æ­£è¦åŒ–ç‡
    total_queries = stats.get('queries_processed', 1)
    normalized_queries = stats.get('queries_normalized', 0)
    normalization_rate = normalized_queries / total_queries * 100
    print(f"   - ã‚¯ã‚¨ãƒªæ­£è¦åŒ–ç‡: {normalization_rate:.1f}%")
    
    # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçµ±è¨ˆ
    print("\\nğŸ”§ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçµ±è¨ˆ:")
    
    components = ['retriever_stats', 'reranker_stats', 'reader_stats']
    for component in components:
        if component in stats:
            comp_stats = stats[component]
            comp_name = component.replace('_stats', '').title()
            
            print(f"   {comp_name}:")
            print(f"     - å‡¦ç†å›æ•°: {comp_stats.get('queries_processed', 0)}")
            print(f"     - å‡¦ç†æ™‚é–“: {comp_stats.get('processing_time', 0):.3f}ç§’")
            print(f"     - ã‚¨ãƒ©ãƒ¼æ•°: {comp_stats.get('errors_encountered', 0)}")
    
    # è¨­å®šæƒ…å ±
    print("\\nâš™ï¸ è¨­å®šæƒ…å ±:")
    config_info = stats.get('config', {})
    for key, value in config_info.items():
        print(f"   - {key}: {value}")
    
    # ã‚³ãƒ¼ãƒ‘ã‚¹çŠ¶æ…‹
    print("\\nğŸ“š ã‚³ãƒ¼ãƒ‘ã‚¹çŠ¶æ…‹:")
    corpus_state = stats.get('corpus_state', {})
    for key, value in corpus_state.items():
        print(f"   - {key}: {value}")
```

## å®Œå…¨ãªã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚°ãƒ©ãƒ 

```python
#!/usr/bin/env python3
"""
ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«3: ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã¨å›ç­”ç”Ÿæˆ
"""

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    print("ğŸš€ ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã¨å›ç­”ç”Ÿæˆ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«")
    print("="*60)
    print("QueryEngineã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªã‚¯ã‚¨ãƒªå‡¦ç†ã¨å›ç­”ç”Ÿæˆã‚’å­¦ç¿’ã—ã¾ã™")
    
    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: æ­£è¦åŒ–ã‚³ãƒ¼ãƒ‘ã‚¹æº–å‚™
        print("\\nğŸ“š ã‚¹ãƒ†ãƒƒãƒ—1: æ­£è¦åŒ–ã‚³ãƒ¼ãƒ‘ã‚¹æº–å‚™")
        document_store, vector_store, embedder, dict_path, temp_dir = setup_normalized_corpus()
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: åŸºæœ¬QueryEngineä½œæˆ
        print("\\nğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—2: åŸºæœ¬QueryEngineä½œæˆ")
        basic_engine = create_basic_query_engine(document_store, vector_store, embedder)
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¯ã‚¨ãƒªæ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ
        test_query_normalization(basic_engine, dict_path)
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæœ€é©åŒ–
        optimized_engine = demo_component_optimization(document_store, vector_store, embedder)
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: å›ç­”å“è³ªè©•ä¾¡
        evaluate_answer_quality(optimized_engine)
        
        # ã‚¹ãƒ†ãƒƒãƒ—6: çµ±è¨ˆåˆ†æ
        analyze_engine_statistics(optimized_engine)
        
        print("\\nğŸ‰ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«3ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("\\nğŸ“š å­¦ç¿’å†…å®¹:")
        print("   âœ… QueryEngineã®åŸºæœ¬ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£")
        print("   âœ… ã‚¯ã‚¨ãƒªæ­£è¦åŒ–ã«ã‚ˆã‚‹æ¤œç´¢ç²¾åº¦å‘ä¸Š")
        print("   âœ… Retriever/Reranker/Readerã®æœ€é©åŒ–")
        print("   âœ… å›ç­”å“è³ªã®å®šé‡çš„è©•ä¾¡")
        print("   âœ… çµ±è¨ˆæƒ…å ±ã«ã‚ˆã‚‹æ€§èƒ½åˆ†æ")
        
    except Exception as e:
        print(f"\\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    main()
```

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã«ã‚ˆã‚Šã€QueryEngineã®é«˜åº¦ãªæ©Ÿèƒ½ã‚’æ´»ç”¨ã—ãŸåŠ¹æœçš„ãªRAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰æ–¹æ³•ã‚’å­¦ç¿’ã§ãã¾ã™ã€‚æ¬¡ã¯[ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«4: é«˜åº¦ãªæ­£è¦åŒ–ã¨ã‚¯ã‚¨ãƒªå‡¦ç†](tutorial_04_normalization.md)ã§ã€ã•ã‚‰ã«è©³ç´°ãªæ­£è¦åŒ–æŠ€è¡“ã‚’å­¦ç¿’ã—ã¾ã—ã‚‡ã†ã€‚