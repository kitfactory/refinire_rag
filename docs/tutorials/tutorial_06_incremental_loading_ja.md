# ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« 6: å¢—åˆ†æ–‡æ›¸ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«5ã®ä¼æ¥­RAGã‚·ã‚¹ãƒ†ãƒ ã‚’åŸºã«ã€å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦å¤§è¦æ¨¡ãªæ–‡æ›¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’åŠ¹ç‡çš„ã«ç®¡ç†ã™ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚

## æ¦‚è¦

å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚ˆã‚Šä»¥ä¸‹ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ï¼š
- æ–°è¦ãƒ»æ›´æ–°ã•ã‚ŒãŸæ–‡æ›¸ã®ã¿ã‚’å‡¦ç†
- æœªå¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦åŠ¹ç‡çš„ãªæ›´æ–°
- å¤§è¦æ¨¡æ–‡æ›¸ãƒªãƒã‚¸ãƒˆãƒªã®åŠ¹ç‡çš„ãªå‡¦ç†
- æ–‡æ›¸ç³»è­œã¨å±¥æ­´ã®ç¶­æŒ

## å‰ææ¡ä»¶

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¯ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«5ï¼ˆä¼æ¥­RAGã®åˆ©ç”¨ï¼‰ã®å†…å®¹ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã€äº‹å‰ã«ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«5ã‚’å®Œäº†ã—ã¦ãã ã•ã„ã€‚

## å®Ÿè£…

### ã‚¹ãƒ†ãƒƒãƒ— 1: å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œä¼æ¥­RAG

```python
#!/usr/bin/env python3
"""
ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« 6: å¢—åˆ†æ–‡æ›¸ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
ä¼æ¥­ç’°å¢ƒã§ã®RAGã‚·ã‚¹ãƒ†ãƒ ã«å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ©Ÿèƒ½ã‚’è¿½åŠ 
"""

import sys
import os
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

# srcã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from refinire_rag.loaders.incremental_loader import IncrementalLoader
from refinire_rag.application.corpus_manager import CorpusManager, CorpusManagerConfig
from refinire_rag.application.query_engine import QueryEngine, QueryEngineConfig
from refinire_rag.storage import SQLiteDocumentStore, InMemoryVectorStore
from refinire_rag.embedding import TFIDFEmbedder, TFIDFEmbeddingConfig
from refinire_rag.retrieval import SimpleRetriever, SimpleReranker, SimpleReader
from refinire_rag.processing import Normalizer, NormalizerConfig, TokenBasedChunker, ChunkingConfig
from refinire_rag.models.document import Document


class IncrementalEnterpriseRAG:
    """
    å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œã®ä¼æ¥­RAGã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self, department: str, base_dir: Path):
        self.department = department
        self.base_dir = base_dir
        self.docs_dir = base_dir / f"{department}_docs"
        self.db_path = str(base_dir / f"{department}_rag.db")
        self.cache_path = str(base_dir / f".{department}_cache.json")
        
        # éƒ¨ç½²ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        self.docs_dir.mkdir(parents=True, exist_ok=True)
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–
        self._setup_components()
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "total_documents": 0,
            "last_update": None,
            "incremental_runs": 0,
            "total_processing_time": 0.0
        }
    
    def _setup_components(self):
        """RAGã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        
        # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        self.document_store = SQLiteDocumentStore(self.db_path)
        self.vector_store = InMemoryVectorStore(similarity_metric="cosine")
        
        # åŸ‹ã‚è¾¼ã¿
        embedder_config = TFIDFEmbeddingConfig(min_df=1, max_df=1.0, max_features=5000)
        self.embedder = TFIDFEmbedder(config=embedder_config)
        
        # å‡¦ç†ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        normalizer_config = NormalizerConfig(
            dictionary_file_path=str(self.base_dir / f"{self.department}_dictionary.md"),
            whole_word_only=False
        )
        
        chunking_config = ChunkingConfig(
            chunk_size=300,
            overlap=30,
            split_by_sentence=True
        )
        
        # ãƒ—ãƒ­ã‚»ãƒƒã‚µä»˜ãã‚³ãƒ¼ãƒ‘ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        corpus_config = CorpusManagerConfig(
            document_store=self.document_store,
            vector_store=self.vector_store,
            embedder=self.embedder,
            processors=[
                Normalizer(normalizer_config),
                TokenBasedChunker(chunking_config)
            ],
            enable_progress_reporting=True
        )
        
        self.corpus_manager = CorpusManager(corpus_config)
        
        # ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³
        retriever = SimpleRetriever(self.vector_store, self.embedder)
        reranker = SimpleReranker()
        reader = SimpleReader()
        
        query_config = QueryEngineConfig(
            retriever=retriever,
            reranker=reranker,
            reader=reader,
            normalizer=Normalizer(normalizer_config)
        )
        
        self.query_engine = QueryEngine(query_config)
        
        # å¢—åˆ†ãƒ­ãƒ¼ãƒ€ãƒ¼
        self.incremental_loader = IncrementalLoader(
            document_store=self.document_store,
            base_loader=self.corpus_manager._loader,
            cache_file=self.cache_path
        )
    
    def add_documents(self, documents: Dict[str, str], force_update: bool = False):
        """
        éƒ¨ç½²ã®æ–‡æ›¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«æ–‡æ›¸ã‚’è¿½åŠ ã¾ãŸã¯æ›´æ–°
        
        Args:
            documents: ãƒ•ã‚¡ã‚¤ãƒ«å -> å†…å®¹ã®è¾æ›¸
            force_update: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤‰æ›´ã•ã‚Œã¦ã„ãªãã¦ã‚‚å¼·åˆ¶æ›´æ–°
        """
        print(f"\\nğŸ“ {self.department}éƒ¨ç½²ã«æ–‡æ›¸ã‚’è¿½åŠ ä¸­...")
        
        # æ–‡æ›¸ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        for filename, content in documents.items():
            file_path = self.docs_dir / filename
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"   ä½œæˆ: {filename}")
        
        # å¢—åˆ†å‡¦ç†
        start_time = time.time()
        
        force_reload = set()
        if force_update:
            force_reload = {str(self.docs_dir / filename) for filename in documents.keys()}
        
        results = self.incremental_loader.process_incremental(
            sources=[self.docs_dir],
            force_reload=force_reload
        )
        
        processing_time = time.time() - start_time
        
        # æ–°ã—ã„æ–‡æ›¸ãŒã‚ã‚‹å ´åˆã¯åŸ‹ã‚è¾¼ã¿å™¨ã‚’ãƒ•ã‚£ãƒƒãƒˆ
        all_docs = results['new'] + results['updated']
        if all_docs and not self.embedder.is_fitted():
            print(f"   {len(all_docs)}ä»¶ã®æ–‡æ›¸ã§åŸ‹ã‚è¾¼ã¿å™¨ã‚’ãƒ•ã‚£ãƒƒãƒˆä¸­...")
            texts = [doc.content for doc in all_docs if doc.content.strip()]
            if texts:
                self.embedder.fit(texts)
        
        # ã‚³ãƒ¼ãƒ‘ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å‡¦ç†
        if all_docs:
            print(f"   {len(all_docs)}ä»¶ã®æ–‡æ›¸ã‚’RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å‡¦ç†ä¸­...")
            corpus_results = self.corpus_manager.process_documents(all_docs)
            
            # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã¨ä¿å­˜
            embedded_docs = self.corpus_manager.embed_documents(corpus_results)
            print(f"   {len(embedded_docs)}ä»¶ã®æ–‡æ›¸ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ")
        
        # çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°
        self.stats["total_documents"] = self.document_store.get_stats().total_documents
        self.stats["last_update"] = datetime.now().isoformat()
        self.stats["incremental_runs"] += 1
        self.stats["total_processing_time"] += processing_time
        
        # çµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆ
        print(f"\\nğŸ“Š å‡¦ç†çµæœ:")
        print(f"   æ–°è¦æ–‡æ›¸: {len(results['new'])}ä»¶")
        print(f"   æ›´æ–°æ–‡æ›¸: {len(results['updated'])}ä»¶")
        print(f"   ã‚¹ã‚­ãƒƒãƒ—æ–‡æ›¸: {len(results['skipped'])}ä»¶")
        print(f"   å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        
        return results
    
    def update_document(self, filename: str, new_content: str):
        """ç‰¹å®šã®æ–‡æ›¸ã‚’æ›´æ–°"""
        print(f"\\nğŸ“ æ–‡æ›¸ã‚’æ›´æ–°: {filename}")
        
        file_path = self.docs_dir / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        # æ›´æ–°ã‚’å‡¦ç†
        results = self.add_documents({filename: new_content})
        return results
    
    def delete_document(self, filename: str):
        """æ–‡æ›¸ã‚’å‰Šé™¤ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        print(f"\\nğŸ—‘ï¸ æ–‡æ›¸ã‚’å‰Šé™¤: {filename}")
        
        file_path = self.docs_dir / filename
        if file_path.exists():
            file_path.unlink()
            print(f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤: {filename}")
            
            # ã‚¹ãƒˆã‚¢ã‹ã‚‰ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            deleted_docs = self.incremental_loader.cleanup_deleted_files([self.docs_dir])
            print(f"   ã‚¹ãƒˆã‚¢ã‹ã‚‰{len(deleted_docs)}ä»¶ã®æ–‡æ›¸ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
            
            return deleted_docs
        else:
            print(f"   ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filename}")
            return []
    
    def query(self, question: str) -> Dict[str, Any]:
        """éƒ¨ç½²ã®RAGã‚·ã‚¹ãƒ†ãƒ ã«ã‚¯ã‚¨ãƒª"""
        return self.query_engine.answer(question)
    
    def get_statistics(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆã‚’å–å¾—"""
        cache_stats = self.incremental_loader.get_cache_stats()
        store_stats = self.document_store.get_stats()
        
        return {
            **self.stats,
            "cache_statistics": cache_stats,
            "document_store": {
                "total_documents": store_stats.total_documents,
                "db_path": self.db_path
            },
            "vector_store": {
                "total_vectors": len(self.vector_store.vectors),
                "dimension": getattr(self.vector_store, 'dimension', 'ä¸æ˜')
            }
        }


def demo_incremental_enterprise_rag():
    """ä¼æ¥­RAGã§ã®å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    print("ğŸ¢ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« 6: å¢—åˆ†æ–‡æ›¸ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°")
    print("=" * 60)
    
    # ãƒ‡ãƒ¢ç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    demo_dir = Path("enterprise_incremental_demo")
    demo_dir.mkdir(exist_ok=True)
    
    try:
        # äººäº‹éƒ¨RAGã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ
        hr_rag = IncrementalEnterpriseRAG("hr", demo_dir)
        
        print("\\nğŸ¯ ã‚¹ãƒ†ãƒƒãƒ— 1: åˆæœŸæ–‡æ›¸ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°")
        print("-" * 40)
        
        # åˆæœŸæ–‡æ›¸
        initial_docs = {
            "employee_handbook.md": '''
# å¾“æ¥­å“¡ãƒãƒ³ãƒ‰ãƒ–ãƒƒã‚¯ v1.0

## å‹¤å‹™æ™‚é–“
- å¹³æ—¥: 9:00-18:00
- æ˜¼ä¼‘ã¿: 12:00-13:00

## ä¼‘æš‡åˆ¶åº¦
- å¹´æ¬¡æœ‰çµ¦ä¼‘æš‡: 20æ—¥
- å¤å­£ä¼‘æš‡: 3æ—¥
- å¹´æœ«å¹´å§‹: 5æ—¥

## ç¦åˆ©åšç”Ÿ
- å¥åº·ä¿é™º
- åšç”Ÿå¹´é‡‘
- é›‡ç”¨ä¿é™º
            ''',
            
            "remote_work_policy.md": '''
# ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯è¦å®š

## å¯¾è±¡è€…
æ­£ç¤¾å“¡ãŠã‚ˆã³å¥‘ç´„ç¤¾å“¡

## ç”³è«‹æ‰‹ç¶šã
1. ä¸Šå¸ã«äº‹å‰ç”³è«‹
2. äººäº‹éƒ¨æ‰¿èª
3. å®Ÿæ–½å ±å‘Š

## å‹¤å‹™ç’°å¢ƒ
- å®‰å®šã—ãŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š
- é©åˆ‡ãªä½œæ¥­ã‚¹ãƒšãƒ¼ã‚¹
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–
            '''
        }
        
        # åˆæœŸæ–‡æ›¸ã‚’è¿½åŠ 
        results1 = hr_rag.add_documents(initial_docs)
        
        print("\\nğŸ” ã‚¹ãƒ†ãƒƒãƒ— 2: åˆæœŸã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ")
        print("-" * 40)
        
        result = hr_rag.query("ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã®ç”³è«‹æ‰‹ç¶šãã¯ï¼Ÿ")
        print(f"è³ªå•: ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã®ç”³è«‹æ‰‹ç¶šãã¯ï¼Ÿ")
        print(f"å›ç­”: {result['answer']}")
        
        print("\\nâ±ï¸ ã‚¹ãƒ†ãƒƒãƒ— 3: å¢—åˆ†æ›´æ–°ï¼ˆå¤‰æ›´ãªã—ï¼‰")
        print("-" * 40)
        
        # å¤‰æ›´ãªã—ã§å†å®Ÿè¡Œ - ã™ã¹ã¦ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹ã¯ãš
        results2 = hr_rag.add_documents({})
        
        print("\\nğŸ“ ã‚¹ãƒ†ãƒƒãƒ— 4: æ—¢å­˜æ–‡æ›¸ã®æ›´æ–°")
        print("-" * 40)
        
        # å¾“æ¥­å“¡ãƒãƒ³ãƒ‰ãƒ–ãƒƒã‚¯ã‚’æ›´æ–°
        updated_handbook = '''
# å¾“æ¥­å“¡ãƒãƒ³ãƒ‰ãƒ–ãƒƒã‚¯ v2.0

## å‹¤å‹™æ™‚é–“
- å¹³æ—¥: 9:00-17:30 (å¤‰æ›´)
- æ˜¼ä¼‘ã¿: 12:00-13:00

## ä¼‘æš‡åˆ¶åº¦
- å¹´æ¬¡æœ‰çµ¦ä¼‘æš‡: 25æ—¥ (æ”¹å–„)
- å¤å­£ä¼‘æš‡: 5æ—¥ (æ”¹å–„)
- å¹´æœ«å¹´å§‹: 5æ—¥
- ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ä¼‘æš‡: 3æ—¥ (æ–°è¦)

## ç¦åˆ©åšç”Ÿ
- å¥åº·ä¿é™º
- åšç”Ÿå¹´é‡‘
- é›‡ç”¨ä¿é™º
- ä½å®…æ‰‹å½“ (æ–°è¦)

æœ€çµ‚æ›´æ–°: 2024å¹´6æœˆ
        '''
        
        results3 = hr_rag.update_document("employee_handbook.md", updated_handbook)
        
        print("\\nğŸ“„ ã‚¹ãƒ†ãƒƒãƒ— 5: æ–°è¦æ–‡æ›¸ã®è¿½åŠ ")
        print("-" * 40)
        
        new_docs = {
            "performance_review.md": '''
# äººäº‹è©•ä¾¡åˆ¶åº¦

## è©•ä¾¡æœŸé–“
- ä¸ŠåŠæœŸ: 4æœˆ-9æœˆ
- ä¸‹åŠæœŸ: 10æœˆ-3æœˆ

## è©•ä¾¡é …ç›®
1. æ¥­å‹™æˆæœ
2. ãƒ—ãƒ­ã‚»ã‚¹
3. è¡Œå‹•ç‰¹æ€§

## è©•ä¾¡ãƒ—ãƒ­ã‚»ã‚¹
1. è‡ªå·±è©•ä¾¡
2. ä¸Šå¸è©•ä¾¡
3. ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é¢è«‡
4. è©•ä¾¡ç¢ºå®š

## æ˜‡é€²ãƒ»æ˜‡æ ¼
è©•ä¾¡çµæœã«åŸºã¥ãå¹´2å›æ¤œè¨
            '''
        }
        
        results4 = hr_rag.add_documents(new_docs)
        
        print("\\nğŸ” ã‚¹ãƒ†ãƒƒãƒ— 6: æ›´æ–°ã•ã‚ŒãŸçŸ¥è­˜ã®ãƒ†ã‚¹ãƒˆ")
        print("-" * 40)
        
        # æ›´æ–°ã•ã‚ŒãŸæƒ…å ±ã‚’ãƒ†ã‚¹ãƒˆ
        questions = [
            "å‹¤å‹™æ™‚é–“ã¯ä½•æ™‚ã¾ã§ã§ã™ã‹ï¼Ÿ",
            "å¹´æ¬¡æœ‰çµ¦ä¼‘æš‡ã¯ä½•æ—¥ã‚‚ã‚‰ãˆã¾ã™ã‹ï¼Ÿ",
            "äººäº‹è©•ä¾¡ã¯ã„ã¤è¡Œã‚ã‚Œã¾ã™ã‹ï¼Ÿ"
        ]
        
        for question in questions:
            result = hr_rag.query(question)
            print(f"\\nè³ªå•: {question}")
            print(f"å›ç­”: {result['answer']}")
        
        print("\\nğŸ—‘ï¸ ã‚¹ãƒ†ãƒƒãƒ— 7: æ–‡æ›¸ã®å‰Šé™¤")
        print("-" * 40)
        
        # æ–‡æ›¸ã‚’å‰Šé™¤
        deleted = hr_rag.delete_document("remote_work_policy.md")
        
        print("\\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ— 8: æœ€çµ‚çµ±è¨ˆ")
        print("-" * 40)
        
        stats = hr_rag.get_statistics()
        print(f"ğŸ“ˆ ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ:")
        print(f"   ç·æ–‡æ›¸æ•°: {stats['total_documents']}")
        print(f"   å¢—åˆ†å®Ÿè¡Œå›æ•°: {stats['incremental_runs']}")
        print(f"   ç·å‡¦ç†æ™‚é–“: {stats['total_processing_time']:.2f}ç§’")
        print(f"   æœ€çµ‚æ›´æ–°: {stats['last_update']}")
        print(f"   ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['cache_statistics']['total_files']}")
        print(f"   ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚µã‚¤ã‚º: {stats['vector_store']['total_vectors']}")
        
        print("\\nğŸ¯ å¢—åˆ†ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®åˆ©ç‚¹")
        print("-" * 40)
        print("âœ… åŠ¹ç‡çš„ãªæ›´æ–° - å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å‡¦ç†")
        print("âœ… é«˜é€Ÿå†å‡¦ç† - æœªå¤‰æ›´æ–‡æ›¸ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        print("âœ… å¤§è¦æ¨¡å¯¾å¿œ - æ•°åƒã®æ–‡æ›¸ã‚’å‡¦ç†å¯èƒ½")
        print("âœ… è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— - å‰Šé™¤ã•ã‚ŒãŸæ–‡æ›¸ã‚’é™¤å»")
        print("âœ… å¤‰æ›´æ¤œå‡º - è¤‡æ•°ã®æ¤œè¨¼æ–¹æ³•")
        print("âœ… ä¼æ¥­å¯¾å¿œ - æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆé©ç”¨å¯èƒ½")
        
        print("\\nğŸ’¡ æœ¬ç•ªç’°å¢ƒã§ã®ä½¿ç”¨ãƒ’ãƒ³ãƒˆ")
        print("-" * 40)
        print("1. ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å¢—åˆ†æ›´æ–°ã‚’å®Ÿè¡Œï¼ˆå¤œé–“/æ™‚é–“æ¯ï¼‰")
        print("2. æœ€é©åŒ–ã®ãŸã‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’ç›£è¦–")
        print("3. ä½“ç³»çš„æ›´æ–°ã§force_reloadã‚’ä½¿ç”¨")
        print("4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ã§ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚’å®Ÿè£…")
        print("5. ç½å®³å¾©æ—§ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—")
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¢ã§ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        cleanup = input("\\nãƒ‡ãƒ¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ (y/N): ").lower().strip()
        if cleanup == 'y':
            import shutil
            if demo_dir.exists():
                shutil.rmtree(demo_dir)
            print("âœ… ãƒ‡ãƒ¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")
        else:
            print(f"ğŸ“ ãƒ‡ãƒ¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿æŒ: {demo_dir}")
    
    print("\\nğŸ‰ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« 6 å®Œäº†ï¼")
    print("\\næ¬¡å›: ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« 7ã§ã¯é«˜åº¦ãªã‚¯ã‚¨ãƒªæœ€é©åŒ–ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™")


if __name__ == "__main__":
    demo_incremental_enterprise_rag()