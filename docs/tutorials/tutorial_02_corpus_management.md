# ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«2: ã‚³ãƒ¼ãƒ‘ã‚¹ç®¡ç†ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€refinire-ragã®é«˜åº¦ãªã‚³ãƒ¼ãƒ‘ã‚¹ç®¡ç†æ©Ÿèƒ½ã¨ã€ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å­¦ç¿’ã—ã¾ã™ã€‚

## å­¦ç¿’ç›®æ¨™

- CorpusManagerã®3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç†è§£ã™ã‚‹
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ­£è¦åŒ–ã¨è¾æ›¸ä½œæˆã‚’ä½“é¨“ã™ã‚‹
- ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹
- ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®šã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹

## CorpusManagerã®3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

refinire-ragã®CorpusManagerã¯ã€ç•°ãªã‚‹ãƒ¬ãƒ™ãƒ«ã®è¤‡é›‘ã•ã«å¯¾å¿œã™ã‚‹3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æä¾›ã—ã¾ã™ï¼š

### 1. ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šï¼ˆPreset Configurationsï¼‰
äº‹å‰å®šç¾©ã•ã‚ŒãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹æˆï¼š
- **Simple RAG**: Load â†’ Chunk â†’ Vector
- **Semantic RAG**: Load â†’ Dictionary â†’ Normalize â†’ Chunk â†’ Vector  
- **Knowledge RAG**: Load â†’ Dictionary â†’ Graph â†’ Normalize â†’ Chunk â†’ Vector

### 2. ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠï¼ˆStage Selectionï¼‰
å€‹åˆ¥ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’é¸æŠã—ã¦ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ï¼š
- `["load", "dictionary", "chunk", "vector"]`
- å„ã‚¹ãƒ†ãƒ¼ã‚¸ã«å€‹åˆ¥è¨­å®šã‚’é©ç”¨å¯èƒ½

### 3. ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆCustom Pipelinesï¼‰
å®Œå…¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼š
- è¤‡æ•°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®é †æ¬¡å®Ÿè¡Œ
- ä»»æ„ã®DocumentProcessorã®çµ„ã¿åˆã‚ã›

## ã‚¹ãƒ†ãƒƒãƒ—1: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ

```python
import tempfile
from pathlib import Path

def create_sample_files_with_variations(temp_dir: Path):
    """è¡¨ç¾æºã‚‰ãã‚’å«ã‚€ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
    
    files = []
    
    # ãƒ•ã‚¡ã‚¤ãƒ«1: RAGã¨ãã®è¡¨ç¾æºã‚‰ã
    file1 = temp_dir / "rag_overview.txt"
    file1.write_text("""
    RAGï¼ˆRetrieval-Augmented Generationï¼‰ã¯é©æ–°çš„ãªAIæŠ€è¡“ã§ã™ã€‚
    æ¤œç´¢æ‹¡å¼µç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦ã€LLMã¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’çµ±åˆã—ã¾ã™ã€‚
    ã“ã®RAGã‚·ã‚¹ãƒ†ãƒ ã¯ä¼æ¥­ã§åºƒãä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚
    æ¤œç´¢å¼·åŒ–ç”Ÿæˆã¨ã‚‚å‘¼ã°ã‚Œã€æƒ…å ±æ¤œç´¢ã¨ç”Ÿæˆã‚’çµ„ã¿åˆã‚ã›ã¾ã™ã€‚
    """, encoding='utf-8')
    files.append(str(file1))
    
    # ãƒ•ã‚¡ã‚¤ãƒ«2: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®è¡¨ç¾æºã‚‰ã
    file2 = temp_dir / "vector_search.txt"
    file2.write_text("""
    ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯æ„å‘³çš„é¡ä¼¼æ€§ã‚’åŸºã«ã—ãŸæ¤œç´¢æ‰‹æ³•ã§ã™ã€‚
    ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã¨ã‚‚å‘¼ã°ã‚Œã¾ã™ã€‚
    æ–‡æ›¸åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ã£ã¦æ„å‘³æ¤œç´¢ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
    å¾“æ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¨ç•°ãªã‚Šã€æ–‡è„ˆã‚’ç†è§£ã—ã¾ã™ã€‚
    """, encoding='utf-8')
    files.append(str(file2))
    
    # ãƒ•ã‚¡ã‚¤ãƒ«3: LLMã¨ãã®ç”¨é€”
    file3 = temp_dir / "llm_applications.txt"
    file3.write_text("""
    å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã¯è‡ªç„¶è¨€èªå‡¦ç†ã®ä¸­æ ¸ã§ã™ã€‚
    è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€æ–‡ç« ç”Ÿæˆã‚„ç¿»è¨³ã‚’è¡Œã„ã¾ã™ã€‚
    LLMãƒ¢ãƒ‡ãƒ«ã¯ä¼æ¥­ã§ã‚‚åºƒãæ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
    GPTã€Claudeã€Geminiãªã©ãŒä»£è¡¨çš„ãªLLMã§ã™ã€‚
    """, encoding='utf-8')
    files.append(str(file3))
    
    return files

def create_test_dictionary(temp_dir: Path):
    """ç”¨èªçµ±ä¸€è¾æ›¸ã‚’ä½œæˆ"""
    
    dict_file = temp_dir / "domain_dictionary.md"
    dict_file.write_text("""# ãƒ‰ãƒ¡ã‚¤ãƒ³ç”¨èªè¾æ›¸

## AIãƒ»æ©Ÿæ¢°å­¦ç¿’ç”¨èª

- **RAG** (Retrieval-Augmented Generation): æ¤œç´¢æ‹¡å¼µç”Ÿæˆ
  - è¡¨ç¾æºã‚‰ã: æ¤œç´¢æ‹¡å¼µç”Ÿæˆ, æ¤œç´¢å¼·åŒ–ç”Ÿæˆ, RAGã‚·ã‚¹ãƒ†ãƒ 

- **ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢** (Vector Search): ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
  - è¡¨ç¾æºã‚‰ã: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢, æ„å‘³æ¤œç´¢, ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢

- **LLM** (Large Language Model): å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«
  - è¡¨ç¾æºã‚‰ã: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«, è¨€èªãƒ¢ãƒ‡ãƒ«, LLMãƒ¢ãƒ‡ãƒ«

- **åŸ‹ã‚è¾¼ã¿** (Embedding): åŸ‹ã‚è¾¼ã¿
  - è¡¨ç¾æºã‚‰ã: åŸ‹ã‚è¾¼ã¿, ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°, ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾
""", encoding='utf-8')
    
    return str(dict_file)
```

## ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šã®æ¯”è¼ƒ

3ã¤ã®ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šã‚’æ¯”è¼ƒã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

```python
def demo_preset_configurations(temp_dir: Path, file_paths: list):
    """ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šã®ãƒ‡ãƒ¢"""
    
    from refinire_rag.use_cases.corpus_manager_new import CorpusManager
    from refinire_rag.storage.sqlite_store import SQLiteDocumentStore
    from refinire_rag.storage.in_memory_vector_store import InMemoryVectorStore
    
    print("\\n" + "="*60)
    print("ğŸ¯ ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šæ¯”è¼ƒãƒ‡ãƒ¢")
    print("="*60)
    
    # 1. Simple RAG
    print("\\nğŸ“Œ Simple RAG: Load â†’ Chunk â†’ Vector")
    doc_store1 = SQLiteDocumentStore(":memory:")
    vector_store1 = InMemoryVectorStore()
    
    simple_manager = CorpusManager.create_simple_rag(doc_store1, vector_store1)
    
    try:
        simple_stats = simple_manager.build_corpus(file_paths)
        print(f"âœ… Simple RAGå®Œäº†:")
        print(f"   - å‡¦ç†æ–‡æ›¸æ•°: {simple_stats.total_documents_created}")
        print(f"   - å‡¦ç†æ™‚é–“: {simple_stats.total_processing_time:.3f}ç§’")
        print(f"   - å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¸æ•°: {simple_stats.pipeline_stages_executed}")
    except Exception as e:
        print(f"âŒ Simple RAGå¤±æ•—: {e}")
    
    # 2. Semantic RAG
    print("\\nğŸ“Œ Semantic RAG: Load â†’ Dictionary â†’ Normalize â†’ Chunk â†’ Vector")
    doc_store2 = SQLiteDocumentStore(":memory:")
    vector_store2 = InMemoryVectorStore()
    
    semantic_manager = CorpusManager.create_semantic_rag(doc_store2, vector_store2)
    
    try:
        semantic_stats = semantic_manager.build_corpus(file_paths)
        print(f"âœ… Semantic RAGå®Œäº†:")
        print(f"   - å‡¦ç†æ–‡æ›¸æ•°: {semantic_stats.total_documents_created}")
        print(f"   - å‡¦ç†æ™‚é–“: {semantic_stats.total_processing_time:.3f}ç§’")
        print(f"   - å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¸æ•°: {semantic_stats.pipeline_stages_executed}")
    except Exception as e:
        print(f"âŒ Semantic RAGå¤±æ•—: {e}")
    
    # 3. Knowledge RAG  
    print("\\nğŸ“Œ Knowledge RAG: Load â†’ Dictionary â†’ Graph â†’ Normalize â†’ Chunk â†’ Vector")
    doc_store3 = SQLiteDocumentStore(":memory:")
    vector_store3 = InMemoryVectorStore()
    
    knowledge_manager = CorpusManager.create_knowledge_rag(doc_store3, vector_store3)
    
    try:
        knowledge_stats = knowledge_manager.build_corpus(file_paths)
        print(f"âœ… Knowledge RAGå®Œäº†:")
        print(f"   - å‡¦ç†æ–‡æ›¸æ•°: {knowledge_stats.total_documents_created}")
        print(f"   - å‡¦ç†æ™‚é–“: {knowledge_stats.total_processing_time:.3f}ç§’")
        print(f"   - å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¸æ•°: {knowledge_stats.pipeline_stages_executed}")
    except Exception as e:
        print(f"âŒ Knowledge RAGå¤±æ•—: {e}")
```

## ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

å€‹åˆ¥ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’é¸æŠã—ã¦ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ï¼š

```python
def demo_stage_selection(temp_dir: Path, file_paths: list, dict_path: str):
    """ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ãƒ‡ãƒ¢"""
    
    from refinire_rag.processing.dictionary_maker import DictionaryMakerConfig
    from refinire_rag.processing.normalizer import NormalizerConfig
    from refinire_rag.processing.chunker import ChunkingConfig
    from refinire_rag.loaders.base import LoaderConfig
    
    print("\\n" + "="*60)
    print("ğŸ›ï¸ ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠã‚¢ãƒ—ãƒ­ãƒ¼ãƒãƒ‡ãƒ¢")
    print("="*60)
    
    # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åˆæœŸåŒ–
    doc_store = SQLiteDocumentStore(":memory:")
    vector_store = InMemoryVectorStore()
    corpus_manager = CorpusManager(doc_store, vector_store)
    
    # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒ†ãƒ¼ã‚¸è¨­å®š
    stage_configs = {
        "loader_config": LoaderConfig(),
        "dictionary_config": DictionaryMakerConfig(
            dictionary_file_path=dict_path,
            focus_on_technical_terms=True,
            extract_abbreviations=True
        ),
        "normalizer_config": NormalizerConfig(
            dictionary_file_path=dict_path,
            normalize_variations=True,
            expand_abbreviations=True,
            whole_word_only=False  # æ—¥æœ¬èªå¯¾å¿œ
        ),
        "chunker_config": ChunkingConfig(
            chunk_size=300,
            overlap=50,
            split_by_sentence=True
        )
    }
    
    # é¸æŠã™ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸
    selected_stages = ["load", "dictionary", "normalize", "chunk", "vector"]
    
    print(f"ğŸ“‹ é¸æŠã‚¹ãƒ†ãƒ¼ã‚¸: {selected_stages}")
    print("ğŸ“ å„ã‚¹ãƒ†ãƒ¼ã‚¸ã®è¨­å®š:")
    for key, config in stage_configs.items():
        print(f"   - {key}: {type(config).__name__}")
    
    try:
        stats = corpus_manager.build_corpus(
            file_paths=file_paths,
            stages=selected_stages,
            stage_configs=stage_configs
        )
        
        print(f"\\nâœ… ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†:")
        print(f"   - å‡¦ç†æ–‡æ›¸æ•°: {stats.total_documents_created}")
        print(f"   - ãƒãƒ£ãƒ³ã‚¯æ•°: {stats.total_chunks_created}")
        print(f"   - å‡¦ç†æ™‚é–“: {stats.total_processing_time:.3f}ç§’")
        print(f"   - ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥æ–‡æ›¸æ•°: {stats.documents_by_stage}")
        
        # ç”Ÿæˆã•ã‚ŒãŸè¾æ›¸ã®ç¢ºèª
        if Path(dict_path).exists():
            print(f"\\nğŸ“– ç”Ÿæˆã•ã‚ŒãŸè¾æ›¸:")
            with open(dict_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\\n')[:10]
                for line in lines:
                    if line.strip():
                        print(f"   {line}")
                if len(content.split('\\n')) > 10:
                    print(f"   ... (ä»–{len(content.split('\\n')) - 10}è¡Œ)")
        
    except Exception as e:
        print(f"âŒ ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
```

## ã‚¹ãƒ†ãƒƒãƒ—4: ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

å®Œå…¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼š

```python
def demo_custom_pipelines(temp_dir: Path, file_paths: list, dict_path: str):
    """ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ‡ãƒ¢"""
    
    from refinire_rag.processing.document_pipeline import DocumentPipeline
    from refinire_rag.processing.document_store_processor import DocumentStoreProcessor
    from refinire_rag.processing.document_store_loader import DocumentStoreLoader, DocumentStoreLoaderConfig
    from refinire_rag.loaders.specialized import TextLoader
    from refinire_rag.processing.dictionary_maker import DictionaryMaker, DictionaryMakerConfig
    from refinire_rag.processing.normalizer import Normalizer, NormalizerConfig
    from refinire_rag.processing.chunker import Chunker, ChunkingConfig
    from refinire_rag.loaders.base import LoaderConfig
    
    print("\\n" + "="*60)
    print("ğŸ”§ ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¢")
    print("="*60)
    
    # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åˆæœŸåŒ–
    doc_store = SQLiteDocumentStore(":memory:")
    vector_store = InMemoryVectorStore()
    
    # ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®šç¾©
    custom_pipelines = [
        # ã‚¹ãƒ†ãƒ¼ã‚¸1: ãƒ­ãƒ¼ãƒ‰ã¨åŸæ–‡ä¿å­˜
        DocumentPipeline([
            TextLoader(LoaderConfig()),
            DocumentStoreProcessor(doc_store)
        ]),
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸2: è¾æ›¸ä½œæˆï¼ˆåŸæ–‡ã‹ã‚‰ï¼‰
        DocumentPipeline([
            DocumentStoreLoader(doc_store, 
                              config=DocumentStoreLoaderConfig(processing_stage="original")),
            DictionaryMaker(DictionaryMakerConfig(
                dictionary_file_path=str(temp_dir / "custom_dictionary.md"),
                focus_on_technical_terms=True
            ))
        ]),
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸3: æ­£è¦åŒ–ã¨ä¿å­˜
        DocumentPipeline([
            DocumentStoreLoader(doc_store,
                              config=DocumentStoreLoaderConfig(processing_stage="original")),
            Normalizer(NormalizerConfig(
                dictionary_file_path=str(temp_dir / "custom_dictionary.md"),
                normalize_variations=True,
                whole_word_only=False
            )),
            DocumentStoreProcessor(doc_store)
        ]),
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸4: ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
        DocumentPipeline([
            DocumentStoreLoader(doc_store,
                              config=DocumentStoreLoaderConfig(processing_stage="normalized")),
            Chunker(ChunkingConfig(
                chunk_size=200,
                overlap=30,
                split_by_sentence=True
            ))
        ])
    ]
    
    print(f"ğŸ“‹ ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹æˆ:")
    for i, pipeline in enumerate(custom_pipelines, 1):
        processors = [type(p).__name__ for p in pipeline.processors]
        print(f"   ã‚¹ãƒ†ãƒ¼ã‚¸{i}: {' â†’ '.join(processors)}")
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    corpus_manager = CorpusManager(doc_store, vector_store)
    
    try:
        stats = corpus_manager.build_corpus(
            file_paths=file_paths,
            custom_pipelines=custom_pipelines
        )
        
        print(f"\\nâœ… ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†:")
        print(f"   - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ•°: {len(custom_pipelines)}")
        print(f"   - å‡¦ç†æ–‡æ›¸æ•°: {stats.total_documents_created}")
        print(f"   - ãƒãƒ£ãƒ³ã‚¯æ•°: {stats.total_chunks_created}")
        print(f"   - å‡¦ç†æ™‚é–“: {stats.total_processing_time:.3f}ç§’")
        print(f"   - ã‚¨ãƒ©ãƒ¼æ•°: {stats.errors_encountered}")
        
    except Exception as e:
        print(f"âŒ ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
```

## ã‚¹ãƒ†ãƒƒãƒ—5: æ­£è¦åŒ–åŠ¹æœã®ç¢ºèª

ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ­£è¦åŒ–ã®åŠ¹æœã‚’ç¢ºèªï¼š

```python
def demonstrate_normalization_effects(temp_dir: Path):
    """æ­£è¦åŒ–åŠ¹æœã®ãƒ‡ãƒ¢"""
    
    from refinire_rag.processing.normalizer import Normalizer, NormalizerConfig
    from refinire_rag.models.document import Document
    
    print("\\n" + "="*60)
    print("ğŸ”„ æ­£è¦åŒ–åŠ¹æœãƒ‡ãƒ¢")
    print("="*60)
    
    # è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    dict_path = create_test_dictionary(temp_dir)
    
    # æ­£è¦åŒ–è¨­å®š
    normalizer_config = NormalizerConfig(
        dictionary_file_path=dict_path,
        normalize_variations=True,
        expand_abbreviations=True,
        whole_word_only=False
    )
    
    normalizer = Normalizer(normalizer_config)
    
    # ãƒ†ã‚¹ãƒˆæ–‡æ›¸
    test_texts = [
        "æ¤œç´¢å¼·åŒ–ç”Ÿæˆã¯é©æ–°çš„ãªæŠ€è¡“ã§ã™",
        "æ„å‘³æ¤œç´¢ã®ä»•çµ„ã¿ã‚’èª¬æ˜ã—ã¾ã™", 
        "LLMãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã«ã¤ã„ã¦",
        "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã¨RAGã‚·ã‚¹ãƒ†ãƒ "
    ]
    
    print("ğŸ“ æ­£è¦åŒ–å‰å¾Œã®æ¯”è¼ƒ:")
    print("-" * 50)
    
    for i, text in enumerate(test_texts, 1):
        # æ­£è¦åŒ–å®Ÿè¡Œ
        doc = Document(id=f"test_{i}", content=text, metadata={})
        normalized_docs = normalizer.process(doc)
        
        normalized_text = normalized_docs[0].content if normalized_docs else text
        
        print(f"\\n{i}. å…ƒã®æ–‡ç« :")
        print(f"   ã€Œ{text}ã€")
        print(f"   æ­£è¦åŒ–å¾Œ:")
        print(f"   ã€Œ{normalized_text}ã€")
        
        if text != normalized_text:
            print(f"   ğŸ”„ å¤‰æ›´: ã‚ã‚Š")
        else:
            print(f"   ğŸ”„ å¤‰æ›´: ãªã—")
```

## ã‚¹ãƒ†ãƒƒãƒ—6: å®Œå…¨ãªã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚°ãƒ©ãƒ 

å…¨ã¦ã®æ©Ÿèƒ½ã‚’çµ±åˆã—ãŸã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ï¼š

```python
#!/usr/bin/env python3
"""
ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«2: ã‚³ãƒ¼ãƒ‘ã‚¹ç®¡ç†ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
"""

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    print("ğŸš€ ã‚³ãƒ¼ãƒ‘ã‚¹ç®¡ç†ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç† ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«")
    print("="*60)
    print("é«˜åº¦ãªã‚³ãƒ¼ãƒ‘ã‚¹ç®¡ç†æ©Ÿèƒ½ã¨ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸å‡¦ç†ã‚’å­¦ç¿’ã—ã¾ã™")
    
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        print("\\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ")
        file_paths = create_sample_files_with_variations(temp_dir)
        dict_path = create_test_dictionary(temp_dir)
        print(f"âœ… {len(file_paths)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨è¾æ›¸ã‚’ä½œæˆ")
        
        # ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šã®ãƒ‡ãƒ¢
        demo_preset_configurations(temp_dir, file_paths)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠã®ãƒ‡ãƒ¢
        demo_stage_selection(temp_dir, file_paths, dict_path)
        
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ‡ãƒ¢
        demo_custom_pipelines(temp_dir, file_paths, dict_path)
        
        # æ­£è¦åŒ–åŠ¹æœã®ãƒ‡ãƒ¢
        demonstrate_normalization_effects(temp_dir)
        
        print("\\nğŸ‰ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«2ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("\\nğŸ“š å­¦ç¿’å†…å®¹:")
        print("   âœ… 3ã¤ã®ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šï¼ˆSimple/Semantic/Knowledge RAGï¼‰")
        print("   âœ… ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠã«ã‚ˆã‚‹æŸ”è»Ÿãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰")
        print("   âœ… ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ã‚ˆã‚‹å®Œå…¨åˆ¶å¾¡")
        print("   âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ­£è¦åŒ–ã¨è¡¨ç¾æºã‚‰ãçµ±ä¸€")
        
        print(f"\\nğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«: {temp_dir}")
        
    except Exception as e:
        print(f"\\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    main()
```

## å®Ÿè¡Œã¨çµæœ

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå‡¦ç†ãƒ•ãƒ­ãƒ¼ãŒç¢ºèªã§ãã¾ã™ï¼š

### ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šã®æ¯”è¼ƒ
- Simple RAG: åŸºæœ¬çš„ãªãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
- Semantic RAG: ç”¨èªæ­£è¦åŒ–ã«ã‚ˆã‚‹æ¤œç´¢ç²¾åº¦å‘ä¸Š
- Knowledge RAG: ã‚°ãƒ©ãƒ•æƒ…å ±ã‚‚æ´»ç”¨ã—ãŸé«˜åº¦ãªå‡¦ç†

### ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠã®æŸ”è»Ÿæ€§
- å¿…è¦ãªã‚¹ãƒ†ãƒ¼ã‚¸ã®ã¿é¸æŠ
- å„ã‚¹ãƒ†ãƒ¼ã‚¸ã®è©³ç´°è¨­å®š
- æ®µéšçš„ãªå“è³ªå‘ä¸Š

### ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å¨åŠ›
- è¤‡æ•°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®é †æ¬¡å®Ÿè¡Œ
- DocumentStoreã‚’ä»‹ã—ãŸãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–
- å‡¦ç†æ®µéšã®å®Œå…¨åˆ¶å¾¡

## ç†è§£åº¦ãƒã‚§ãƒƒã‚¯

1. **3ã¤ã®ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®š**ã®é•ã„ã¯ï¼Ÿ
2. **ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠ**ã§å¯èƒ½ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã¯ï¼Ÿ
3. **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ­£è¦åŒ–**ã®åŠ¹æœã¯ï¼Ÿ
4. **ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ã®åˆ©ç‚¹ã¯ï¼Ÿ

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

[ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«3: ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã¨å›ç­”ç”Ÿæˆ](tutorial_03_query_engine.md)ã§ã€æ§‹ç¯‰ã—ãŸã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ã£ãŸé«˜åº¦ãªã‚¯ã‚¨ãƒªå‡¦ç†ã‚’å­¦ç¿’ã—ã¾ã—ã‚‡ã†ã€‚