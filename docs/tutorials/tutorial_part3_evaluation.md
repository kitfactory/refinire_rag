# Part 3: RAGè©•ä¾¡ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

## Overview / æ¦‚è¦

This tutorial demonstrates how to evaluate RAG system performance using refinire-rag's QualityLab. QualityLab provides comprehensive evaluation capabilities including automated QA pair generation, answer quality assessment, contradiction detection, and detailed reporting.

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€refinire-ragã®QualityLabã‚’ä½¿ç”¨ã—ãŸRAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚QualityLabã¯ã€è‡ªå‹•QAãƒšã‚¢ç”Ÿæˆã€å›ç­”å“è³ªè©•ä¾¡ã€çŸ›ç›¾æ¤œå‡ºã€è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’å«ã‚€åŒ…æ‹¬çš„ãªè©•ä¾¡æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

## Learning Objectives / å­¦ç¿’ç›®æ¨™

- Understand RAG evaluation methodologies / RAGè©•ä¾¡æ‰‹æ³•ã®ç†è§£
- Generate automated test datasets / è‡ªå‹•ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆ
- Evaluate QueryEngine performance / QueryEngineãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è©•ä¾¡
- Analyze answer quality and accuracy / å›ç­”å“è³ªã¨ç²¾åº¦ã®åˆ†æ
- Detect contradictions and inconsistencies / çŸ›ç›¾ã¨ä¸æ•´åˆã®æ¤œå‡º
- Generate comprehensive evaluation reports / åŒ…æ‹¬çš„ãªè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ

## Prerequisites / å‰ææ¡ä»¶

```bash
# Complete Part 1 (Corpus Creation) and Part 2 (QueryEngine)
# Part 1ï¼ˆã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆï¼‰ã¨Part 2ï¼ˆQueryEngineï¼‰ã‚’å®Œäº†

# Set environment variables for LLM-based evaluation
export OPENAI_API_KEY="your-api-key"
export REFINIRE_RAG_LLM_MODEL="gpt-4o-mini"
export REFINIRE_RAG_EVALUATION_MODEL="gpt-4"  # For higher quality evaluation
```

## Quick Start Example / ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆä¾‹

```python
from refinire_rag.application.quality_lab import QualityLab, QualityLabConfig
from refinire_rag.application.query_engine import QueryEngine

# Initialize QualityLab
config = QualityLabConfig(
    qa_pairs_per_document=3,
    similarity_threshold=0.8,
    include_detailed_analysis=True
)

quality_lab = QualityLab(
    corpus_name="my_corpus",
    config=config
)

# Generate test cases and evaluate
qa_pairs = quality_lab.generate_qa_pairs(documents, num_pairs=20)
results = quality_lab.evaluate_query_engine(query_engine, qa_pairs)
report = quality_lab.generate_evaluation_report(results, "evaluation_report.md")
```

## 1. QualityLab Architecture / QualityLab ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### 1.1 Core Components / ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

```python
from refinire_rag.application.quality_lab import QualityLab, QualityLabConfig
from refinire_rag.processing.evaluator import (
    BaseEvaluator, BleuEvaluator, RougeEvaluator, 
    LLMJudgeEvaluator, QuestEvalEvaluator
)
from refinire_rag.processing.contradiction_detector import ContradictionDetector
from refinire_rag.processing.test_suite import TestSuite

# QualityLab configuration / QualityLabè¨­å®š
config = QualityLabConfig(
    qa_pairs_per_document=2,           # QA pairs to generate per document
    similarity_threshold=0.75,         # Minimum similarity for answer matching
    question_types=[                   # Types of questions to generate
        "factual",                     # Direct fact extraction
        "conceptual",                  # Concept understanding
        "analytical",                  # Analysis and reasoning
        "comparative",                 # Comparison questions
        "application"                  # Application scenarios
    ],
    evaluation_metrics=[               # Metrics to calculate
        "bleu", "rouge", "llm_judge", "questeval"
    ],
    include_detailed_analysis=True,    # Include detailed analysis
    include_contradiction_detection=True,  # Check for contradictions
    output_format="markdown",          # Report format
    llm_model="gpt-4o-mini",          # Model for QA generation
    evaluation_model="gpt-4"          # Model for evaluation (higher quality)
)

# Initialize QualityLab / QualityLabåˆæœŸåŒ–
quality_lab = QualityLab(
    corpus_name="technical_knowledge_base",
    config=config
)
```

### 1.2 Evaluation Metrics / è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
# Available evaluation metrics / åˆ©ç”¨å¯èƒ½ãªè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹

# 1. BLEU Score - N-gram based similarity
bleu_evaluator = BleuEvaluator(
    max_ngram=4,
    smooth=True
)

# 2. ROUGE Score - Recall-oriented similarity
rouge_evaluator = RougeEvaluator(
    rouge_types=["rouge-1", "rouge-2", "rouge-l"],
    use_stemmer=True
)

# 3. LLM Judge - Model-based quality assessment
llm_judge = LLMJudgeEvaluator(
    model="gpt-4",
    criteria=[
        "accuracy",      # Factual correctness
        "completeness",  # Information completeness
        "relevance",     # Query relevance
        "coherence",     # Answer coherence
        "helpfulness"    # Overall helpfulness
    ],
    scale=5  # 1-5 rating scale
)

# 4. QuestEval - Question-answering evaluation
questeval = QuestEvalEvaluator(
    model="gpt-4o-mini",
    check_answerability=True,
    check_consistency=True
)
```

## 2. QA Pair Management / QAãƒšã‚¢ç®¡ç†

### 2.1 Registering Existing QA Pairs / æ—¢å­˜QAãƒšã‚¢ã®ç™»éŒ²

If you already have high-quality QA pairs from human experts, domain specialists, or previous evaluations, you can register them directly in QualityLab for evaluation purposes.

ã™ã§ã«äººé–“ã®å°‚é–€å®¶ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚¹ãƒšã‚·ãƒ£ãƒªã‚¹ãƒˆã€ã¾ãŸã¯éå»ã®è©•ä¾¡ã‹ã‚‰é«˜å“è³ªãªQAãƒšã‚¢ãŒã‚ã‚‹å ´åˆã€è©•ä¾¡ç›®çš„ã§QualityLabã«ç›´æ¥ç™»éŒ²ã§ãã¾ã™ã€‚

```python
from refinire_rag.models.qa_pair import QAPair

# Create existing QA pairs from expert knowledge / å°‚é–€çŸ¥è­˜ã‹ã‚‰æ—¢å­˜QAãƒšã‚¢ã‚’ä½œæˆ
expert_qa_pairs = [
    QAPair(
        question="What is the fundamental difference between RAG and traditional LLM approaches?",
        answer="RAG combines information retrieval with text generation, allowing LLMs to access external knowledge bases in real-time, while traditional approaches rely solely on pre-trained parameters.",
        document_id="rag_concepts_001",
        metadata={
            "qa_id": "expert_001",
            "question_type": "conceptual",
            "topic": "rag_fundamentals", 
            "difficulty": "intermediate",
            "source": "domain_expert",
            "expert_reviewer": "Dr. Smith",
            "review_date": "2024-01-15",
            "expected_sources": ["rag_concepts_001", "rag_architecture_002"]
        }
    ),
    QAPair(
        question="How does semantic search improve retrieval accuracy in RAG systems?",
        answer="Semantic search uses dense vector embeddings to capture meaning rather than just keyword matching, enabling retrieval of contextually relevant documents even when exact terms don't match.",
        document_id="semantic_search_002",
        metadata={
            "qa_id": "expert_002",
            "question_type": "technical",
            "topic": "semantic_search",
            "difficulty": "advanced",
            "source": "technical_review",
            "expert_reviewer": "Engineering Team",
            "review_date": "2024-01-16",
            "expected_sources": ["semantic_search_002", "vector_embeddings_003"]
        }
    ),
    QAPair(
        question="What evaluation metrics best assess RAG system performance?",
        answer="Key metrics include retrieval metrics (Hit Rate, MRR, NDCG), generation quality (BLEU, ROUGE, BERTScore), and end-to-end metrics (Faithfulness, Answer Relevance, Context Precision/Recall).",
        document_id="evaluation_metrics_003",
        metadata={
            "qa_id": "expert_003",
            "question_type": "analytical", 
            "topic": "evaluation_metrics",
            "difficulty": "advanced",
            "source": "research_paper",
            "citation": "Smith et al. 2024",
            "expected_sources": ["evaluation_metrics_003", "rag_benchmarks_004"]
        }
    )
]

# Register the expert QA pairs / å°‚é–€å®¶QAãƒšã‚¢ã‚’ç™»éŒ²
print("ğŸ“‹ Registering expert QA pairs...")
registration_success = quality_lab.register_qa_pairs(
    qa_pairs=expert_qa_pairs,
    qa_set_name="expert_rag_knowledge_v1",
    metadata={
        "collection_source": "domain_experts",
        "validation_status": "expert_reviewed",
        "intended_use": "benchmark_evaluation",
        "creation_date": "2024-01-15",
        "quality_level": "gold_standard",
        "language": "english",
        "domain": "rag_systems"
    }
)

if registration_success:
    print("âœ… Successfully registered expert QA pairs!")
    
    # Verify registration metadata / ç™»éŒ²ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
    print("\nRegistered QA Pairs Summary:")
    for qa_pair in expert_qa_pairs:
        print(f"- {qa_pair.metadata['qa_id']}: {qa_pair.question[:60]}...")
        print(f"  Enhanced metadata: qa_set_name, registration_timestamp added")
    
    # Get QualityLab statistics / QualityLabçµ±è¨ˆã‚’å–å¾—
    stats = quality_lab.get_lab_stats()
    print(f"\nTotal QA pairs in QualityLab: {stats['qa_pairs_generated']}")
else:
    print("âŒ Failed to register QA pairs")
```

### 2.2 Loading QA Pairs from External Sources / å¤–éƒ¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®QAãƒšã‚¢èª­ã¿è¾¼ã¿

```python
import json
import csv
from pathlib import Path

def load_qa_pairs_from_json(file_path: str) -> List[QAPair]:
    """
    Load QA pairs from JSON file
    JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰QAãƒšã‚¢ã‚’èª­ã¿è¾¼ã¿
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    qa_pairs = []
    for item in data:
        qa_pair = QAPair(
            question=item['question'],
            answer=item['answer'],
            document_id=item.get('document_id', 'unknown'),
            metadata={
                'qa_id': item.get('id', f"imported_{len(qa_pairs)}"),
                'question_type': item.get('type', 'imported'),
                'source_file': file_path,
                'import_timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                **item.get('metadata', {})
            }
        )
        qa_pairs.append(qa_pair)
    
    return qa_pairs

def load_qa_pairs_from_csv(file_path: str) -> List[QAPair]:
    """
    Load QA pairs from CSV file
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰QAãƒšã‚¢ã‚’èª­ã¿è¾¼ã¿
    """
    qa_pairs = []
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for i, row in enumerate(reader):
            qa_pair = QAPair(
                question=row['question'],
                answer=row['answer'],
                document_id=row.get('document_id', 'csv_import'),
                metadata={
                    'qa_id': row.get('id', f"csv_{i}"),
                    'question_type': row.get('type', 'imported'),
                    'source_file': file_path,
                    'import_timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                    'original_row': i
                }
            )
            qa_pairs.append(qa_pair)
    
    return qa_pairs

# Example usage / ä½¿ç”¨ä¾‹
if Path("expert_qa_pairs.json").exists():
    imported_qa_pairs = load_qa_pairs_from_json("expert_qa_pairs.json")
    quality_lab.register_qa_pairs(
        qa_pairs=imported_qa_pairs,
        qa_set_name="imported_expert_set",
        metadata={"source": "json_import"}
    )
    print(f"Imported {len(imported_qa_pairs)} QA pairs from JSON")

if Path("evaluation_dataset.csv").exists():
    csv_qa_pairs = load_qa_pairs_from_csv("evaluation_dataset.csv")
    quality_lab.register_qa_pairs(
        qa_pairs=csv_qa_pairs,
        qa_set_name="csv_evaluation_set",
        metadata={"source": "csv_import"}
    )
    print(f"Imported {len(csv_qa_pairs)} QA pairs from CSV")
```

### 2.3 Automated QA Pair Generation / è‡ªå‹•QAãƒšã‚¢ç”Ÿæˆ

### 2.3.1 Basic QA Generation / åŸºæœ¬QAç”Ÿæˆ

```python
from refinire_rag.models.document import Document

# Sample documents for evaluation / è©•ä¾¡ç”¨ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸
documents = [
    Document(
        id="ai_doc_001",
        content="""
        Artificial Intelligence (AI) is the simulation of human intelligence 
        in machines programmed to think like humans. Key applications include 
        machine learning, natural language processing, and computer vision.
        """,
        metadata={
            "source": "AI Textbook",
            "topic": "ai_fundamentals",
            "difficulty": "beginner"
        }
    ),
    Document(
        id="ml_doc_002",
        content="""
        Machine Learning is a subset of AI that enables computers to learn 
        without explicit programming. Popular algorithms include neural networks, 
        decision trees, and support vector machines.
        """,
        metadata={
            "source": "ML Guide",
            "topic": "machine_learning",
            "difficulty": "intermediate"
        }
    )
]

# Generate QA pairs / QAãƒšã‚¢ç”Ÿæˆ
qa_pairs = quality_lab.generate_qa_pairs(
    documents=documents,
    num_pairs=10,
    question_types=["factual", "conceptual", "analytical"]
)

print(f"Generated {len(qa_pairs)} QA pairs")

# Inspect generated QA pairs / ç”Ÿæˆã•ã‚ŒãŸQAãƒšã‚¢ã‚’ç¢ºèª
for i, qa_pair in enumerate(qa_pairs[:3]):
    print(f"\nQA Pair {i+1}:")
    print(f"Document: {qa_pair.document_id}")
    print(f"Question Type: {qa_pair.metadata['question_type']}")
    print(f"Question: {qa_pair.question}")
    print(f"Expected Answer: {qa_pair.answer[:100]}...")
    print(f"Difficulty: {qa_pair.metadata.get('difficulty', 'N/A')}")
```

### 2.2 Advanced QA Generation / é«˜åº¦ãªQAç”Ÿæˆ

```python
# Custom QA generation with specific instructions / ç‰¹å®šæŒ‡ç¤ºä»˜ãã‚«ã‚¹ã‚¿ãƒ QAç”Ÿæˆ
custom_config = QualityLabConfig(
    qa_pairs_per_document=3,
    question_types=["factual", "conceptual", "analytical", "comparative"],
    qa_generation_instructions="""
    Generate high-quality question-answer pairs that test deep understanding:
    
    1. Factual questions should test specific details and definitions
    2. Conceptual questions should test understanding of key concepts
    3. Analytical questions should require reasoning and analysis
    4. Comparative questions should test ability to compare concepts
    
    Ensure questions are:
    - Clear and unambiguous
    - Answerable from the provided content
    - Varied in complexity
    - Relevant to the domain
    """,
    include_context_questions=True,    # Questions requiring context
    include_edge_cases=True,           # Test edge cases
    difficulty_levels=["beginner", "intermediate", "advanced"]
)

# Generate with custom configuration / ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ç”Ÿæˆ
custom_qa_pairs = quality_lab.generate_qa_pairs(
    documents=documents,
    num_pairs=15,
    config_override=custom_config
)

# Analyze question diversity / è³ªå•ã®å¤šæ§˜æ€§ã‚’åˆ†æ
question_types = {}
difficulty_levels = {}

for qa_pair in custom_qa_pairs:
    q_type = qa_pair.metadata.get('question_type', 'unknown')
    difficulty = qa_pair.metadata.get('difficulty', 'unknown')
    
    question_types[q_type] = question_types.get(q_type, 0) + 1
    difficulty_levels[difficulty] = difficulty_levels.get(difficulty, 0) + 1

print("Question Type Distribution:")
for q_type, count in question_types.items():
    print(f"  {q_type}: {count}")

print("Difficulty Distribution:")
for difficulty, count in difficulty_levels.items():
    print(f"  {difficulty}: {count}")
```

## 3. QueryEngine Evaluation / QueryEngineè©•ä¾¡

### 3.1 Basic Performance Evaluation / åŸºæœ¬ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡

```python
# Evaluate QueryEngine with generated QA pairs
# ç”Ÿæˆã•ã‚ŒãŸQAãƒšã‚¢ã§QueryEngineã‚’è©•ä¾¡
evaluation_results = quality_lab.evaluate_query_engine(
    query_engine=query_engine,
    qa_pairs=qa_pairs,
    evaluation_metrics=["bleu", "rouge", "llm_judge"],
    include_contradiction_detection=True,
    parallel_evaluation=True  # Speed up evaluation
)

print("Evaluation Results Summary:")
print(f"Total test cases: {len(evaluation_results['test_results'])}")
print(f"Processing time: {evaluation_results['processing_time']:.2f}s")

# Calculate pass rate / åˆæ ¼ç‡ã‚’è¨ˆç®—
passed_tests = sum(1 for result in evaluation_results['test_results'] if result['passed'])
pass_rate = (passed_tests / len(evaluation_results['test_results'])) * 100

print(f"Pass rate: {pass_rate:.1f}% ({passed_tests}/{len(evaluation_results['test_results'])})")

# Show metric summaries / ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¦ç´„ã‚’è¡¨ç¤º
if 'metric_summaries' in evaluation_results:
    print("\nMetric Summaries:")
    for metric, summary in evaluation_results['metric_summaries'].items():
        print(f"  {metric.upper()}:")
        print(f"    Average: {summary.get('average', 0):.3f}")
        print(f"    Min: {summary.get('min', 0):.3f}")
        print(f"    Max: {summary.get('max', 0):.3f}")
```

### 3.2 Detailed Analysis / è©³ç´°åˆ†æ

```python
def analyze_evaluation_results(evaluation_results):
    """
    Perform detailed analysis of evaluation results
    è©•ä¾¡çµæœã®è©³ç´°åˆ†æã‚’å®Ÿè¡Œ
    """
    
    test_results = evaluation_results['test_results']
    
    print("="*60)
    print("DETAILED EVALUATION ANALYSIS / è©³ç´°è©•ä¾¡åˆ†æ")
    print("="*60)
    
    # Performance by question type / è³ªå•ã‚¿ã‚¤ãƒ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    performance_by_type = {}
    for result in test_results:
        q_type = result.get('question_type', 'unknown')
        if q_type not in performance_by_type:
            performance_by_type[q_type] = {'total': 0, 'passed': 0, 'scores': []}
        
        performance_by_type[q_type]['total'] += 1
        if result['passed']:
            performance_by_type[q_type]['passed'] += 1
        performance_by_type[q_type]['scores'].append(result.get('score', 0))
    
    print("\nğŸ“Š Performance by Question Type / è³ªå•ã‚¿ã‚¤ãƒ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
    for q_type, stats in performance_by_type.items():
        pass_rate = (stats['passed'] / stats['total']) * 100
        avg_score = sum(stats['scores']) / len(stats['scores'])
        print(f"  {q_type.capitalize()}:")
        print(f"    Pass rate / åˆæ ¼ç‡: {pass_rate:.1f}% ({stats['passed']}/{stats['total']})")
        print(f"    Average score / å¹³å‡ã‚¹ã‚³ã‚¢: {avg_score:.3f}")
    
    # Response time analysis / å¿œç­”æ™‚é–“åˆ†æ
    response_times = [result.get('processing_time', 0) for result in test_results]
    if response_times:
        avg_time = sum(response_times) / len(response_times)
        min_time = min(response_times)
        max_time = max(response_times)
        
        print(f"\nâ±ï¸  Response Time Analysis / å¿œç­”æ™‚é–“åˆ†æ:")
        print(f"   Average: {avg_time:.3f}s")
        print(f"   Fastest: {min_time:.3f}s")
        print(f"   Slowest: {max_time:.3f}s")
    
    # Confidence analysis / ä¿¡é ¼åº¦åˆ†æ
    confidences = [result.get('confidence', 0) for result in test_results if 'confidence' in result]
    if confidences:
        avg_confidence = sum(confidences) / len(confidences)
        high_confidence = sum(1 for c in confidences if c > 0.8)
        low_confidence = sum(1 for c in confidences if c < 0.3)
        
        print(f"\nğŸ¯ Confidence Analysis / ä¿¡é ¼åº¦åˆ†æ:")
        print(f"   Average confidence / å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.3f}")
        print(f"   High confidence (>0.8) / é«˜ä¿¡é ¼åº¦: {high_confidence}/{len(confidences)}")
        print(f"   Low confidence (<0.3) / ä½ä¿¡é ¼åº¦: {low_confidence}/{len(confidences)}")
    
    # Error analysis / ã‚¨ãƒ©ãƒ¼åˆ†æ
    failed_tests = [result for result in test_results if not result['passed']]
    if failed_tests:
        print(f"\nâŒ Failed Test Analysis / å¤±æ•—ãƒ†ã‚¹ãƒˆåˆ†æ:")
        print(f"   Total failures / ç·å¤±æ•—æ•°: {len(failed_tests)}")
        
        # Group failures by reason / ç†ç”±åˆ¥å¤±æ•—ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        failure_reasons = {}
        for result in failed_tests:
            reason = result.get('failure_reason', 'unknown')
            failure_reasons[reason] = failure_reasons.get(reason, 0) + 1
        
        print("   Failure reasons / å¤±æ•—ç†ç”±:")
        for reason, count in failure_reasons.items():
            print(f"     {reason}: {count}")

# Perform detailed analysis / è©³ç´°åˆ†æã‚’å®Ÿè¡Œ
analyze_evaluation_results(evaluation_results)
```

### 3.3 Contradiction Detection / çŸ›ç›¾æ¤œå‡º

```python
# Advanced contradiction detection / é«˜åº¦ãªçŸ›ç›¾æ¤œå‡º
contradiction_results = quality_lab.detect_contradictions(
    corpus_documents=documents,
    query_engine=query_engine,
    test_queries=[
        "What is artificial intelligence?",
        "How does machine learning work?",
        "What are the applications of AI?"
    ]
)

print("Contradiction Detection Results:")
print(f"Documents analyzed: {len(documents)}")
print(f"Contradictions found: {len(contradiction_results['contradictions'])}")

# Analyze contradictions / çŸ›ç›¾ã‚’åˆ†æ
if contradiction_results['contradictions']:
    print("\nDetected Contradictions:")
    for i, contradiction in enumerate(contradiction_results['contradictions'][:3]):
        print(f"\n{i+1}. Contradiction:")
        print(f"   Statement 1: {contradiction['statement_1'][:100]}...")
        print(f"   Statement 2: {contradiction['statement_2'][:100]}...")
        print(f"   Confidence: {contradiction['confidence']:.3f}")
        print(f"   Type: {contradiction['type']}")
        print(f"   Source documents: {contradiction['source_documents']}")

# Consistency check across answers / å›ç­”é–“ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
consistency_results = quality_lab.check_answer_consistency(
    query_engine=query_engine,
    similar_queries=[
        ["What is AI?", "Define artificial intelligence", "Explain AI"],
        ["How does ML work?", "Explain machine learning", "Describe ML process"]
    ]
)

print(f"\nConsistency Analysis:")
print(f"Query groups tested: {len(similar_queries)}")
print(f"Average consistency score: {consistency_results['average_consistency']:.3f}")
```

## 4. Evaluation Metrics Deep Dive / è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©³ç´°è§£èª¬

### 4.1 Understanding Evaluation Metrics / è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç†è§£

Before using evaluation metrics, it's crucial to understand what each metric measures and when to use them.

è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒä½•ã‚’æ¸¬å®šã—ã€ã„ã¤ä½¿ç”¨ã™ã‚‹ã‹ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

#### 4.1.1 Evaluation Perspectives / è©•ä¾¡ã®è¦³ç‚¹

RAG system evaluation should be approached from multiple perspectives to ensure comprehensive assessment:

RAGã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡ã¯ã€åŒ…æ‹¬çš„ãªè©•ä¾¡ã‚’ç¢ºå®Ÿã«ã™ã‚‹ãŸã‚ã«è¤‡æ•°ã®è¦³ç‚¹ã‹ã‚‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š

```python
# Evaluation Framework by Perspective / è¦³ç‚¹åˆ¥è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

evaluation_perspectives = {
    "linguistic_quality": {
        "description": "How well-formed and natural is the generated text?",
        "description_ja": "ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã¯ã©ã®ç¨‹åº¦è‡ªç„¶ã§é©åˆ‡ã«å½¢æˆã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ",
        "metrics": ["bleu", "rouge", "perplexity", "fluency_score"],
        "focus": "Text surface quality and linguistic correctness",
        "focus_ja": "ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨é¢çš„å“è³ªã¨è¨€èªçš„æ­£ç¢ºæ€§",
        "key_questions": [
            "Is the grammar correct?",
            "Does it sound natural?", 
            "Is the vocabulary appropriate?",
            "Are there any linguistic errors?"
        ],
        "key_questions_ja": [
            "æ–‡æ³•ã¯æ­£ã—ã„ã‹ï¼Ÿ",
            "è‡ªç„¶ã«èã“ãˆã‚‹ã‹ï¼Ÿ",
            "èªå½™ã¯é©åˆ‡ã‹ï¼Ÿ",
            "è¨€èªçš„ã‚¨ãƒ©ãƒ¼ã¯ãªã„ã‹ï¼Ÿ"
        ]
    },
    
    "semantic_accuracy": {
        "description": "How accurately does the answer preserve meaning?",
        "description_ja": "å›ç­”ã¯ã©ã®ç¨‹åº¦æ­£ç¢ºã«æ„å‘³ã‚’ä¿æŒã—ã¦ã„ã‚‹ã‹ï¼Ÿ",
        "metrics": ["bertscore", "semantic_similarity", "meaning_preservation"],
        "focus": "Semantic correctness and meaning alignment",
        "focus_ja": "æ„å‘³çš„æ­£ç¢ºæ€§ã¨æ„å‘³ã®ä¸€è‡´",
        "key_questions": [
            "Does the answer convey the correct meaning?",
            "Are the concepts accurately represented?",
            "Is the semantic relationship preserved?",
            "How close is the meaning to the reference?"
        ],
        "key_questions_ja": [
            "å›ç­”ã¯æ­£ã—ã„æ„å‘³ã‚’ä¼ãˆã¦ã„ã‚‹ã‹ï¼Ÿ",
            "æ¦‚å¿µã¯æ­£ç¢ºã«è¡¨ç¾ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ",
            "æ„å‘³çš„é–¢ä¿‚ã¯ä¿æŒã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ",
            "å‚ç…§ã¨ã®æ„å‘³çš„è·é›¢ã¯ã©ã®ç¨‹åº¦ã‹ï¼Ÿ"
        ]
    },
    
    "factual_correctness": {
        "description": "Are the facts and information presented accurately?",
        "description_ja": "æç¤ºã•ã‚ŒãŸäº‹å®Ÿã¨æƒ…å ±ã¯æ­£ç¢ºã‹ï¼Ÿ",
        "metrics": ["fact_verification", "claim_accuracy", "entity_consistency"],
        "focus": "Truth and factual accuracy of information",
        "focus_ja": "æƒ…å ±ã®çœŸå®Ÿæ€§ã¨äº‹å®Ÿçš„æ­£ç¢ºæ€§",
        "key_questions": [
            "Are all stated facts correct?",
            "Are there any factual errors or hallucinations?",
            "Do the entities and relationships match reality?",
            "Is the information up-to-date and accurate?"
        ],
        "key_questions_ja": [
            "è¨˜è¼‰ã•ã‚ŒãŸäº‹å®Ÿã¯ã™ã¹ã¦æ­£ã—ã„ã‹ï¼Ÿ",
            "äº‹å®Ÿçš„ã‚¨ãƒ©ãƒ¼ã‚„å¹»è¦šã¯ãªã„ã‹ï¼Ÿ",
            "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨é–¢ä¿‚ã¯ç¾å®Ÿã¨ä¸€è‡´ã™ã‚‹ã‹ï¼Ÿ",
            "æƒ…å ±ã¯æœ€æ–°ã§æ­£ç¢ºã‹ï¼Ÿ"
        ]
    },
    
    "retrieval_effectiveness": {
        "description": "How well does the system find relevant information?",
        "description_ja": "ã‚·ã‚¹ãƒ†ãƒ ã¯ã©ã®ç¨‹åº¦é–¢é€£æƒ…å ±ã‚’è¦‹ã¤ã‘ã‚‰ã‚Œã‚‹ã‹ï¼Ÿ",
        "metrics": ["hit_rate", "mrr", "ndcg", "precision_at_k", "recall_at_k"],
        "focus": "Quality and relevance of retrieved documents",
        "focus_ja": "æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸ã®å“è³ªã¨é–¢é€£æ€§",
        "key_questions": [
            "Are the most relevant documents retrieved?",
            "How good is the ranking of retrieved documents?",
            "Is important information being missed?",
            "How precise are the search results?"
        ],
        "key_questions_ja": [
            "æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æ–‡æ›¸ãŒæ¤œç´¢ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ",
            "æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®å“è³ªã¯ï¼Ÿ",
            "é‡è¦ãªæƒ…å ±ãŒè¦‹é€ƒã•ã‚Œã¦ã„ãªã„ã‹ï¼Ÿ",
            "æ¤œç´¢çµæœã®ç²¾åº¦ã¯ã©ã®ç¨‹åº¦ã‹ï¼Ÿ"
        ]
    },
    
    "answer_completeness": {
        "description": "Does the answer provide complete information?",
        "description_ja": "å›ç­”ã¯å®Œå…¨ãªæƒ…å ±ã‚’æä¾›ã—ã¦ã„ã‚‹ã‹ï¼Ÿ",
        "metrics": ["coverage_score", "information_completeness", "aspect_coverage"],
        "focus": "Comprehensiveness of the provided answer",
        "focus_ja": "æä¾›ã•ã‚ŒãŸå›ç­”ã®åŒ…æ‹¬æ€§",
        "key_questions": [
            "Are all aspects of the question addressed?",
            "Is any critical information missing?",
            "Does the answer cover the main points?",
            "Is the level of detail appropriate?"
        ],
        "key_questions_ja": [
            "è³ªå•ã®ã™ã¹ã¦ã®å´é¢ãŒæ‰±ã‚ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ",
            "é‡è¦ãªæƒ…å ±ãŒæ¬ ã‘ã¦ã„ãªã„ã‹ï¼Ÿ",
            "å›ç­”ã¯ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ã‹ï¼Ÿ",
            "è©³ç´°ãƒ¬ãƒ™ãƒ«ã¯é©åˆ‡ã‹ï¼Ÿ"
        ]
    },
    
    "context_faithfulness": {
        "description": "How faithful is the answer to the source context?",
        "description_ja": "å›ç­”ã¯å…ƒã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ã©ã®ç¨‹åº¦å¿ å®Ÿã‹ï¼Ÿ",
        "metrics": ["faithfulness", "context_adherence", "hallucination_detection"],
        "focus": "Alignment between answer and source information",
        "focus_ja": "å›ç­”ã¨å…ƒæƒ…å ±ã®ä¸€è‡´åº¦",
        "key_questions": [
            "Does the answer stay true to the source material?",
            "Are there any contradictions with the context?",
            "Is information being invented or hallucinated?",
            "How well does the answer reflect the source content?"
        ],
        "key_questions_ja": [
            "å›ç­”ã¯å…ƒã®è³‡æ–™ã«å¿ å®Ÿã‹ï¼Ÿ",
            "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã®çŸ›ç›¾ã¯ãªã„ã‹ï¼Ÿ",
            "æƒ…å ±ãŒå‰µä½œã•ã‚ŒãŸã‚Šå¹»è¦šã•ã‚Œã¦ã„ãªã„ã‹ï¼Ÿ",
            "å›ç­”ã¯å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã©ã®ç¨‹åº¦åæ˜ ã—ã¦ã„ã‚‹ã‹ï¼Ÿ"
        ]
    },
    
    "relevance_appropriateness": {
        "description": "How relevant and appropriate is the answer to the question?",
        "description_ja": "å›ç­”ã¯è³ªå•ã«å¯¾ã—ã¦ã©ã®ç¨‹åº¦é–¢é€£æ€§ãŒã‚ã‚Šé©åˆ‡ã‹ï¼Ÿ",
        "metrics": ["answer_relevance", "topic_relevance", "appropriateness_score"],
        "focus": "Direct relevance and appropriateness to the query",
        "focus_ja": "ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹ç›´æ¥çš„é–¢é€£æ€§ã¨é©åˆ‡æ€§",
        "key_questions": [
            "Does the answer directly address the question?",
            "Is the response appropriate for the question type?",
            "How well does the answer match the user's intent?",
            "Is the answer on-topic and focused?"
        ],
        "key_questions_ja": [
            "å›ç­”ã¯è³ªå•ã«ç›´æ¥ç­”ãˆã¦ã„ã‚‹ã‹ï¼Ÿ",
            "è³ªå•ã‚¿ã‚¤ãƒ—ã«å¯¾ã—ã¦å›ç­”ã¯é©åˆ‡ã‹ï¼Ÿ",
            "å›ç­”ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã¨ã©ã®ç¨‹åº¦ä¸€è‡´ã™ã‚‹ã‹ï¼Ÿ",
            "å›ç­”ã¯ãƒˆãƒ”ãƒƒã‚¯ã«é›†ä¸­ã—ç„¦ç‚¹ãŒåˆã£ã¦ã„ã‚‹ã‹ï¼Ÿ"
        ]
    },
    
    "usability_practicality": {
        "description": "How useful and practical is the answer for the user?",
        "description_ja": "å›ç­”ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦ã©ã®ç¨‹åº¦æœ‰ç”¨ã§å®Ÿç”¨çš„ã‹ï¼Ÿ",
        "metrics": ["helpfulness", "actionability", "clarity", "understandability"],
        "focus": "Practical utility and user experience",
        "focus_ja": "å®Ÿç”¨çš„æœ‰ç”¨æ€§ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“",
        "key_questions": [
            "Can the user act on this information?",
            "Is the answer clear and understandable?",
            "Does it provide practical value?",
            "How helpful is this for solving the user's problem?"
        ],
        "key_questions_ja": [
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã“ã®æƒ…å ±ã«åŸºã¥ã„ã¦è¡Œå‹•ã§ãã‚‹ã‹ï¼Ÿ",
            "å›ç­”ã¯æ˜ç¢ºã§ç†è§£ã—ã‚„ã™ã„ã‹ï¼Ÿ",
            "å®Ÿç”¨çš„ä¾¡å€¤ã‚’æä¾›ã—ã¦ã„ã‚‹ã‹ï¼Ÿ",
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å•é¡Œè§£æ±ºã«ã©ã®ç¨‹åº¦å½¹ç«‹ã¤ã‹ï¼Ÿ"
        ]
    },
    
    "consistency_reliability": {
        "description": "How consistent and reliable are the system outputs?",
        "description_ja": "ã‚·ã‚¹ãƒ†ãƒ ã®å‡ºåŠ›ã¯ã©ã®ç¨‹åº¦ä¸€è²«æ€§ãŒã‚ã‚Šä¿¡é ¼ã§ãã‚‹ã‹ï¼Ÿ",
        "metrics": ["consistency_score", "variance_analysis", "reliability_measure"],
        "focus": "Consistency across similar queries and reliability",
        "focus_ja": "é¡ä¼¼ã‚¯ã‚¨ãƒªé–“ã®ä¸€è²«æ€§ã¨ä¿¡é ¼æ€§",
        "key_questions": [
            "Does the system give consistent answers to similar questions?",
            "Are outputs stable across multiple runs?",
            "How reliable is the system's performance?",
            "Are there significant variations in quality?"
        ],
        "key_questions_ja": [
            "é¡ä¼¼è³ªå•ã«å¯¾ã—ã¦ä¸€è²«ã—ãŸå›ç­”ã‚’ã™ã‚‹ã‹ï¼Ÿ",
            "è¤‡æ•°å›ã®å®Ÿè¡Œã§å‡ºåŠ›ã¯å®‰å®šã—ã¦ã„ã‚‹ã‹ï¼Ÿ",
            "ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ã©ã®ç¨‹åº¦ä¿¡é ¼ã§ãã‚‹ã‹ï¼Ÿ",
            "å“è³ªã«å¤§ããªã°ã‚‰ã¤ãã¯ãªã„ã‹ï¼Ÿ"
        ]
    }
}

# Display evaluation perspectives / è©•ä¾¡è¦³ç‚¹ã‚’è¡¨ç¤º
print("ğŸ¯ RAG Evaluation Perspectives / RAGè©•ä¾¡ã®è¦³ç‚¹\n")
for perspective, details in evaluation_perspectives.items():
    print(f"ğŸ“Š {perspective.upper().replace('_', ' ')}")
    print(f"   Focus: {details['focus']}")
    print(f"   ç„¦ç‚¹: {details['focus_ja']}")
    print(f"   Key Metrics: {', '.join(details['metrics'])}")
    print(f"   Example Questions:")
    for i, question in enumerate(details['key_questions'][:2]):
        print(f"   - {question}")
        print(f"   - {details['key_questions_ja'][i]}")
    print()
```

#### 4.1.2 Metric Selection by Evaluation Goal / è©•ä¾¡ç›®æ¨™åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹é¸æŠ

```python
# Choose metrics based on your evaluation goals / è©•ä¾¡ç›®æ¨™ã«åŸºã¥ã„ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é¸æŠ

evaluation_goals = {
    "development_debugging": {
        "description": "Identify specific issues during development",
        "description_ja": "é–‹ç™ºä¸­ã®ç‰¹å®šã®å•é¡Œã‚’ç‰¹å®š",
        "recommended_metrics": [
            "hit_rate",  # Are we finding relevant docs?
            "faithfulness",  # Are we staying true to context?
            "answer_relevance",  # Are we answering the right question?
            "bertscore"  # Is the semantic meaning preserved?
        ],
        "frequency": "Every development iteration",
        "frequency_ja": "é–‹ç™ºã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨"
    },
    
    "performance_benchmarking": {
        "description": "Compare system performance against baselines",
        "description_ja": "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«å¯¾ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ã®æ¯”è¼ƒ",
        "recommended_metrics": [
            "bleu",  # Standard text generation quality
            "rouge",  # Content coverage comparison
            "ndcg",  # Retrieval ranking quality
            "mrr"  # Search effectiveness
        ],
        "frequency": "Weekly or monthly",
        "frequency_ja": "é€±æ¬¡ã¾ãŸã¯æœˆæ¬¡"
    },
    
    "production_monitoring": {
        "description": "Monitor system quality in production",
        "description_ja": "æœ¬ç•ªç’°å¢ƒã§ã®ã‚·ã‚¹ãƒ†ãƒ å“è³ªç›£è¦–",
        "recommended_metrics": [
            "consistency_score",  # System reliability
            "response_time",  # Performance efficiency
            "error_rate",  # System stability
            "user_satisfaction"  # End-user experience
        ],
        "frequency": "Continuous monitoring",
        "frequency_ja": "ç¶™ç¶šçš„ç›£è¦–"
    },
    
    "research_evaluation": {
        "description": "Comprehensive academic or research assessment",
        "description_ja": "åŒ…æ‹¬çš„å­¦è¡“ç ”ç©¶è©•ä¾¡",
        "recommended_metrics": [
            "all_linguistic_metrics",  # Complete linguistic analysis
            "all_semantic_metrics",  # Thorough semantic evaluation
            "all_retrieval_metrics",  # Full retrieval assessment
            "human_evaluation"  # Gold standard comparison
        ],
        "frequency": "For major releases or research papers",
        "frequency_ja": "ãƒ¡ã‚¸ãƒ£ãƒ¼ãƒªãƒªãƒ¼ã‚¹ã¾ãŸã¯ç ”ç©¶è«–æ–‡æ™‚"
    }
}

def recommend_metrics_for_goal(evaluation_goal: str) -> dict:
    """
    Recommend specific metrics configuration for evaluation goals
    è©•ä¾¡ç›®æ¨™ã«ç‰¹åŒ–ã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨­å®šã‚’æ¨å¥¨
    """
    if evaluation_goal not in evaluation_goals:
        return {"error": "Unknown evaluation goal"}
    
    goal_info = evaluation_goals[evaluation_goal]
    
    return {
        "goal": evaluation_goal,
        "description": goal_info["description"],
        "metrics": goal_info["recommended_metrics"],
        "evaluation_frequency": goal_info["frequency"],
        "é…ç½®_rationale": f"These metrics focus on {goal_info['description'].lower()}"
    }

# Example usage / ä½¿ç”¨ä¾‹
print("ğŸ¯ Metric Recommendations by Goal / ç›®æ¨™åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¨å¥¨")
for goal in evaluation_goals.keys():
    recommendation = recommend_metrics_for_goal(goal)
    print(f"\nğŸ“‹ {goal.replace('_', ' ').title()}:")
    print(f"   Description: {recommendation['description']}")
    print(f"   Recommended Metrics: {', '.join(recommendation['metrics'])}")
    print(f"   Frequency: {recommendation['evaluation_frequency']}")
```

```python
# Comprehensive guide to evaluation metrics / è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åŒ…æ‹¬çš„ã‚¬ã‚¤ãƒ‰

# 1. BLEU (Bilingual Evaluation Understudy) Score
# - Measures n-gram overlap between reference and candidate text
# - Range: 0-1 (higher is better)
# - Good for: Exact match evaluation, translation quality
# - Limitations: Focuses on precision, sensitive to exact wording

from refinire_rag.evaluation.bleu_evaluator import BleuEvaluator

bleu_config = {
    "max_ngram": 4,              # Consider up to 4-grams
    "smooth": True,              # Apply smoothing for short texts
    "weights": [0.25, 0.25, 0.25, 0.25],  # Equal weight for all n-grams
    "case_sensitive": False      # Ignore case differences
}

bleu_evaluator = BleuEvaluator(bleu_config)

# Example BLEU evaluation / BLEUè©•ä¾¡ã®ä¾‹
reference_answer = "RAG combines retrieval and generation to improve LLM responses with external knowledge."
candidate_answer = "RAG merges information retrieval with text generation to enhance LLM answers using external data."

bleu_score = bleu_evaluator.evaluate(
    reference=reference_answer,
    candidate=candidate_answer
)

print(f"BLEU Score: {bleu_score['bleu_score']:.3f}")
print(f"N-gram precisions: {bleu_score['precisions']}")
print(f"Brevity penalty: {bleu_score['brevity_penalty']:.3f}")

# 2. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
# - Measures recall of important words/phrases
# - Multiple variants: ROUGE-1 (unigrams), ROUGE-2 (bigrams), ROUGE-L (longest common subsequence)
# - Good for: Content coverage, summarization quality
# - Focuses more on recall than BLEU

from refinire_rag.evaluation.rouge_evaluator import RougeEvaluator

rouge_config = {
    "rouge_types": ["rouge-1", "rouge-2", "rouge-l"],
    "use_stemmer": True,         # Apply stemming for better matching
    "alpha": 0.5,               # Balance precision and recall (F1 score)
    "split_summaries": True,     # Split long texts into sentences
    "remove_stopwords": False    # Keep stopwords for context
}

rouge_evaluator = RougeEvaluator(rouge_config)

rouge_scores = rouge_evaluator.evaluate(
    reference=reference_answer,
    candidate=candidate_answer
)

print("\nROUGE Scores:")
for rouge_type, scores in rouge_scores.items():
    print(f"  {rouge_type.upper()}:")
    print(f"    Precision: {scores['precision']:.3f}")
    print(f"    Recall: {scores['recall']:.3f}")
    print(f"    F1-Score: {scores['f1']:.3f}")

# 3. BERTScore - Semantic similarity using contextual embeddings
# - Uses pre-trained BERT models to compute semantic similarity
# - More robust to paraphrasing than n-gram based metrics
# - Good for: Semantic accuracy, meaning preservation

from refinire_rag.evaluation.bertscore_evaluator import BertScoreEvaluator

bertscore_config = {
    "model_type": "microsoft/deberta-xlarge-mnli",  # High-quality model
    "num_layers": 40,           # Number of layers to use
    "verbose": False,           # Reduce output verbosity
    "idf": True,               # Use inverse document frequency weighting
    "device": "auto",          # Automatically detect GPU/CPU
    "lang": "en"               # Language for tokenization
}

bertscore_evaluator = BertScoreEvaluator(bertscore_config)

bertscore_result = bertscore_evaluator.evaluate(
    reference=reference_answer,
    candidate=candidate_answer
)

print(f"\nBERTScore:")
print(f"  Precision: {bertscore_result['precision']:.3f}")
print(f"  Recall: {bertscore_result['recall']:.3f}")
print(f"  F1-Score: {bertscore_result['f1']:.3f}")

# 4. Semantic Similarity (Cosine Similarity of Embeddings)
# - Direct cosine similarity between sentence embeddings
# - Fast and simple semantic comparison
# - Good for: Quick semantic similarity assessment

from refinire_rag.evaluation.semantic_evaluator import SemanticSimilarityEvaluator

semantic_config = {
    "embedding_model": "all-mpnet-base-v2",  # High-quality sentence transformer
    "similarity_metric": "cosine",          # cosine, dot_product, euclidean
    "normalize_embeddings": True            # L2 normalize embeddings
}

semantic_evaluator = SemanticSimilarityEvaluator(semantic_config)

semantic_score = semantic_evaluator.evaluate(
    reference=reference_answer,
    candidate=candidate_answer
)

print(f"\nSemantic Similarity: {semantic_score['similarity']:.3f}")
```

### 4.2 Comprehensive Metric Configuration / åŒ…æ‹¬çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨­å®š

```python
# Configure evaluation metrics / è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨­å®š
evaluation_config = {
    "bleu": {
        "max_ngram": 4,
        "smooth": True,
        "weights": [0.25, 0.25, 0.25, 0.25],
        "case_sensitive": False
    },
    "rouge": {
        "rouge_types": ["rouge-1", "rouge-2", "rouge-l"],
        "use_stemmer": True,
        "alpha": 0.5,  # Balance precision and recall
        "remove_stopwords": False
    },
    "bertscore": {
        "model_type": "microsoft/deberta-xlarge-mnli",
        "num_layers": 40,
        "idf": True,
        "lang": "en"
    },
    "semantic_similarity": {
        "embedding_model": "all-mpnet-base-v2",
        "similarity_metric": "cosine",
        "normalize_embeddings": True
    },
    "questeval": {
        "model": "gpt-4o-mini",
        "check_answerability": True,
        "check_consistency": True,
        "language": "english"  # For Japanese evaluation use "japanese"
    }
}

# Practical example using multiple metrics / è¤‡æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ãŸå®Ÿè·µä¾‹
def evaluate_answer_quality(reference_answer: str, generated_answer: str, evaluation_config: dict):
    """
    Comprehensive answer quality evaluation using multiple metrics
    è¤‡æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ãŸåŒ…æ‹¬çš„å›ç­”å“è³ªè©•ä¾¡
    """
    results = {}
    
    # BLEU evaluation / BLEUè©•ä¾¡
    if "bleu" in evaluation_config:
        bleu_evaluator = BleuEvaluator(evaluation_config["bleu"])
        bleu_result = bleu_evaluator.evaluate(reference_answer, generated_answer)
        results["bleu"] = bleu_result["bleu_score"]
    
    # ROUGE evaluation / ROUGEè©•ä¾¡
    if "rouge" in evaluation_config:
        rouge_evaluator = RougeEvaluator(evaluation_config["rouge"])
        rouge_result = rouge_evaluator.evaluate(reference_answer, generated_answer)
        results["rouge"] = {
            "rouge-1": rouge_result["rouge-1"]["f1"],
            "rouge-2": rouge_result["rouge-2"]["f1"], 
            "rouge-l": rouge_result["rouge-l"]["f1"]
        }
    
    # BERTScore evaluation / BERTScoreè©•ä¾¡
    if "bertscore" in evaluation_config:
        bertscore_evaluator = BertScoreEvaluator(evaluation_config["bertscore"])
        bertscore_result = bertscore_evaluator.evaluate(reference_answer, generated_answer)
        results["bertscore"] = bertscore_result["f1"]
    
    # Semantic similarity / æ„å‘³çš„é¡ä¼¼åº¦
    if "semantic_similarity" in evaluation_config:
        semantic_evaluator = SemanticSimilarityEvaluator(evaluation_config["semantic_similarity"])
        semantic_result = semantic_evaluator.evaluate(reference_answer, generated_answer)
        results["semantic_similarity"] = semantic_result["similarity"]
    
    return results

# Example usage / ä½¿ç”¨ä¾‹
reference = "RAG systems combine retrieval and generation to provide accurate, contextual responses."
generated = "RAG approaches merge information retrieval with text generation for contextual answers."

quality_scores = evaluate_answer_quality(reference, generated, evaluation_config)

print("Comprehensive Quality Assessment:")
print(f"  BLEU Score: {quality_scores.get('bleu', 0):.3f}")
print(f"  ROUGE-1 F1: {quality_scores.get('rouge', {}).get('rouge-1', 0):.3f}")
print(f"  ROUGE-2 F1: {quality_scores.get('rouge', {}).get('rouge-2', 0):.3f}")
print(f"  ROUGE-L F1: {quality_scores.get('rouge', {}).get('rouge-l', 0):.3f}")
print(f"  BERTScore F1: {quality_scores.get('bertscore', 0):.3f}")
print(f"  Semantic Similarity: {quality_scores.get('semantic_similarity', 0):.3f}")
```

### 4.3 RAG-Specific Evaluation Metrics / RAGç‰¹åŒ–è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
# RAG systems require specialized metrics beyond traditional NLG metrics
# RAGã‚·ã‚¹ãƒ†ãƒ ã«ã¯å¾“æ¥ã®NLGãƒ¡ãƒˆãƒªã‚¯ã‚¹ä»¥ä¸Šã®ç‰¹åŒ–ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå¿…è¦

# 1. Retrieval Metrics / æ¤œç´¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹
from refinire_rag.evaluation.retrieval_evaluator import RetrievalEvaluator

retrieval_config = {
    "metrics": ["hit_rate", "mrr", "ndcg"],
    "k_values": [1, 3, 5, 10],  # Top-k values to evaluate
    "relevance_threshold": 0.5   # Minimum relevance score
}

retrieval_evaluator = RetrievalEvaluator(retrieval_config)

# Example retrieval evaluation / æ¤œç´¢è©•ä¾¡ã®ä¾‹
def evaluate_retrieval_performance(qa_pairs, query_engine):
    """
    Evaluate retrieval component performance
    æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
    """
    retrieval_results = []
    
    for qa_pair in qa_pairs:
        # Get retrieval results / æ¤œç´¢çµæœã‚’å–å¾—
        search_results = query_engine.retriever.search(
            query=qa_pair.question,
            top_k=10
        )
        
        # Expected relevant documents / æœŸå¾…ã•ã‚Œã‚‹é–¢é€£æ–‡æ›¸
        expected_docs = qa_pair.metadata.get("expected_sources", [])
        
        # Evaluate retrieval / æ¤œç´¢ã‚’è©•ä¾¡
        retrieval_metrics = retrieval_evaluator.evaluate(
            search_results=search_results,
            relevant_docs=expected_docs
        )
        
        retrieval_results.append({
            "qa_id": qa_pair.metadata.get("qa_id"),
            "question": qa_pair.question,
            "hit_rate@5": retrieval_metrics["hit_rate@5"],
            "mrr": retrieval_metrics["mrr"],
            "ndcg@5": retrieval_metrics["ndcg@5"],
            "precision@5": retrieval_metrics["precision@5"],
            "recall@5": retrieval_metrics["recall@5"]
        })
    
    return retrieval_results

# 2. Answer Faithfulness / å›ç­”å¿ å®Ÿåº¦
from refinire_rag.evaluation.faithfulness_evaluator import FaithfulnessEvaluator

faithfulness_config = {
    "model": "gpt-4",
    "evaluation_prompt": """
    Evaluate if the answer is faithful to the provided context.
    Rate faithfulness on a scale of 1-5:
    1 = Answer contradicts the context
    2 = Answer has major inconsistencies with context
    3 = Answer is partially consistent with context
    4 = Answer is mostly consistent with context
    5 = Answer is completely faithful to context
    """,
    "check_hallucination": True,
    "check_consistency": True
}

faithfulness_evaluator = FaithfulnessEvaluator(faithfulness_config)

# 3. Context Precision & Recall / ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé©åˆç‡ãƒ»å†ç¾ç‡
from refinire_rag.evaluation.context_evaluator import ContextEvaluator

context_config = {
    "model": "gpt-4o-mini",
    "precision_prompt": """
    Evaluate the relevance of each retrieved context to answering the question.
    Return relevance scores (0-1) for each context passage.
    """,
    "recall_prompt": """
    Evaluate if the retrieved contexts contain all necessary information
    to completely answer the question.
    """
}

context_evaluator = ContextEvaluator(context_config)

# 4. Answer Relevance / å›ç­”é–¢é€£æ€§
from refinire_rag.evaluation.relevance_evaluator import AnswerRelevanceEvaluator

relevance_config = {
    "model": "gpt-4o-mini",
    "evaluation_criteria": [
        "directness",      # Direct answer to question
        "completeness",    # Complete information
        "accuracy",        # Factual accuracy
        "clarity"          # Clear presentation
    ]
}

relevance_evaluator = AnswerRelevanceEvaluator(relevance_config)

# Comprehensive RAG evaluation / åŒ…æ‹¬çš„RAGè©•ä¾¡
def evaluate_rag_system_comprehensively(qa_pairs, query_engine):
    """
    Comprehensive RAG system evaluation with all metrics
    å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ã‚ˆã‚‹åŒ…æ‹¬çš„RAGã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡
    """
    results = {
        "retrieval_metrics": [],
        "generation_metrics": [],
        "rag_specific_metrics": []
    }
    
    for qa_pair in qa_pairs:
        # Get full RAG response / å®Œå…¨ãªRAGå¿œç­”ã‚’å–å¾—
        rag_response = query_engine.query(qa_pair.question)
        
        # 1. Evaluate retrieval / æ¤œç´¢ã‚’è©•ä¾¡
        retrieval_metrics = retrieval_evaluator.evaluate(
            search_results=rag_response.search_results,
            relevant_docs=qa_pair.metadata.get("expected_sources", [])
        )
        
        # 2. Evaluate generation quality / ç”Ÿæˆå“è³ªã‚’è©•ä¾¡
        generation_metrics = evaluate_answer_quality(
            reference_answer=qa_pair.answer,
            generated_answer=rag_response.answer,
            evaluation_config=evaluation_config
        )
        
        # 3. Evaluate RAG-specific aspects / RAGç‰¹åŒ–å´é¢ã‚’è©•ä¾¡
        faithfulness_score = faithfulness_evaluator.evaluate(
            answer=rag_response.answer,
            contexts=rag_response.contexts
        )
        
        context_precision = context_evaluator.evaluate_precision(
            question=qa_pair.question,
            contexts=rag_response.contexts
        )
        
        context_recall = context_evaluator.evaluate_recall(
            question=qa_pair.question,
            contexts=rag_response.contexts,
            expected_answer=qa_pair.answer
        )
        
        answer_relevance = relevance_evaluator.evaluate(
            question=qa_pair.question,
            answer=rag_response.answer
        )
        
        # Store results / çµæœã‚’ä¿å­˜
        results["retrieval_metrics"].append(retrieval_metrics)
        results["generation_metrics"].append(generation_metrics)
        results["rag_specific_metrics"].append({
            "faithfulness": faithfulness_score,
            "context_precision": context_precision,
            "context_recall": context_recall,
            "answer_relevance": answer_relevance
        })
    
    return results

# Calculate aggregate scores / é›†è¨ˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
def calculate_aggregate_scores(evaluation_results):
    """
    Calculate aggregate scores across all test cases
    å…¨ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã«ã‚ãŸã‚‹é›†è¨ˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    """
    aggregates = {}
    
    # Retrieval aggregates / æ¤œç´¢é›†è¨ˆ
    retrieval_metrics = evaluation_results["retrieval_metrics"]
    aggregates["retrieval"] = {
        "avg_hit_rate@5": sum(r["hit_rate@5"] for r in retrieval_metrics) / len(retrieval_metrics),
        "avg_mrr": sum(r["mrr"] for r in retrieval_metrics) / len(retrieval_metrics),
        "avg_ndcg@5": sum(r["ndcg@5"] for r in retrieval_metrics) / len(retrieval_metrics)
    }
    
    # Generation aggregates / ç”Ÿæˆé›†è¨ˆ
    generation_metrics = evaluation_results["generation_metrics"]
    aggregates["generation"] = {
        "avg_bleu": sum(g.get("bleu", 0) for g in generation_metrics) / len(generation_metrics),
        "avg_rouge_1": sum(g.get("rouge", {}).get("rouge-1", 0) for g in generation_metrics) / len(generation_metrics),
        "avg_bertscore": sum(g.get("bertscore", 0) for g in generation_metrics) / len(generation_metrics)
    }
    
    # RAG-specific aggregates / RAGç‰¹åŒ–é›†è¨ˆ
    rag_metrics = evaluation_results["rag_specific_metrics"]
    aggregates["rag_specific"] = {
        "avg_faithfulness": sum(r["faithfulness"] for r in rag_metrics) / len(rag_metrics),
        "avg_context_precision": sum(r["context_precision"] for r in rag_metrics) / len(rag_metrics),
        "avg_context_recall": sum(r["context_recall"] for r in rag_metrics) / len(rag_metrics),
        "avg_answer_relevance": sum(r["answer_relevance"] for r in rag_metrics) / len(rag_metrics)
    }
    
    return aggregates

print("RAG System Comprehensive Evaluation:")
comprehensive_results = evaluate_rag_system_comprehensively(qa_pairs, query_engine)
aggregate_scores = calculate_aggregate_scores(comprehensive_results)

print("\nRetrieval Performance:")
for metric, score in aggregate_scores["retrieval"].items():
    print(f"  {metric}: {score:.3f}")

print("\nGeneration Quality:")
for metric, score in aggregate_scores["generation"].items():
    print(f"  {metric}: {score:.3f}")

print("\nRAG-Specific Metrics:")
for metric, score in aggregate_scores["rag_specific"].items():
    print(f"  {metric}: {score:.3f}")
```

### 4.4 QualityLab Integration with Custom Metrics / QualityLabã¨ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®çµ±åˆ

```python
# Integrate all evaluation approaches with QualityLab
# å…¨è©•ä¾¡ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’QualityLabã¨çµ±åˆ

# Step 1: Register expert QA pairs / ã‚¹ãƒ†ãƒƒãƒ—1: å°‚é–€å®¶QAãƒšã‚¢ã‚’ç™»éŒ²
quality_lab.register_qa_pairs(
    qa_pairs=expert_qa_pairs,
    qa_set_name="gold_standard_benchmark",
    metadata={
        "evaluation_type": "comprehensive_rag_metrics",
        "metrics_included": ["bleu", "rouge", "bertscore", "retrieval", "faithfulness"]
    }
)

# Step 2: Configure comprehensive evaluation / ã‚¹ãƒ†ãƒƒãƒ—2: åŒ…æ‹¬çš„è©•ä¾¡ã‚’è¨­å®š
rag_evaluation_config = QualityLabConfig(
    qa_pairs_per_document=0,  # Don't generate, use registered pairs
    evaluation_metrics=[
        "bleu", "rouge", "bertscore", "semantic_similarity",
        "hit_rate", "mrr", "ndcg", "faithfulness", 
        "context_precision", "context_recall", "answer_relevance"
    ],
    similarity_threshold=0.7,
    include_detailed_analysis=True,
    include_contradiction_detection=True,
    llm_model="gpt-4o-mini",
    evaluation_model="gpt-4"
)

# Step 3: Run comprehensive evaluation / ã‚¹ãƒ†ãƒƒãƒ—3: åŒ…æ‹¬çš„è©•ä¾¡ã‚’å®Ÿè¡Œ
comprehensive_evaluation = quality_lab.evaluate_query_engine(
    query_engine=query_engine,
    qa_pairs=expert_qa_pairs,  # Use registered expert QA pairs
    evaluation_config=rag_evaluation_config,
    include_detailed_breakdown=True
)

# Step 4: Generate comprehensive report / ã‚¹ãƒ†ãƒƒãƒ—4: åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
evaluation_report = quality_lab.generate_evaluation_report(
    evaluation_results=comprehensive_evaluation,
    output_file="comprehensive_rag_evaluation_with_metrics.md",
    include_metric_explanations=True,
    include_improvement_recommendations=True
)

print("Comprehensive RAG Evaluation with All Metrics:")
print(f"âœ… Evaluated {len(expert_qa_pairs)} expert QA pairs")
print(f"ğŸ“Š Generated comprehensive report: comprehensive_rag_evaluation_with_metrics.md")
print(f"ğŸ“ˆ Included {len(rag_evaluation_config.evaluation_metrics)} different metrics")

# Display summary results / è¦ç´„çµæœã‚’è¡¨ç¤º
if 'metric_summaries' in comprehensive_evaluation:
    print("\nğŸ“‹ Evaluation Summary:")
    for metric_category, results in comprehensive_evaluation['metric_summaries'].items():
        print(f"\n{metric_category.upper()} Metrics:")
        for metric, score in results.items():
            print(f"  {metric}: {score:.3f}")

# Show recommendations if available / æ¨å¥¨äº‹é …ãŒã‚ã‚Œã°è¡¨ç¤º
if 'recommendations' in comprehensive_evaluation:
    print("\nğŸ’¡ Improvement Recommendations:")
    for i, recommendation in enumerate(comprehensive_evaluation['recommendations'][:3]):
        print(f"  {i+1}. {recommendation}")
}

# Run comprehensive evaluation / åŒ…æ‹¬çš„è©•ä¾¡ã‚’å®Ÿè¡Œ
comprehensive_results = quality_lab.run_comprehensive_evaluation(
    corpus_documents=documents,
    query_engine=query_engine,
    num_qa_pairs=20,
    evaluation_config=evaluation_config,
    include_human_baseline=False,  # Set True if human answers available
    cross_validation_folds=3       # For robust evaluation
)

print("Comprehensive Evaluation Results:")
print(f"Total QA pairs: {comprehensive_results['total_qa_pairs']}")
print(f"Evaluation time: {comprehensive_results['total_evaluation_time']:.2f}s")

# Display metric results / ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµæœã‚’è¡¨ç¤º
for metric, results in comprehensive_results['metric_results'].items():
    print(f"\n{metric.upper()} Results:")
    print(f"  Average: {results['average']:.3f}")
    print(f"  Standard deviation: {results['std_dev']:.3f}")
    print(f"  Min: {results['min']:.3f}")
    print(f"  Max: {results['max']:.3f}")
    
    # Percentile analysis / ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«åˆ†æ
    if 'percentiles' in results:
        print(f"  25th percentile: {results['percentiles']['25']:.3f}")
        print(f"  50th percentile (median): {results['percentiles']['50']:.3f}")
        print(f"  75th percentile: {results['percentiles']['75']:.3f}")
```

### 4.2 LLM-based Evaluation / LLMãƒ™ãƒ¼ã‚¹è©•ä¾¡

```python
# Configure LLM Judge for detailed evaluation / è©³ç´°è©•ä¾¡ç”¨LLMåˆ¤å®šã‚’è¨­å®š
llm_judge_config = {
    "model": "gpt-4",
    "evaluation_criteria": {
        "accuracy": {
            "description": "How factually correct is the answer?",
            "scale": "1-5 (1=incorrect, 5=completely accurate)",
            "weight": 0.3
        },
        "completeness": {
            "description": "How complete is the answer?",
            "scale": "1-5 (1=missing key info, 5=comprehensive)",
            "weight": 0.25
        },
        "relevance": {
            "description": "How relevant is the answer to the question?",
            "scale": "1-5 (1=off-topic, 5=directly relevant)",
            "weight": 0.25
        },
        "clarity": {
            "description": "How clear and understandable is the answer?",
            "scale": "1-5 (1=confusing, 5=very clear)",
            "weight": 0.2
        }
    },
    "evaluation_instructions": """
    You are an expert evaluator of question-answering systems.
    Evaluate each answer based on the provided criteria.
    
    For each criterion:
    1. Provide a score from 1-5
    2. Give a brief explanation for your score
    3. Identify specific strengths and weaknesses
    
    Be objective and consistent in your evaluations.
    """
}

# Run LLM-based evaluation / LLMãƒ™ãƒ¼ã‚¹è©•ä¾¡ã‚’å®Ÿè¡Œ
llm_evaluation = quality_lab.evaluate_with_llm_judge(
    test_results=evaluation_results['test_results'],
    config=llm_judge_config
)

print("LLM Judge Evaluation Results:")
print(f"Evaluated answers: {len(llm_evaluation['evaluations'])}")

# Aggregate LLM scores / LLMã‚¹ã‚³ã‚¢ã‚’é›†è¨ˆ
criteria_scores = {}
for evaluation in llm_evaluation['evaluations']:
    for criterion, score_data in evaluation['scores'].items():
        if criterion not in criteria_scores:
            criteria_scores[criterion] = []
        criteria_scores[criterion].append(score_data['score'])

print("\nLLM Judge Scores by Criteria:")
for criterion, scores in criteria_scores.items():
    avg_score = sum(scores) / len(scores)
    print(f"  {criterion.capitalize()}: {avg_score:.2f}/5.0")

# Show sample evaluations / ã‚µãƒ³ãƒ—ãƒ«è©•ä¾¡ã‚’è¡¨ç¤º
print("\nSample LLM Evaluations:")
for i, evaluation in enumerate(llm_evaluation['evaluations'][:2]):
    print(f"\nEvaluation {i+1}:")
    print(f"Question: {evaluation['question'][:80]}...")
    print(f"Answer: {evaluation['answer'][:80]}...")
    print("Scores:")
    for criterion, score_data in evaluation['scores'].items():
        print(f"  {criterion}: {score_data['score']}/5 - {score_data['explanation']}")
```

## 5. Report Generation / ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

### 5.1 Comprehensive Reports / åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆ

```python
# Generate detailed evaluation report / è©³ç´°è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
report_config = {
    "include_executive_summary": True,
    "include_detailed_analysis": True,
    "include_recommendations": True,
    "include_visualizations": True,
    "include_raw_data": False,  # Set True for full data export
    "format": "markdown",
    "language": "bilingual"  # English and Japanese
}

# Generate comprehensive report / åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
report = quality_lab.generate_evaluation_report(
    evaluation_results=comprehensive_results,
    output_file="comprehensive_rag_evaluation.md",
    config=report_config
)

print(f"Generated comprehensive report: comprehensive_rag_evaluation.md")
print(f"Report length: {len(report)} characters")

# Generate executive summary / è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
executive_summary = quality_lab.generate_executive_summary(
    evaluation_results=comprehensive_results,
    target_audience="technical_management",  # Options: technical, management, technical_management
    key_findings_limit=5
)

print("\nExecutive Summary Preview:")
print("="*40)
print(executive_summary[:500] + "...")
```

### 5.2 Custom Report Templates / ã‚«ã‚¹ã‚¿ãƒ ãƒ¬ãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```python
# Custom report template / ã‚«ã‚¹ã‚¿ãƒ ãƒ¬ãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
custom_template = """
# RAG System Evaluation Report
# RAGã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

## Executive Summary / è¦ç´„
{executive_summary}

## Test Configuration / ãƒ†ã‚¹ãƒˆè¨­å®š
- Total QA Pairs: {total_qa_pairs}
- Evaluation Metrics: {metrics_used}
- Test Duration: {test_duration}
- Query Engine Version: {query_engine_version}

## Performance Results / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµæœ
{performance_table}

## Quality Analysis / å“è³ªåˆ†æ
{quality_analysis}

## Recommendations / æ¨å¥¨äº‹é …
{recommendations}

## Detailed Results / è©³ç´°çµæœ
{detailed_results}
"""

# Generate custom report / ã‚«ã‚¹ã‚¿ãƒ ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
custom_report = quality_lab.generate_custom_report(
    evaluation_results=comprehensive_results,
    template=custom_template,
    output_file="custom_evaluation_report.md"
)

print("Generated custom report with template")
```

## 6. Continuous Evaluation / ç¶™ç¶šçš„è©•ä¾¡

### 6.1 Automated Testing Pipeline / è‡ªå‹•ãƒ†ã‚¹ãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
# Setup continuous evaluation pipeline / ç¶™ç¶šçš„è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
pipeline_config = {
    "evaluation_frequency": "daily",      # daily, weekly, on_update
    "baseline_threshold": 0.75,           # Minimum acceptable performance
    "regression_threshold": 0.05,         # Alert if performance drops > 5%
    "auto_report_generation": True,
    "alert_recipients": ["team@company.com"],
    "comparison_window": 7                # Compare with last N evaluations
}

# Create evaluation pipeline / è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
evaluation_pipeline = quality_lab.create_evaluation_pipeline(
    config=pipeline_config,
    qa_pairs=qa_pairs,
    query_engine=query_engine
)

# Run pipeline evaluation / ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚’å®Ÿè¡Œ
pipeline_results = evaluation_pipeline.run_evaluation()

print("Pipeline Evaluation Results:")
print(f"Current performance: {pipeline_results['current_score']:.3f}")
print(f"Baseline performance: {pipeline_results['baseline_score']:.3f}")
print(f"Performance change: {pipeline_results['score_change']:+.3f}")

# Check for regressions / å›å¸°ã‚’ãƒã‚§ãƒƒã‚¯
if pipeline_results['regression_detected']:
    print("âš ï¸  Performance regression detected!")
    print(f"   Regression severity: {pipeline_results['regression_severity']}")
    print(f"   Affected areas: {pipeline_results['affected_areas']}")
else:
    print("âœ… No performance regression detected")
```

### 6.2 A/B Testing Framework / A/Bãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

```python
# Setup A/B test for QueryEngine improvements / QueryEngineæ”¹å–„ã®A/Bãƒ†ã‚¹ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
ab_test_config = {
    "test_name": "retriever_optimization_v1",
    "control_group": "current_retriever",
    "treatment_group": "optimized_retriever", 
    "sample_size": 100,                   # QA pairs per group
    "confidence_level": 0.95,
    "minimum_effect_size": 0.05           # Minimum improvement to detect
}

# Run A/B test / A/Bãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
ab_results = quality_lab.run_ab_test(
    control_engine=current_query_engine,
    treatment_engine=optimized_query_engine,
    test_config=ab_test_config,
    qa_pairs=qa_pairs
)

print("A/B Test Results:")
print(f"Control group performance: {ab_results['control_score']:.3f}")
print(f"Treatment group performance: {ab_results['treatment_score']:.3f}")
print(f"Improvement: {ab_results['improvement']:.3f}")
print(f"Statistical significance: {ab_results['p_value']:.4f}")
print(f"Significant improvement: {'Yes' if ab_results['significant'] else 'No'}")

# Detailed A/B analysis / è©³ç´°A/Båˆ†æ
if ab_results['significant']:
    print(f"\nâœ… Significant improvement detected!")
    print(f"   Effect size: {ab_results['effect_size']:.3f}")
    print(f"   Confidence interval: [{ab_results['ci_lower']:.3f}, {ab_results['ci_upper']:.3f}]")
    print(f"   Recommendation: Deploy treatment to production")
else:
    print(f"\nâŒ No significant improvement")
    print(f"   Required sample size for detection: {ab_results['required_sample_size']}")
```

## 7. Advanced Evaluation Techniques / é«˜åº¦ãªè©•ä¾¡æŠ€è¡“

### 7.1 Domain-Specific Evaluation / ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–è©•ä¾¡

```python
# Configure domain-specific evaluation / ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–è©•ä¾¡ã‚’è¨­å®š
domain_config = QualityLabConfig(
    domain="technical_ai",
    specialized_metrics=[
        "technical_accuracy",      # Technical term usage
        "concept_coverage",        # Concept completeness  
        "depth_of_explanation",    # Explanation depth
        "practical_applicability"  # Real-world relevance
    ],
    domain_expert_validation=True,  # Enable expert review
    terminology_consistency=True,   # Check term consistency
    citation_accuracy=True         # Verify source citations
)

# Run domain-specific evaluation / ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–è©•ä¾¡ã‚’å®Ÿè¡Œ
domain_results = quality_lab.evaluate_domain_specific(
    query_engine=query_engine,
    domain_config=domain_config,
    expert_qa_pairs=expert_generated_qa_pairs  # Human expert annotations
)

print("Domain-Specific Evaluation Results:")
for metric, score in domain_results['specialized_scores'].items():
    print(f"  {metric}: {score:.3f}")
```

### 7.2 Adversarial Testing / æ•µå¯¾çš„ãƒ†ã‚¹ãƒˆ

```python
# Generate adversarial test cases / æ•µå¯¾çš„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ç”Ÿæˆ
adversarial_cases = quality_lab.generate_adversarial_tests(
    base_documents=documents,
    adversarial_types=[
        "ambiguous_questions",     # Intentionally ambiguous
        "misleading_context",      # Misleading information
        "edge_case_scenarios",     # Unusual edge cases
        "contradictory_sources",   # Conflicting information
        "out_of_scope_queries"     # Beyond corpus knowledge
    ],
    difficulty_levels=["moderate", "hard", "extreme"]
)

# Evaluate robustness / å …ç‰¢æ€§ã‚’è©•ä¾¡
robustness_results = quality_lab.evaluate_robustness(
    query_engine=query_engine,
    adversarial_cases=adversarial_cases
)

print("Robustness Evaluation Results:")
print(f"Robustness score: {robustness_results['overall_robustness']:.3f}")
print(f"Edge case handling: {robustness_results['edge_case_score']:.3f}")
print(f"Ambiguity handling: {robustness_results['ambiguity_score']:.3f}")
print(f"Out-of-scope detection: {robustness_results['oos_detection']:.3f}")
```

## 8. Complete Example / å®Œå…¨ãªä¾‹

```python
#!/usr/bin/env python3
"""
Complete RAG evaluation example
å®Œå…¨ãªRAGè©•ä¾¡ä¾‹
"""

from pathlib import Path
from refinire_rag.application.quality_lab import QualityLab, QualityLabConfig
from refinire_rag.application.query_engine import QueryEngine
from refinire_rag.application.corpus_manager_new import CorpusManager

def main():
    print("ğŸš€ RAG Evaluation Tutorial / RAGè©•ä¾¡ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«")
    print("="*60)
    
    # Setup corpus and QueryEngine (from previous tutorials)
    # ã‚³ãƒ¼ãƒ‘ã‚¹ã¨QueryEngineã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆå‰ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‹ã‚‰ï¼‰
    # ... (corpus setup code)
    
    # Initialize QualityLab / QualityLabåˆæœŸåŒ–
    config = QualityLabConfig(
        qa_pairs_per_document=3,
        similarity_threshold=0.8,
        question_types=["factual", "conceptual", "analytical"],
        evaluation_metrics=["bleu", "rouge", "llm_judge"],
        include_detailed_analysis=True,
        include_contradiction_detection=True
    )
    
    quality_lab = QualityLab(
        corpus_name="tutorial_corpus",
        config=config
    )
    
    # Step 1: Generate QA pairs / ã‚¹ãƒ†ãƒƒãƒ—1: QAãƒšã‚¢ç”Ÿæˆ
    print("\nğŸ“ Step 1: Generating QA pairs...")
    qa_pairs = quality_lab.generate_qa_pairs(documents, num_pairs=20)
    print(f"Generated {len(qa_pairs)} QA pairs")
    
    # Step 2: Evaluate QueryEngine / ã‚¹ãƒ†ãƒƒãƒ—2: QueryEngineè©•ä¾¡
    print("\nğŸ” Step 2: Evaluating QueryEngine...")
    results = quality_lab.evaluate_query_engine(query_engine, qa_pairs)
    
    # Step 3: Generate report / ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\nğŸ“Š Step 3: Generating evaluation report...")
    report = quality_lab.generate_evaluation_report(
        results, 
        "tutorial_evaluation_report.md"
    )
    
    # Step 4: Continuous monitoring / ã‚¹ãƒ†ãƒƒãƒ—4: ç¶™ç¶šçš„ç›£è¦–
    print("\nğŸ”„ Step 4: Setting up continuous monitoring...")
    pipeline = quality_lab.create_evaluation_pipeline(
        config={"evaluation_frequency": "daily"},
        qa_pairs=qa_pairs,
        query_engine=query_engine
    )
    
    print("\nğŸ‰ Evaluation tutorial completed!")
    print("Generated files:")
    print("  - tutorial_evaluation_report.md")
    print("  - Evaluation pipeline configured")
    
    return True

if __name__ == "__main__":
    success = main()
    print("âœ… All done!" if success else "âŒ Failed")
```

## Next Steps / æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

After completing all three parts:
3ã¤ã®ãƒ‘ãƒ¼ãƒˆå…¨ã¦ã‚’å®Œäº†ã—ãŸå¾Œï¼š

1. **Integration** - Combine all components in production workflows
   **çµ±åˆ** - æœ¬ç•ªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµåˆ
2. **Optimization** - Fine-tune based on evaluation results
   **æœ€é©åŒ–** - è©•ä¾¡çµæœã«åŸºã¥ãå¾®èª¿æ•´
3. **Scaling** - Deploy to production with monitoring
   **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°** - ç›£è¦–ä»˜ãã§æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤

## Resources / ãƒªã‚½ãƒ¼ã‚¹

- [QualityLab API Documentation](../api/quality_lab.md)
- [Evaluation Metrics Guide](../development/evaluation_metrics.md)
- [Production Deployment Guide](../development/production_deployment.md)
- [Example Scripts](../../examples/)