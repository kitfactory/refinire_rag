# Part 3: RAGè©•ä¾¡ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

## Overview / æ¦‚è¦

This tutorial demonstrates how to evaluate RAG system performance using refinire-rag's QualityLab. QualityLab provides comprehensive evaluation capabilities including automated QA pair generation, answer quality assessment, contradiction detection, and detailed reporting.

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€refinire-ragã®QualityLabã‚’ä½¿ç”¨ã—ãŸRAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚QualityLabã¯ã€è‡ªå‹•QAãƒšã‚¢ç”Ÿæˆã€å›ç­”å“è³ªè©•ä¾¡ã€çŸ›ç›¾æ¤œå‡ºã€è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’å«ã‚€åŒ…æ‹¬çš„ãªè©•ä¾¡æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

## Learning Objectives / å­¦ç¿’ç›®æ¨™

- Understand RAG evaluation methodologies / RAGè©•ä¾¡æ‰‹æ³•ã®ç†è§£
- Generate automated test datasets / è‡ªå‹•ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆ
- Evaluate QueryEngine performance / QueryEngineãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è©•ä¾¡
- Analyze answer quality and accuracy / å›ç­”å“è³ªã¨ç²¾åº¦ã®åˆ†æ
- Detect contradictions and inconsistencies / çŸ›ç›¾ã¨ä¸æ•´åˆã®æ¤œå‡º
- Generate comprehensive evaluation reports / åŒ…æ‹¬çš„ãªè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ

## Prerequisites / å‰ææ¡ä»¶

```bash
# Complete Part 1 (Corpus Creation) and Part 2 (QueryEngine)
# Part 1ï¼ˆã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆï¼‰ã¨Part 2ï¼ˆQueryEngineï¼‰ã‚’å®Œäº†

# Set environment variables for LLM-based evaluation
export OPENAI_API_KEY="your-api-key"
export REFINIRE_RAG_LLM_MODEL="gpt-4o-mini"
export REFINIRE_RAG_EVALUATION_MODEL="gpt-4"  # For higher quality evaluation
```

## Quick Start Example / ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆä¾‹

```python
from refinire_rag.application.quality_lab import QualityLab, QualityLabConfig
from refinire_rag.application.query_engine import QueryEngine

# Initialize QualityLab
config = QualityLabConfig(
    qa_pairs_per_document=3,
    similarity_threshold=0.8,
    include_detailed_analysis=True
)

quality_lab = QualityLab(
    corpus_name="my_corpus",
    config=config
)

# Generate test cases and evaluate
qa_pairs = quality_lab.generate_qa_pairs(documents, num_pairs=20)
results = quality_lab.evaluate_query_engine(query_engine, qa_pairs)
report = quality_lab.generate_evaluation_report(results, "evaluation_report.md")
```

## 1. QualityLab Architecture / QualityLab ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### 1.1 Core Components / ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

```python
from refinire_rag.application.quality_lab import QualityLab, QualityLabConfig
from refinire_rag.processing.evaluator import (
    BaseEvaluator, BleuEvaluator, RougeEvaluator, 
    LLMJudgeEvaluator, QuestEvalEvaluator
)
from refinire_rag.processing.contradiction_detector import ContradictionDetector
from refinire_rag.processing.test_suite import TestSuite

# QualityLab configuration / QualityLabè¨­å®š
config = QualityLabConfig(
    qa_pairs_per_document=2,           # QA pairs to generate per document
    similarity_threshold=0.75,         # Minimum similarity for answer matching
    question_types=[                   # Types of questions to generate
        "factual",                     # Direct fact extraction
        "conceptual",                  # Concept understanding
        "analytical",                  # Analysis and reasoning
        "comparative",                 # Comparison questions
        "application"                  # Application scenarios
    ],
    evaluation_metrics=[               # Metrics to calculate
        "bleu", "rouge", "llm_judge", "questeval"
    ],
    include_detailed_analysis=True,    # Include detailed analysis
    include_contradiction_detection=True,  # Check for contradictions
    output_format="markdown",          # Report format
    llm_model="gpt-4o-mini",          # Model for QA generation
    evaluation_model="gpt-4"          # Model for evaluation (higher quality)
)

# Initialize QualityLab / QualityLabåˆæœŸåŒ–
quality_lab = QualityLab(
    corpus_name="technical_knowledge_base",
    config=config
)
```

### 1.2 Evaluation Metrics / è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
# Available evaluation metrics / åˆ©ç”¨å¯èƒ½ãªè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹

# 1. BLEU Score - N-gram based similarity
bleu_evaluator = BleuEvaluator(
    max_ngram=4,
    smooth=True
)

# 2. ROUGE Score - Recall-oriented similarity
rouge_evaluator = RougeEvaluator(
    rouge_types=["rouge-1", "rouge-2", "rouge-l"],
    use_stemmer=True
)

# 3. LLM Judge - Model-based quality assessment
llm_judge = LLMJudgeEvaluator(
    model="gpt-4",
    criteria=[
        "accuracy",      # Factual correctness
        "completeness",  # Information completeness
        "relevance",     # Query relevance
        "coherence",     # Answer coherence
        "helpfulness"    # Overall helpfulness
    ],
    scale=5  # 1-5 rating scale
)

# 4. QuestEval - Question-answering evaluation
questeval = QuestEvalEvaluator(
    model="gpt-4o-mini",
    check_answerability=True,
    check_consistency=True
)
```

## 2. Automated QA Pair Generation / è‡ªå‹•QAãƒšã‚¢ç”Ÿæˆ

### 2.1 Basic QA Generation / åŸºæœ¬QAç”Ÿæˆ

```python
from refinire_rag.models.document import Document

# Sample documents for evaluation / è©•ä¾¡ç”¨ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸
documents = [
    Document(
        id="ai_doc_001",
        content="""
        Artificial Intelligence (AI) is the simulation of human intelligence 
        in machines programmed to think like humans. Key applications include 
        machine learning, natural language processing, and computer vision.
        """,
        metadata={
            "source": "AI Textbook",
            "topic": "ai_fundamentals",
            "difficulty": "beginner"
        }
    ),
    Document(
        id="ml_doc_002",
        content="""
        Machine Learning is a subset of AI that enables computers to learn 
        without explicit programming. Popular algorithms include neural networks, 
        decision trees, and support vector machines.
        """,
        metadata={
            "source": "ML Guide",
            "topic": "machine_learning",
            "difficulty": "intermediate"
        }
    )
]

# Generate QA pairs / QAãƒšã‚¢ç”Ÿæˆ
qa_pairs = quality_lab.generate_qa_pairs(
    documents=documents,
    num_pairs=10,
    question_types=["factual", "conceptual", "analytical"]
)

print(f"Generated {len(qa_pairs)} QA pairs")

# Inspect generated QA pairs / ç”Ÿæˆã•ã‚ŒãŸQAãƒšã‚¢ã‚’ç¢ºèª
for i, qa_pair in enumerate(qa_pairs[:3]):
    print(f"\nQA Pair {i+1}:")
    print(f"Document: {qa_pair.document_id}")
    print(f"Question Type: {qa_pair.metadata['question_type']}")
    print(f"Question: {qa_pair.question}")
    print(f"Expected Answer: {qa_pair.answer[:100]}...")
    print(f"Difficulty: {qa_pair.metadata.get('difficulty', 'N/A')}")
```

### 2.2 Advanced QA Generation / é«˜åº¦ãªQAç”Ÿæˆ

```python
# Custom QA generation with specific instructions / ç‰¹å®šæŒ‡ç¤ºä»˜ãã‚«ã‚¹ã‚¿ãƒ QAç”Ÿæˆ
custom_config = QualityLabConfig(
    qa_pairs_per_document=3,
    question_types=["factual", "conceptual", "analytical", "comparative"],
    qa_generation_instructions="""
    Generate high-quality question-answer pairs that test deep understanding:
    
    1. Factual questions should test specific details and definitions
    2. Conceptual questions should test understanding of key concepts
    3. Analytical questions should require reasoning and analysis
    4. Comparative questions should test ability to compare concepts
    
    Ensure questions are:
    - Clear and unambiguous
    - Answerable from the provided content
    - Varied in complexity
    - Relevant to the domain
    """,
    include_context_questions=True,    # Questions requiring context
    include_edge_cases=True,           # Test edge cases
    difficulty_levels=["beginner", "intermediate", "advanced"]
)

# Generate with custom configuration / ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ç”Ÿæˆ
custom_qa_pairs = quality_lab.generate_qa_pairs(
    documents=documents,
    num_pairs=15,
    config_override=custom_config
)

# Analyze question diversity / è³ªå•ã®å¤šæ§˜æ€§ã‚’åˆ†æ
question_types = {}
difficulty_levels = {}

for qa_pair in custom_qa_pairs:
    q_type = qa_pair.metadata.get('question_type', 'unknown')
    difficulty = qa_pair.metadata.get('difficulty', 'unknown')
    
    question_types[q_type] = question_types.get(q_type, 0) + 1
    difficulty_levels[difficulty] = difficulty_levels.get(difficulty, 0) + 1

print("Question Type Distribution:")
for q_type, count in question_types.items():
    print(f"  {q_type}: {count}")

print("Difficulty Distribution:")
for difficulty, count in difficulty_levels.items():
    print(f"  {difficulty}: {count}")
```

## 3. QueryEngine Evaluation / QueryEngineè©•ä¾¡

### 3.1 Basic Performance Evaluation / åŸºæœ¬ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡

```python
# Evaluate QueryEngine with generated QA pairs
# ç”Ÿæˆã•ã‚ŒãŸQAãƒšã‚¢ã§QueryEngineã‚’è©•ä¾¡
evaluation_results = quality_lab.evaluate_query_engine(
    query_engine=query_engine,
    qa_pairs=qa_pairs,
    evaluation_metrics=["bleu", "rouge", "llm_judge"],
    include_contradiction_detection=True,
    parallel_evaluation=True  # Speed up evaluation
)

print("Evaluation Results Summary:")
print(f"Total test cases: {len(evaluation_results['test_results'])}")
print(f"Processing time: {evaluation_results['processing_time']:.2f}s")

# Calculate pass rate / åˆæ ¼ç‡ã‚’è¨ˆç®—
passed_tests = sum(1 for result in evaluation_results['test_results'] if result['passed'])
pass_rate = (passed_tests / len(evaluation_results['test_results'])) * 100

print(f"Pass rate: {pass_rate:.1f}% ({passed_tests}/{len(evaluation_results['test_results'])})")

# Show metric summaries / ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¦ç´„ã‚’è¡¨ç¤º
if 'metric_summaries' in evaluation_results:
    print("\nMetric Summaries:")
    for metric, summary in evaluation_results['metric_summaries'].items():
        print(f"  {metric.upper()}:")
        print(f"    Average: {summary.get('average', 0):.3f}")
        print(f"    Min: {summary.get('min', 0):.3f}")
        print(f"    Max: {summary.get('max', 0):.3f}")
```

### 3.2 Detailed Analysis / è©³ç´°åˆ†æ

```python
def analyze_evaluation_results(evaluation_results):
    """
    Perform detailed analysis of evaluation results
    è©•ä¾¡çµæœã®è©³ç´°åˆ†æã‚’å®Ÿè¡Œ
    """
    
    test_results = evaluation_results['test_results']
    
    print("="*60)
    print("DETAILED EVALUATION ANALYSIS / è©³ç´°è©•ä¾¡åˆ†æ")
    print("="*60)
    
    # Performance by question type / è³ªå•ã‚¿ã‚¤ãƒ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    performance_by_type = {}
    for result in test_results:
        q_type = result.get('question_type', 'unknown')
        if q_type not in performance_by_type:
            performance_by_type[q_type] = {'total': 0, 'passed': 0, 'scores': []}
        
        performance_by_type[q_type]['total'] += 1
        if result['passed']:
            performance_by_type[q_type]['passed'] += 1
        performance_by_type[q_type]['scores'].append(result.get('score', 0))
    
    print("\nğŸ“Š Performance by Question Type / è³ªå•ã‚¿ã‚¤ãƒ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
    for q_type, stats in performance_by_type.items():
        pass_rate = (stats['passed'] / stats['total']) * 100
        avg_score = sum(stats['scores']) / len(stats['scores'])
        print(f"  {q_type.capitalize()}:")
        print(f"    Pass rate / åˆæ ¼ç‡: {pass_rate:.1f}% ({stats['passed']}/{stats['total']})")
        print(f"    Average score / å¹³å‡ã‚¹ã‚³ã‚¢: {avg_score:.3f}")
    
    # Response time analysis / å¿œç­”æ™‚é–“åˆ†æ
    response_times = [result.get('processing_time', 0) for result in test_results]
    if response_times:
        avg_time = sum(response_times) / len(response_times)
        min_time = min(response_times)
        max_time = max(response_times)
        
        print(f"\nâ±ï¸  Response Time Analysis / å¿œç­”æ™‚é–“åˆ†æ:")
        print(f"   Average: {avg_time:.3f}s")
        print(f"   Fastest: {min_time:.3f}s")
        print(f"   Slowest: {max_time:.3f}s")
    
    # Confidence analysis / ä¿¡é ¼åº¦åˆ†æ
    confidences = [result.get('confidence', 0) for result in test_results if 'confidence' in result]
    if confidences:
        avg_confidence = sum(confidences) / len(confidences)
        high_confidence = sum(1 for c in confidences if c > 0.8)
        low_confidence = sum(1 for c in confidences if c < 0.3)
        
        print(f"\nğŸ¯ Confidence Analysis / ä¿¡é ¼åº¦åˆ†æ:")
        print(f"   Average confidence / å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.3f}")
        print(f"   High confidence (>0.8) / é«˜ä¿¡é ¼åº¦: {high_confidence}/{len(confidences)}")
        print(f"   Low confidence (<0.3) / ä½ä¿¡é ¼åº¦: {low_confidence}/{len(confidences)}")
    
    # Error analysis / ã‚¨ãƒ©ãƒ¼åˆ†æ
    failed_tests = [result for result in test_results if not result['passed']]
    if failed_tests:
        print(f"\nâŒ Failed Test Analysis / å¤±æ•—ãƒ†ã‚¹ãƒˆåˆ†æ:")
        print(f"   Total failures / ç·å¤±æ•—æ•°: {len(failed_tests)}")
        
        # Group failures by reason / ç†ç”±åˆ¥å¤±æ•—ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        failure_reasons = {}
        for result in failed_tests:
            reason = result.get('failure_reason', 'unknown')
            failure_reasons[reason] = failure_reasons.get(reason, 0) + 1
        
        print("   Failure reasons / å¤±æ•—ç†ç”±:")
        for reason, count in failure_reasons.items():
            print(f"     {reason}: {count}")

# Perform detailed analysis / è©³ç´°åˆ†æã‚’å®Ÿè¡Œ
analyze_evaluation_results(evaluation_results)
```

### 3.3 Contradiction Detection / çŸ›ç›¾æ¤œå‡º

```python
# Advanced contradiction detection / é«˜åº¦ãªçŸ›ç›¾æ¤œå‡º
contradiction_results = quality_lab.detect_contradictions(
    corpus_documents=documents,
    query_engine=query_engine,
    test_queries=[
        "What is artificial intelligence?",
        "How does machine learning work?",
        "What are the applications of AI?"
    ]
)

print("Contradiction Detection Results:")
print(f"Documents analyzed: {len(documents)}")
print(f"Contradictions found: {len(contradiction_results['contradictions'])}")

# Analyze contradictions / çŸ›ç›¾ã‚’åˆ†æ
if contradiction_results['contradictions']:
    print("\nDetected Contradictions:")
    for i, contradiction in enumerate(contradiction_results['contradictions'][:3]):
        print(f"\n{i+1}. Contradiction:")
        print(f"   Statement 1: {contradiction['statement_1'][:100]}...")
        print(f"   Statement 2: {contradiction['statement_2'][:100]}...")
        print(f"   Confidence: {contradiction['confidence']:.3f}")
        print(f"   Type: {contradiction['type']}")
        print(f"   Source documents: {contradiction['source_documents']}")

# Consistency check across answers / å›ç­”é–“ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
consistency_results = quality_lab.check_answer_consistency(
    query_engine=query_engine,
    similar_queries=[
        ["What is AI?", "Define artificial intelligence", "Explain AI"],
        ["How does ML work?", "Explain machine learning", "Describe ML process"]
    ]
)

print(f"\nConsistency Analysis:")
print(f"Query groups tested: {len(similar_queries)}")
print(f"Average consistency score: {consistency_results['average_consistency']:.3f}")
```

## 4. Evaluation Metrics / è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹

### 4.1 Automatic Metrics / è‡ªå‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
# Configure evaluation metrics / è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨­å®š
evaluation_config = {
    "bleu": {
        "max_ngram": 4,
        "smooth": True,
        "weights": [0.25, 0.25, 0.25, 0.25]
    },
    "rouge": {
        "rouge_types": ["rouge-1", "rouge-2", "rouge-l"],
        "use_stemmer": True,
        "alpha": 0.5  # Balance precision and recall
    },
    "questeval": {
        "model": "gpt-4o-mini",
        "check_answerability": True,
        "check_consistency": True,
        "language": "japanese"  # For Japanese evaluation
    }
}

# Run comprehensive evaluation / åŒ…æ‹¬çš„è©•ä¾¡ã‚’å®Ÿè¡Œ
comprehensive_results = quality_lab.run_comprehensive_evaluation(
    corpus_documents=documents,
    query_engine=query_engine,
    num_qa_pairs=20,
    evaluation_config=evaluation_config,
    include_human_baseline=False,  # Set True if human answers available
    cross_validation_folds=3       # For robust evaluation
)

print("Comprehensive Evaluation Results:")
print(f"Total QA pairs: {comprehensive_results['total_qa_pairs']}")
print(f"Evaluation time: {comprehensive_results['total_evaluation_time']:.2f}s")

# Display metric results / ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµæœã‚’è¡¨ç¤º
for metric, results in comprehensive_results['metric_results'].items():
    print(f"\n{metric.upper()} Results:")
    print(f"  Average: {results['average']:.3f}")
    print(f"  Standard deviation: {results['std_dev']:.3f}")
    print(f"  Min: {results['min']:.3f}")
    print(f"  Max: {results['max']:.3f}")
    
    # Percentile analysis / ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«åˆ†æ
    if 'percentiles' in results:
        print(f"  25th percentile: {results['percentiles']['25']:.3f}")
        print(f"  50th percentile (median): {results['percentiles']['50']:.3f}")
        print(f"  75th percentile: {results['percentiles']['75']:.3f}")
```

### 4.2 LLM-based Evaluation / LLMãƒ™ãƒ¼ã‚¹è©•ä¾¡

```python
# Configure LLM Judge for detailed evaluation / è©³ç´°è©•ä¾¡ç”¨LLMåˆ¤å®šã‚’è¨­å®š
llm_judge_config = {
    "model": "gpt-4",
    "evaluation_criteria": {
        "accuracy": {
            "description": "How factually correct is the answer?",
            "scale": "1-5 (1=incorrect, 5=completely accurate)",
            "weight": 0.3
        },
        "completeness": {
            "description": "How complete is the answer?",
            "scale": "1-5 (1=missing key info, 5=comprehensive)",
            "weight": 0.25
        },
        "relevance": {
            "description": "How relevant is the answer to the question?",
            "scale": "1-5 (1=off-topic, 5=directly relevant)",
            "weight": 0.25
        },
        "clarity": {
            "description": "How clear and understandable is the answer?",
            "scale": "1-5 (1=confusing, 5=very clear)",
            "weight": 0.2
        }
    },
    "evaluation_instructions": """
    You are an expert evaluator of question-answering systems.
    Evaluate each answer based on the provided criteria.
    
    For each criterion:
    1. Provide a score from 1-5
    2. Give a brief explanation for your score
    3. Identify specific strengths and weaknesses
    
    Be objective and consistent in your evaluations.
    """
}

# Run LLM-based evaluation / LLMãƒ™ãƒ¼ã‚¹è©•ä¾¡ã‚’å®Ÿè¡Œ
llm_evaluation = quality_lab.evaluate_with_llm_judge(
    test_results=evaluation_results['test_results'],
    config=llm_judge_config
)

print("LLM Judge Evaluation Results:")
print(f"Evaluated answers: {len(llm_evaluation['evaluations'])}")

# Aggregate LLM scores / LLMã‚¹ã‚³ã‚¢ã‚’é›†è¨ˆ
criteria_scores = {}
for evaluation in llm_evaluation['evaluations']:
    for criterion, score_data in evaluation['scores'].items():
        if criterion not in criteria_scores:
            criteria_scores[criterion] = []
        criteria_scores[criterion].append(score_data['score'])

print("\nLLM Judge Scores by Criteria:")
for criterion, scores in criteria_scores.items():
    avg_score = sum(scores) / len(scores)
    print(f"  {criterion.capitalize()}: {avg_score:.2f}/5.0")

# Show sample evaluations / ã‚µãƒ³ãƒ—ãƒ«è©•ä¾¡ã‚’è¡¨ç¤º
print("\nSample LLM Evaluations:")
for i, evaluation in enumerate(llm_evaluation['evaluations'][:2]):
    print(f"\nEvaluation {i+1}:")
    print(f"Question: {evaluation['question'][:80]}...")
    print(f"Answer: {evaluation['answer'][:80]}...")
    print("Scores:")
    for criterion, score_data in evaluation['scores'].items():
        print(f"  {criterion}: {score_data['score']}/5 - {score_data['explanation']}")
```

## 5. Report Generation / ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

### 5.1 Comprehensive Reports / åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆ

```python
# Generate detailed evaluation report / è©³ç´°è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
report_config = {
    "include_executive_summary": True,
    "include_detailed_analysis": True,
    "include_recommendations": True,
    "include_visualizations": True,
    "include_raw_data": False,  # Set True for full data export
    "format": "markdown",
    "language": "bilingual"  # English and Japanese
}

# Generate comprehensive report / åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
report = quality_lab.generate_evaluation_report(
    evaluation_results=comprehensive_results,
    output_file="comprehensive_rag_evaluation.md",
    config=report_config
)

print(f"Generated comprehensive report: comprehensive_rag_evaluation.md")
print(f"Report length: {len(report)} characters")

# Generate executive summary / è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
executive_summary = quality_lab.generate_executive_summary(
    evaluation_results=comprehensive_results,
    target_audience="technical_management",  # Options: technical, management, technical_management
    key_findings_limit=5
)

print("\nExecutive Summary Preview:")
print("="*40)
print(executive_summary[:500] + "...")
```

### 5.2 Custom Report Templates / ã‚«ã‚¹ã‚¿ãƒ ãƒ¬ãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```python
# Custom report template / ã‚«ã‚¹ã‚¿ãƒ ãƒ¬ãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
custom_template = """
# RAG System Evaluation Report
# RAGã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

## Executive Summary / è¦ç´„
{executive_summary}

## Test Configuration / ãƒ†ã‚¹ãƒˆè¨­å®š
- Total QA Pairs: {total_qa_pairs}
- Evaluation Metrics: {metrics_used}
- Test Duration: {test_duration}
- Query Engine Version: {query_engine_version}

## Performance Results / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµæœ
{performance_table}

## Quality Analysis / å“è³ªåˆ†æ
{quality_analysis}

## Recommendations / æ¨å¥¨äº‹é …
{recommendations}

## Detailed Results / è©³ç´°çµæœ
{detailed_results}
"""

# Generate custom report / ã‚«ã‚¹ã‚¿ãƒ ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
custom_report = quality_lab.generate_custom_report(
    evaluation_results=comprehensive_results,
    template=custom_template,
    output_file="custom_evaluation_report.md"
)

print("Generated custom report with template")
```

## 6. Continuous Evaluation / ç¶™ç¶šçš„è©•ä¾¡

### 6.1 Automated Testing Pipeline / è‡ªå‹•ãƒ†ã‚¹ãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
# Setup continuous evaluation pipeline / ç¶™ç¶šçš„è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
pipeline_config = {
    "evaluation_frequency": "daily",      # daily, weekly, on_update
    "baseline_threshold": 0.75,           # Minimum acceptable performance
    "regression_threshold": 0.05,         # Alert if performance drops > 5%
    "auto_report_generation": True,
    "alert_recipients": ["team@company.com"],
    "comparison_window": 7                # Compare with last N evaluations
}

# Create evaluation pipeline / è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
evaluation_pipeline = quality_lab.create_evaluation_pipeline(
    config=pipeline_config,
    qa_pairs=qa_pairs,
    query_engine=query_engine
)

# Run pipeline evaluation / ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚’å®Ÿè¡Œ
pipeline_results = evaluation_pipeline.run_evaluation()

print("Pipeline Evaluation Results:")
print(f"Current performance: {pipeline_results['current_score']:.3f}")
print(f"Baseline performance: {pipeline_results['baseline_score']:.3f}")
print(f"Performance change: {pipeline_results['score_change']:+.3f}")

# Check for regressions / å›å¸°ã‚’ãƒã‚§ãƒƒã‚¯
if pipeline_results['regression_detected']:
    print("âš ï¸  Performance regression detected!")
    print(f"   Regression severity: {pipeline_results['regression_severity']}")
    print(f"   Affected areas: {pipeline_results['affected_areas']}")
else:
    print("âœ… No performance regression detected")
```

### 6.2 A/B Testing Framework / A/Bãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

```python
# Setup A/B test for QueryEngine improvements / QueryEngineæ”¹å–„ã®A/Bãƒ†ã‚¹ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
ab_test_config = {
    "test_name": "retriever_optimization_v1",
    "control_group": "current_retriever",
    "treatment_group": "optimized_retriever", 
    "sample_size": 100,                   # QA pairs per group
    "confidence_level": 0.95,
    "minimum_effect_size": 0.05           # Minimum improvement to detect
}

# Run A/B test / A/Bãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
ab_results = quality_lab.run_ab_test(
    control_engine=current_query_engine,
    treatment_engine=optimized_query_engine,
    test_config=ab_test_config,
    qa_pairs=qa_pairs
)

print("A/B Test Results:")
print(f"Control group performance: {ab_results['control_score']:.3f}")
print(f"Treatment group performance: {ab_results['treatment_score']:.3f}")
print(f"Improvement: {ab_results['improvement']:.3f}")
print(f"Statistical significance: {ab_results['p_value']:.4f}")
print(f"Significant improvement: {'Yes' if ab_results['significant'] else 'No'}")

# Detailed A/B analysis / è©³ç´°A/Båˆ†æ
if ab_results['significant']:
    print(f"\nâœ… Significant improvement detected!")
    print(f"   Effect size: {ab_results['effect_size']:.3f}")
    print(f"   Confidence interval: [{ab_results['ci_lower']:.3f}, {ab_results['ci_upper']:.3f}]")
    print(f"   Recommendation: Deploy treatment to production")
else:
    print(f"\nâŒ No significant improvement")
    print(f"   Required sample size for detection: {ab_results['required_sample_size']}")
```

## 7. Advanced Evaluation Techniques / é«˜åº¦ãªè©•ä¾¡æŠ€è¡“

### 7.1 Domain-Specific Evaluation / ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–è©•ä¾¡

```python
# Configure domain-specific evaluation / ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–è©•ä¾¡ã‚’è¨­å®š
domain_config = QualityLabConfig(
    domain="technical_ai",
    specialized_metrics=[
        "technical_accuracy",      # Technical term usage
        "concept_coverage",        # Concept completeness  
        "depth_of_explanation",    # Explanation depth
        "practical_applicability"  # Real-world relevance
    ],
    domain_expert_validation=True,  # Enable expert review
    terminology_consistency=True,   # Check term consistency
    citation_accuracy=True         # Verify source citations
)

# Run domain-specific evaluation / ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–è©•ä¾¡ã‚’å®Ÿè¡Œ
domain_results = quality_lab.evaluate_domain_specific(
    query_engine=query_engine,
    domain_config=domain_config,
    expert_qa_pairs=expert_generated_qa_pairs  # Human expert annotations
)

print("Domain-Specific Evaluation Results:")
for metric, score in domain_results['specialized_scores'].items():
    print(f"  {metric}: {score:.3f}")
```

### 7.2 Adversarial Testing / æ•µå¯¾çš„ãƒ†ã‚¹ãƒˆ

```python
# Generate adversarial test cases / æ•µå¯¾çš„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ç”Ÿæˆ
adversarial_cases = quality_lab.generate_adversarial_tests(
    base_documents=documents,
    adversarial_types=[
        "ambiguous_questions",     # Intentionally ambiguous
        "misleading_context",      # Misleading information
        "edge_case_scenarios",     # Unusual edge cases
        "contradictory_sources",   # Conflicting information
        "out_of_scope_queries"     # Beyond corpus knowledge
    ],
    difficulty_levels=["moderate", "hard", "extreme"]
)

# Evaluate robustness / å …ç‰¢æ€§ã‚’è©•ä¾¡
robustness_results = quality_lab.evaluate_robustness(
    query_engine=query_engine,
    adversarial_cases=adversarial_cases
)

print("Robustness Evaluation Results:")
print(f"Robustness score: {robustness_results['overall_robustness']:.3f}")
print(f"Edge case handling: {robustness_results['edge_case_score']:.3f}")
print(f"Ambiguity handling: {robustness_results['ambiguity_score']:.3f}")
print(f"Out-of-scope detection: {robustness_results['oos_detection']:.3f}")
```

## 8. Complete Example / å®Œå…¨ãªä¾‹

```python
#!/usr/bin/env python3
"""
Complete RAG evaluation example
å®Œå…¨ãªRAGè©•ä¾¡ä¾‹
"""

from pathlib import Path
from refinire_rag.application.quality_lab import QualityLab, QualityLabConfig
from refinire_rag.application.query_engine import QueryEngine
from refinire_rag.application.corpus_manager_new import CorpusManager

def main():
    print("ğŸš€ RAG Evaluation Tutorial / RAGè©•ä¾¡ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«")
    print("="*60)
    
    # Setup corpus and QueryEngine (from previous tutorials)
    # ã‚³ãƒ¼ãƒ‘ã‚¹ã¨QueryEngineã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆå‰ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‹ã‚‰ï¼‰
    # ... (corpus setup code)
    
    # Initialize QualityLab / QualityLabåˆæœŸåŒ–
    config = QualityLabConfig(
        qa_pairs_per_document=3,
        similarity_threshold=0.8,
        question_types=["factual", "conceptual", "analytical"],
        evaluation_metrics=["bleu", "rouge", "llm_judge"],
        include_detailed_analysis=True,
        include_contradiction_detection=True
    )
    
    quality_lab = QualityLab(
        corpus_name="tutorial_corpus",
        config=config
    )
    
    # Step 1: Generate QA pairs / ã‚¹ãƒ†ãƒƒãƒ—1: QAãƒšã‚¢ç”Ÿæˆ
    print("\nğŸ“ Step 1: Generating QA pairs...")
    qa_pairs = quality_lab.generate_qa_pairs(documents, num_pairs=20)
    print(f"Generated {len(qa_pairs)} QA pairs")
    
    # Step 2: Evaluate QueryEngine / ã‚¹ãƒ†ãƒƒãƒ—2: QueryEngineè©•ä¾¡
    print("\nğŸ” Step 2: Evaluating QueryEngine...")
    results = quality_lab.evaluate_query_engine(query_engine, qa_pairs)
    
    # Step 3: Generate report / ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\nğŸ“Š Step 3: Generating evaluation report...")
    report = quality_lab.generate_evaluation_report(
        results, 
        "tutorial_evaluation_report.md"
    )
    
    # Step 4: Continuous monitoring / ã‚¹ãƒ†ãƒƒãƒ—4: ç¶™ç¶šçš„ç›£è¦–
    print("\nğŸ”„ Step 4: Setting up continuous monitoring...")
    pipeline = quality_lab.create_evaluation_pipeline(
        config={"evaluation_frequency": "daily"},
        qa_pairs=qa_pairs,
        query_engine=query_engine
    )
    
    print("\nğŸ‰ Evaluation tutorial completed!")
    print("Generated files:")
    print("  - tutorial_evaluation_report.md")
    print("  - Evaluation pipeline configured")
    
    return True

if __name__ == "__main__":
    success = main()
    print("âœ… All done!" if success else "âŒ Failed")
```

## Next Steps / æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

After completing all three parts:
3ã¤ã®ãƒ‘ãƒ¼ãƒˆå…¨ã¦ã‚’å®Œäº†ã—ãŸå¾Œï¼š

1. **Integration** - Combine all components in production workflows
   **çµ±åˆ** - æœ¬ç•ªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµåˆ
2. **Optimization** - Fine-tune based on evaluation results
   **æœ€é©åŒ–** - è©•ä¾¡çµæœã«åŸºã¥ãå¾®èª¿æ•´
3. **Scaling** - Deploy to production with monitoring
   **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°** - ç›£è¦–ä»˜ãã§æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤

## Resources / ãƒªã‚½ãƒ¼ã‚¹

- [QualityLab API Documentation](../api/quality_lab.md)
- [Evaluation Metrics Guide](../development/evaluation_metrics.md)
- [Production Deployment Guide](../development/production_deployment.md)
- [Example Scripts](../../examples/)