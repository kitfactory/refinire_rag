# Part 1: ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

## Overview / æ¦‚è¦

This tutorial demonstrates how to create and manage a document corpus using refinire-rag's CorpusManager. The CorpusManager provides flexible corpus building with preset configurations, stage selection, and custom pipelines.

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€refinire-ragã®CorpusManagerã‚’ä½¿ç”¨ã—ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½œæˆãƒ»ç®¡ç†ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚CorpusManagerã¯ã€ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šã€ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠã€ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ã‚ˆã‚‹æŸ”è»Ÿãªã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰ã‚’æä¾›ã—ã¾ã™ã€‚

## Learning Objectives / å­¦ç¿’ç›®æ¨™

- Understand different corpus building approaches / ç•°ãªã‚‹ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ç†è§£
- Learn preset configurations (Simple, Semantic, Knowledge RAG) / ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šã®å­¦ç¿’ï¼ˆSimpleã€Semanticã€Knowledge RAGï¼‰
- Master stage selection for custom workflows / ã‚«ã‚¹ã‚¿ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç”¨ã®ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠã®ãƒã‚¹ã‚¿ãƒ¼
- Create custom pipelines for specialized processing / å°‚ç”¨å‡¦ç†ã®ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ

## Prerequisites / å‰ææ¡ä»¶

```bash
# Install refinire-rag
pip install refinire-rag

# Set environment variables (if using LLM features)
export OPENAI_API_KEY="your-api-key"
export REFINIRE_RAG_LLM_MODEL="gpt-4o-mini"
```

## Core Concepts / åŸºæœ¬æ¦‚å¿µ

### What is a Document Corpus? / ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹ã¨ã¯ï¼Ÿ

A **document corpus** is a structured collection of processed documents that serves as the foundation for RAG (Retrieval-Augmented Generation) systems. Think of it as a searchable knowledge base where:

**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹**ã¯ã€RAGï¼ˆæ¤œç´¢æ‹¡å¼µç”Ÿæˆï¼‰ã‚·ã‚¹ãƒ†ãƒ ã®åŸºç¤ã¨ãªã‚‹ã€å‡¦ç†æ¸ˆã¿æ–‡æ›¸ã®æ§‹é€ åŒ–ã•ã‚ŒãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã§ã™ã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªæ¤œç´¢å¯èƒ½ãªçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã¨è€ƒãˆã¦ãã ã•ã„ï¼š

- **Raw documents** are converted into machine-readable format / **ç”Ÿæ–‡æ›¸**ã‚’æ©Ÿæ¢°å¯èª­å½¢å¼ã«å¤‰æ›
- **Text is segmented** into meaningful chunks for better retrieval / **ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²**ã—ã¦æ¤œç´¢ç²¾åº¦ã‚’å‘ä¸Š
- **Semantic embeddings** are generated for similarity search / **ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿**ã‚’ç”Ÿæˆã—ã¦é¡ä¼¼æ¤œç´¢ã‚’å®Ÿç¾
- **Metadata** is extracted and indexed for filtering / **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**ã‚’æŠ½å‡ºãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’å¯èƒ½ã«

### Why Different Corpus Building Approaches? / ãªãœç•°ãªã‚‹ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦ï¼Ÿ

Different use cases require different processing strategies:

ç•°ãªã‚‹ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«ã¯ã€ç•°ãªã‚‹å‡¦ç†æˆ¦ç•¥ãŒå¿…è¦ã§ã™ï¼š

#### 1. **Simple RAG** - Quick Prototyping / ã‚¯ã‚¤ãƒƒã‚¯ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°
- **When to use**: Testing, demos, simple applications / **ä½¿ç”¨å ´é¢**: ãƒ†ã‚¹ãƒˆã€ãƒ‡ãƒ¢ã€ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
- **Processing**: Load â†’ Chunk â†’ Vectorize / **å‡¦ç†**: èª­ã¿è¾¼ã¿ â†’ ãƒãƒ£ãƒ³ã‚¯åŒ– â†’ ãƒ™ã‚¯ãƒˆãƒ«åŒ–
- **Benefits**: Fast setup, minimal configuration / **åˆ©ç‚¹**: é«˜é€Ÿã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€æœ€å°è¨­å®š

#### 2. **Semantic RAG** - Enhanced Understanding / ç†è§£åŠ›å‘ä¸Š
- **When to use**: Domain-specific content, terminology consistency / **ä½¿ç”¨å ´é¢**: å°‚é–€åˆ†é‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ç”¨èªçµ±ä¸€
- **Processing**: Load â†’ Dictionary â†’ Normalize â†’ Chunk â†’ Vectorize / **å‡¦ç†**: èª­ã¿è¾¼ã¿ â†’ è¾æ›¸ â†’ æ­£è¦åŒ– â†’ ãƒãƒ£ãƒ³ã‚¯åŒ– â†’ ãƒ™ã‚¯ãƒˆãƒ«åŒ–
- **Benefits**: Better semantic consistency, domain adaptation / **åˆ©ç‚¹**: ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ä¸€è²«æ€§å‘ä¸Šã€ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œ

#### 3. **Knowledge RAG** - Advanced Analytics / é«˜åº¦ãªåˆ†æ
- **When to use**: Complex relationships, knowledge discovery / **ä½¿ç”¨å ´é¢**: è¤‡é›‘ãªé–¢ä¿‚æ€§ã€çŸ¥è­˜ç™ºè¦‹
- **Processing**: Load â†’ Dictionary â†’ Graph â†’ Normalize â†’ Chunk â†’ Vectorize / **å‡¦ç†**: èª­ã¿è¾¼ã¿ â†’ è¾æ›¸ â†’ ã‚°ãƒ©ãƒ• â†’ æ­£è¦åŒ– â†’ ãƒãƒ£ãƒ³ã‚¯åŒ– â†’ ãƒ™ã‚¯ãƒˆãƒ«åŒ–
- **Benefits**: Relationship extraction, enhanced reasoning / **åˆ©ç‚¹**: é–¢ä¿‚æŠ½å‡ºã€æ¨è«–èƒ½åŠ›å‘ä¸Š

### Document Processing Pipeline / æ–‡æ›¸å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

Understanding the document processing stages is crucial for choosing the right approach:

é©åˆ‡ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é¸æŠã™ã‚‹ã«ã¯ã€æ–‡æ›¸å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¸ã®ç†è§£ãŒé‡è¦ã§ã™ï¼š

```
ğŸ“„ Raw Documents â†’ ğŸ”„ Processing Pipeline â†’ ğŸ—‚ï¸ Searchable Corpus
                        â†“
              [Load] â†’ [Analyze] â†’ [Normalize] â†’ [Chunk] â†’ [Vectorize]
```

Each stage serves a specific purpose:

å„ã‚¹ãƒ†ãƒ¼ã‚¸ã«ã¯ç‰¹å®šã®ç›®çš„ãŒã‚ã‚Šã¾ã™ï¼š

| Stage | Purpose | What It Does | When to Use |
|-------|---------|--------------|-------------|
| **Load** | Import | File parsing and document creation / ãƒ•ã‚¡ã‚¤ãƒ«è§£æã¨æ–‡æ›¸ä½œæˆ | Always required / å¸¸ã«å¿…è¦ |
| **Dictionary** | Analyze | Extract domain terms and abbreviations / å°‚é–€ç”¨èªãƒ»ç•¥èªæŠ½å‡º | Domain-specific content / å°‚é–€åˆ†é‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ |
| **Graph** | Relate | Build knowledge relationships / çŸ¥è­˜é–¢ä¿‚ã®æ§‹ç¯‰ | Complex documents / è¤‡é›‘ãªæ–‡æ›¸ |
| **Normalize** | Standardize | Unify terminology variations / ç”¨èªãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã®çµ±ä¸€ | Inconsistent terminology / ç”¨èªã®ä¸çµ±ä¸€ |
| **Chunk** | Segment | Break text into optimal pieces / ãƒ†ã‚­ã‚¹ãƒˆã‚’æœ€é©ãªæ–­ç‰‡ã«åˆ†å‰² | Always required / å¸¸ã«å¿…è¦ |
| **Vector** | Index | Generate semantic embeddings / ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ | Always required / å¸¸ã«å¿…è¦ |

### Choosing Your Approach / ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®é¸æŠ

**Decision Tree** for selecting the right corpus building approach:

é©åˆ‡ãªã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é¸æŠã™ã‚‹ãŸã‚ã®**æ±ºå®šãƒ„ãƒªãƒ¼**ï¼š

```
ğŸ¤” What type of content are you processing?
   ã‚ãªãŸãŒå‡¦ç†ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚¿ã‚¤ãƒ—ã¯ï¼Ÿ

â”œâ”€â”€ ğŸ“ General documents, quick start needed
â”‚   ä¸€èˆ¬çš„ãªæ–‡æ›¸ã€ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆãŒå¿…è¦
â”‚   â†’ Use Simple RAG
â”‚
â”œâ”€â”€ ğŸ¥ Domain-specific with technical terms  
â”‚   æŠ€è¡“ç”¨èªã‚’å«ã‚€å°‚é–€åˆ†é‡
â”‚   â†’ Use Semantic RAG
â”‚
â””â”€â”€ ğŸ”¬ Complex documents with relationships
    é–¢ä¿‚æ€§ã‚’å«ã‚€è¤‡é›‘ãªæ–‡æ›¸
    â†’ Use Knowledge RAG
```

## Quick Start Example / ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆä¾‹

This example demonstrates the simplest way to create a corpus using the **Simple RAG** approach:

ã“ã®ä¾‹ã§ã¯ã€**Simple RAG**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½œæˆã™ã‚‹æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªæ–¹æ³•ã‚’ç¤ºã—ã¾ã™ï¼š

```python
from refinire_rag.application.corpus_manager_new import CorpusManager
from refinire_rag.storage.sqlite_store import SQLiteDocumentStore
from refinire_rag.storage.in_memory_vector_store import InMemoryVectorStore

# ğŸ—„ï¸ STEP 1: Initialize storage components
#    ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
doc_store = SQLiteDocumentStore("documents.db")        # Stores document metadata / æ–‡æ›¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
vector_store = InMemoryVectorStore()                   # Stores vector embeddings / ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã‚’ä¿å­˜

# ğŸ”§ STEP 2: Create Simple RAG corpus manager
#    ã‚¹ãƒ†ãƒƒãƒ—2: Simple RAGã‚³ãƒ¼ãƒ‘ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ä½œæˆ
#    This implements: Load â†’ Chunk â†’ Vector pipeline
#    ã“ã‚Œã¯ä»¥ä¸‹ã‚’å®Ÿè£…: èª­ã¿è¾¼ã¿ â†’ ãƒãƒ£ãƒ³ã‚¯åŒ– â†’ ãƒ™ã‚¯ãƒˆãƒ«åŒ– ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
manager = CorpusManager.create_simple_rag(doc_store, vector_store)

# ğŸ“„ STEP 3: Process documents and build corpus
#    ã‚¹ãƒ†ãƒƒãƒ—3: æ–‡æ›¸ã‚’å‡¦ç†ã—ã¦ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
stats = manager.build_corpus(["documents/"])

# ğŸ“Š STEP 4: Review processing results
#    ã‚¹ãƒ†ãƒƒãƒ—4: å‡¦ç†çµæœã®ç¢ºèª
print(f"âœ… Processed {stats.total_files_processed} files")
print(f"âœ… Created {stats.total_chunks_created} chunks")
```

**What happened behind the scenes? / å†…éƒ¨ã§ä½•ãŒèµ·ã“ã£ãŸã®ã‹ï¼Ÿ**
1. **File loading** - Documents were parsed and converted to internal format / **ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿** - æ–‡æ›¸ãŒè§£æã•ã‚Œå†…éƒ¨å½¢å¼ã«å¤‰æ›
2. **Chunking** - Large texts were split into optimal-sized pieces / **ãƒãƒ£ãƒ³ã‚¯åŒ–** - å¤§ããªãƒ†ã‚­ã‚¹ãƒˆãŒæœ€é©ãªã‚µã‚¤ã‚ºã«åˆ†å‰²
3. **Vectorization** - Each chunk was converted to semantic embeddings / **ãƒ™ã‚¯ãƒˆãƒ«åŒ–** - å„ãƒãƒ£ãƒ³ã‚¯ãŒã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›
4. **Storage** - Results were saved to the database and vector store / **ä¿å­˜** - çµæœãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ä¿å­˜

## Implementation Examples / å®Ÿè£…ä¾‹

Now let's see how each approach translates the concepts into working code:

å„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒæ¦‚å¿µã‚’ã©ã®ã‚ˆã†ã«å‹•ä½œã™ã‚‹ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›ã™ã‚‹ã‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

### 1.1 Simple RAG Implementation / Simple RAGå®Ÿè£…
**Concept**: Quick prototyping with minimal processing / **æ¦‚å¿µ**: æœ€å°å‡¦ç†ã§ã®ã‚¯ã‚¤ãƒƒã‚¯ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°

```python
# ğŸ¯ CONCEPT MAPPING: Simple RAG = Load â†’ Chunk â†’ Vector
#    æ¦‚å¿µãƒãƒƒãƒ”ãƒ³ã‚°: Simple RAG = èª­ã¿è¾¼ã¿ â†’ ãƒãƒ£ãƒ³ã‚¯åŒ– â†’ ãƒ™ã‚¯ãƒˆãƒ«åŒ–

simple_manager = CorpusManager.create_simple_rag(doc_store, vector_store)
simple_stats = simple_manager.build_corpus(file_paths)

print(f"Simple RAG Results:")
print(f"- Files: {simple_stats.total_files_processed}")      # ğŸ“„ Load stage output / èª­ã¿è¾¼ã¿ã‚¹ãƒ†ãƒ¼ã‚¸å‡ºåŠ›
print(f"- Chunks: {simple_stats.total_chunks_created}")      # âœ‚ï¸ Chunk stage output / ãƒãƒ£ãƒ³ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¸å‡ºåŠ›  
print(f"- Time: {simple_stats.total_processing_time:.3f}s")  # â±ï¸ Processing efficiency / å‡¦ç†åŠ¹ç‡
```

**When to use this implementation / ã“ã®å®Ÿè£…ã‚’ä½¿ç”¨ã™ã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°:**
- âœ… Testing new document collections / æ–°ã—ã„æ–‡æ›¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆ
- âœ… Rapid prototyping / é«˜é€Ÿãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°
- âœ… Simple content without domain-specific terminology / å°‚é–€ç”¨èªã®ãªã„ã‚·ãƒ³ãƒ—ãƒ«ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„

### 1.2 Semantic RAG Implementation / Semantic RAGå®Ÿè£…
**Concept**: Enhanced understanding through terminology normalization / **æ¦‚å¿µ**: ç”¨èªæ­£è¦åŒ–ã«ã‚ˆã‚‹ç†è§£åŠ›å‘ä¸Š

```python
# ğŸ¯ CONCEPT MAPPING: Semantic RAG = Load â†’ Dictionary â†’ Normalize â†’ Chunk â†’ Vector
#    æ¦‚å¿µãƒãƒƒãƒ”ãƒ³ã‚°: Semantic RAG = èª­ã¿è¾¼ã¿ â†’ è¾æ›¸ â†’ æ­£è¦åŒ– â†’ ãƒãƒ£ãƒ³ã‚¯åŒ– â†’ ãƒ™ã‚¯ãƒˆãƒ«åŒ–

semantic_manager = CorpusManager.create_semantic_rag(doc_store, vector_store)
semantic_stats = semantic_manager.build_corpus(file_paths)

print(f"Semantic RAG Results:")
print(f"- Files: {semantic_stats.total_files_processed}")           # ğŸ“„ Load stage output
print(f"- Dictionary terms: {semantic_stats.dictionary_terms}")     # ğŸ“š Dictionary stage output  
print(f"- Normalized chunks: {semantic_stats.total_chunks_created}") # ğŸ”„ Normalize + Chunk output
print(f"- Better semantic consistency achieved")                     # ğŸ¯ Core benefit
```

**What the additional processing achieves / è¿½åŠ å‡¦ç†ã§é”æˆã•ã‚Œã‚‹ã“ã¨:**
1. **Dictionary creation** - Domain-specific terms are automatically extracted / **è¾æ›¸ä½œæˆ** - å°‚é–€åˆ†é‡ã®ç”¨èªãŒè‡ªå‹•æŠ½å‡º
2. **Normalization** - Terminology variations are unified (e.g., "AI" = "Artificial Intelligence") / **æ­£è¦åŒ–** - ç”¨èªã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒçµ±ä¸€ï¼ˆä¾‹: "AI" = "Artificial Intelligence"ï¼‰
3. **Enhanced search** - Better matching between queries and content / **æ¤œç´¢å‘ä¸Š** - ã‚¯ã‚¨ãƒªã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é–“ã®ãƒãƒƒãƒãƒ³ã‚°å‘ä¸Š

**When to use this implementation / ã“ã®å®Ÿè£…ã‚’ä½¿ç”¨ã™ã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°:**
- âœ… Medical, legal, or technical documents / åŒ»ç™‚ãƒ»æ³•å¾‹ãƒ»æŠ€è¡“æ–‡æ›¸
- âœ… Content with many abbreviations / ç•¥èªãŒå¤šã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
- âœ… Multi-language or mixed terminology / å¤šè¨€èªã‚„æ··åˆç”¨èª

### 1.3 Knowledge RAG Implementation / Knowledge RAGå®Ÿè£…
**Concept**: Advanced analytics with relationship discovery / **æ¦‚å¿µ**: é–¢ä¿‚ç™ºè¦‹ã«ã‚ˆã‚‹é«˜åº¦ãªåˆ†æ

```python
# ğŸ¯ CONCEPT MAPPING: Knowledge RAG = Load â†’ Dictionary â†’ Graph â†’ Normalize â†’ Chunk â†’ Vector
#    æ¦‚å¿µãƒãƒƒãƒ”ãƒ³ã‚°: Knowledge RAG = èª­ã¿è¾¼ã¿ â†’ è¾æ›¸ â†’ ã‚°ãƒ©ãƒ• â†’ æ­£è¦åŒ– â†’ ãƒãƒ£ãƒ³ã‚¯åŒ– â†’ ãƒ™ã‚¯ãƒˆãƒ«åŒ–

knowledge_manager = CorpusManager.create_knowledge_rag(doc_store, vector_store)
knowledge_stats = knowledge_manager.build_corpus(file_paths)

print(f"Knowledge RAG Results:")
print(f"- Files: {knowledge_stats.total_files_processed}")          # ğŸ“„ Load stage output
print(f"- Dictionary terms: {knowledge_stats.dictionary_terms}")    # ğŸ“š Dictionary stage output
print(f"- Relationships: {knowledge_stats.graph_relationships}")    # ğŸ•¸ï¸ Graph stage output
print(f"- Knowledge chunks: {knowledge_stats.total_chunks_created}") # ğŸ§  Final enriched chunks
```

**What the complete pipeline achieves / å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§é”æˆã•ã‚Œã‚‹ã“ã¨:**
1. **Relationship extraction** - Entities and their connections are identified / **é–¢ä¿‚æŠ½å‡º** - ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ãã®æ¥ç¶šãŒç‰¹å®š
2. **Knowledge graph** - Structured representation of domain knowledge / **çŸ¥è­˜ã‚°ãƒ©ãƒ•** - ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã®æ§‹é€ åŒ–è¡¨ç¾
3. **Enhanced reasoning** - Better understanding of context and implications / **æ¨è«–å‘ä¸Š** - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨å«æ„ã®ç†è§£å‘ä¸Š

**When to use this implementation / ã“ã®å®Ÿè£…ã‚’ä½¿ç”¨ã™ã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°:**
- âœ… Research documents with complex relationships / è¤‡é›‘ãªé–¢ä¿‚æ€§ã‚’æŒã¤ç ”ç©¶æ–‡æ›¸
- âœ… Business processes and workflows / ãƒ“ã‚¸ãƒã‚¹ãƒ—ãƒ­ã‚»ã‚¹ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
- âœ… Educational content requiring deep understanding / æ·±ã„ç†è§£ãŒå¿…è¦ãªæ•™è‚²ã‚³ãƒ³ãƒ†ãƒ³ãƒ„

## 2. Stage Selection Approach / ã‚¹ãƒ†ãƒ¼ã‚¸é¸æŠã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

### 2.1 Custom Stage Selection
Select specific processing stages / ç‰¹å®šã®å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’é¸æŠ

```python
from refinire_rag.processing.dictionary_maker import DictionaryMakerConfig
from refinire_rag.processing.chunker import ChunkingConfig
from refinire_rag.loader.loader import LoaderConfig

# Configure individual stages
stage_configs = {
    "loader_config": LoaderConfig(),
    "dictionary_config": DictionaryMakerConfig(
        dictionary_file_path="custom_dictionary.md",
        focus_on_technical_terms=True,
        extract_abbreviations=True
    ),
    "chunker_config": ChunkingConfig(
        chunk_size=256,
        overlap=32,
        split_by_sentence=True
    )
}

# Execute selected stages only
corpus_manager = CorpusManager(doc_store, vector_store)
selected_stages = ["load", "dictionary", "chunk", "vector"]

stage_stats = corpus_manager.build_corpus(
    file_paths=file_paths,
    stages=selected_stages,
    stage_configs=stage_configs
)

print(f"Selected stages: {selected_stages}")
print(f"Generated dictionary with technical terms")
```

### 2.2 Available Processing Stages / åˆ©ç”¨å¯èƒ½ãªå‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¸

| Stage | Purpose | Output |
|-------|---------|--------|
| `load` | File loading and conversion | Documents in DocumentStore |
| `dictionary` | Domain-specific term extraction | Dictionary file (.md) |
| `graph` | Relationship extraction | Knowledge graph file (.md) |
| `normalize` | Expression normalization | Normalized documents |
| `chunk` | Text segmentation | Document chunks |
| `vector` | Embedding generation | Vector embeddings |

## 3. Custom Pipeline Approach / ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

### 3.1 Multi-Stage Custom Pipeline
Create sophisticated processing workflows / é«˜åº¦ãªå‡¦ç†ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ä½œæˆ

```python
from refinire_rag.processing.document_pipeline import DocumentPipeline
from refinire_rag.processing.document_store_processor import DocumentStoreProcessor
from refinire_rag.loader.text_loader import TextLoader
from refinire_rag.processing.dictionary_maker import DictionaryMaker
from refinire_rag.processing.normalizer import Normalizer
from refinire_rag.processing.chunker import Chunker

# Define custom pipelines
custom_pipelines = [
    # Stage 1: Load and store original documents
    DocumentPipeline([
        TextLoader(LoaderConfig()),
        DocumentStoreProcessor(doc_store)
    ]),
    
    # Stage 2: Extract dictionary from originals
    DocumentPipeline([
        DocumentStoreLoader(doc_store, config=DocumentStoreLoaderConfig(
            processing_stage="original"
        )),
        DictionaryMaker(DictionaryMakerConfig(
            dictionary_file_path="pipeline_dictionary.md"
        ))
    ]),
    
    # Stage 3: Normalize and store
    DocumentPipeline([
        DocumentStoreLoader(doc_store, config=DocumentStoreLoaderConfig(
            processing_stage="original"
        )),
        Normalizer(NormalizerConfig(
            dictionary_file_path="pipeline_dictionary.md"
        )),
        DocumentStoreProcessor(doc_store)
    ]),
    
    # Stage 4: Chunk normalized documents
    DocumentPipeline([
        DocumentStoreLoader(doc_store, config=DocumentStoreLoaderConfig(
            processing_stage="normalized"
        )),
        Chunker(ChunkingConfig(chunk_size=128, overlap=16))
    ])
]

# Execute custom pipelines
pipeline_stats = corpus_manager.build_corpus(
    file_paths=file_paths,
    custom_pipelines=custom_pipelines
)
```

## 4. File Format Support / ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚µãƒãƒ¼ãƒˆ

### 4.1 Supported File Types / ã‚µãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼

```python
# Text files
text_files = ["document.txt", "readme.md"]

# CSV files
csv_files = ["data.csv", "records.csv"]

# JSON files
json_files = ["config.json", "data.json"]

# HTML files
html_files = ["webpage.html", "documentation.html"]

# All formats in one corpus
all_files = text_files + csv_files + json_files + html_files
stats = manager.build_corpus(all_files)
```

### 4.2 Directory Processing / ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‡¦ç†

```python
# Process entire directories
directory_paths = [
    "documents/",
    "knowledge_base/",
    "technical_docs/"
]

# Incremental loading (only changed files)
from refinire_rag.loader.incremental_directory_loader import IncrementalDirectoryLoader
from refinire_rag.loader.file_tracker import FileTracker

tracker = FileTracker("file_tracking.json")
incremental_loader = IncrementalDirectoryLoader(tracker=tracker)

# Only process new/modified files
incremental_stats = manager.build_corpus(
    file_paths=directory_paths,
    use_incremental=True
)
```

## 5. Advanced Configuration / é«˜åº¦ãªè¨­å®š

### 5.1 Dictionary Maker Configuration / è¾æ›¸ä½œæˆè¨­å®š

```python
dictionary_config = DictionaryMakerConfig(
    dictionary_file_path="domain_dictionary.md",
    focus_on_technical_terms=True,
    extract_abbreviations=True,
    include_definitions=True,
    min_term_frequency=2,
    max_terms_per_document=50,
    llm_model="gpt-4o-mini"
)
```

### 5.2 Chunking Configuration / ãƒãƒ£ãƒ³ã‚¯è¨­å®š

```python
chunking_config = ChunkingConfig(
    chunk_size=512,           # Characters per chunk
    overlap=50,               # Overlap between chunks
    split_by_sentence=True,   # Preserve sentence boundaries
    min_chunk_size=100,       # Minimum chunk size
    separators=["\n\n", "\n", ".", "!", "?"]  # Splitting separators
)
```

### 5.3 Normalization Configuration / æ­£è¦åŒ–è¨­å®š

```python
normalizer_config = NormalizerConfig(
    dictionary_file_path="domain_dictionary.md",
    case_sensitive=False,
    preserve_formatting=True,
    expand_abbreviations=True,
    normalize_numbers=True
)
```

## 6. Monitoring and Statistics / ç›£è¦–ã¨çµ±è¨ˆ

### 6.1 Processing Statistics / å‡¦ç†çµ±è¨ˆ

```python
# Get detailed statistics
stats = manager.build_corpus(file_paths)

print(f"Processing Summary:")
print(f"- Total files processed: {stats.total_files_processed}")
print(f"- Documents created: {stats.total_documents_created}")
print(f"- Chunks created: {stats.total_chunks_created}")
print(f"- Processing time: {stats.total_processing_time:.3f}s")
print(f"- Pipeline stages: {stats.pipeline_stages_executed}")
print(f"- Documents by stage: {stats.documents_by_stage}")
print(f"- Errors encountered: {len(stats.errors_encountered)}")
```

### 6.2 Error Handling / ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
try:
    stats = manager.build_corpus(file_paths)
    
    if stats.errors_encountered:
        print(f"Encountered {len(stats.errors_encountered)} errors:")
        for error in stats.errors_encountered:
            print(f"- {error['file']}: {error['message']}")
    
except Exception as e:
    print(f"Corpus building failed: {e}")
    # Handle specific error types
    if "FileNotFoundError" in str(e):
        print("Check file paths and permissions")
    elif "LLMError" in str(e):
        print("Check LLM configuration and API keys")
```

## 7. Best Practices / ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 7.1 Performance Optimization / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

```python
# For large corpora, use incremental loading
manager = CorpusManager(doc_store, vector_store)
stats = manager.build_corpus(
    file_paths=large_directory_paths,
    use_incremental=True,
    batch_size=100  # Process in batches
)

# Use appropriate chunk sizes
# Small chunks (128-256): Better for precise retrieval
# Large chunks (512-1024): Better for context preservation
```

### 7.2 Storage Considerations / ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è€ƒæ…®äº‹é …

```python
# For production: Use persistent storage
doc_store = SQLiteDocumentStore("production_documents.db")

# For development: Use in-memory storage
doc_store = SQLiteDocumentStore(":memory:")

# For departments: Separate databases
hr_store = SQLiteDocumentStore("data/hr_documents.db")
sales_store = SQLiteDocumentStore("data/sales_documents.db")
```

### 7.3 Quality Assurance / å“è³ªä¿è¨¼

```python
# Validate corpus after building
if stats.total_chunks_created == 0:
    print("Warning: No chunks created. Check file formats and content.")

if stats.errors_encountered:
    print(f"Warning: {len(stats.errors_encountered)} files failed processing")

# Check generated artifacts
import os
if os.path.exists("domain_dictionary.md"):
    print("âœ“ Dictionary successfully generated")
if os.path.exists("knowledge_graph.md"):
    print("âœ“ Knowledge graph successfully generated")
```

## 8. Complete Example / å®Œå…¨ãªä¾‹

```python
#!/usr/bin/env python3
"""
Complete corpus creation example
å®Œå…¨ãªã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆä¾‹
"""

from pathlib import Path
from refinire_rag.application.corpus_manager_new import CorpusManager
from refinire_rag.storage.sqlite_store import SQLiteDocumentStore
from refinire_rag.storage.in_memory_vector_store import InMemoryVectorStore

def main():
    # Setup / ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    doc_store = SQLiteDocumentStore("my_corpus.db")
    vector_store = InMemoryVectorStore()
    
    # Create sample documents / ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ä½œæˆ
    sample_dir = Path("sample_docs")
    sample_dir.mkdir(exist_ok=True)
    
    (sample_dir / "ai_overview.txt").write_text("""
    Artificial Intelligence (AI) is the simulation of human intelligence 
    in machines programmed to think and learn like humans. Key applications 
    include machine learning, natural language processing, and computer vision.
    """)
    
    (sample_dir / "machine_learning.txt").write_text("""
    Machine Learning (ML) is a subset of AI that enables computers to learn 
    without explicit programming. Popular algorithms include neural networks, 
    decision trees, and support vector machines.
    """)
    
    # Build corpus with semantic RAG / ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯RAGã§ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰
    print("Building corpus with semantic RAG...")
    manager = CorpusManager.create_semantic_rag(doc_store, vector_store)
    
    stats = manager.build_corpus([str(sample_dir)])
    
    # Results / çµæœ
    print(f"\nâœ… Corpus Creation Complete:")
    print(f"   Files processed: {stats.total_files_processed}")
    print(f"   Documents created: {stats.total_documents_created}")
    print(f"   Chunks created: {stats.total_chunks_created}")
    print(f"   Processing time: {stats.total_processing_time:.3f}s")
    
    # Validate results / çµæœæ¤œè¨¼
    total_docs = doc_store.count_documents()
    total_vectors = vector_store.count()
    
    print(f"\nğŸ“Š Storage Validation:")
    print(f"   Documents in store: {total_docs}")
    print(f"   Vectors in store: {total_vectors}")
    
    return True

if __name__ == "__main__":
    success = main()
    print("\nğŸ‰ Tutorial completed successfully!" if success else "\nâŒ Tutorial failed")
```

## Next Steps / æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

After creating your corpus, proceed to:
ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆå¾Œã€æ¬¡ã«é€²ã‚€ï¼š

1. **Part 2: Query Engine** - Learn to search and retrieve from your corpus
   **Part 2: Query Engine** - ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰ã®æ¤œç´¢ã¨å–å¾—ã‚’å­¦ç¿’
2. **Part 3: Evaluation** - Evaluate your RAG system performance
   **Part 3: Evaluation** - RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡

## Resources / ãƒªã‚½ãƒ¼ã‚¹

- [CorpusManager API Documentation](../api/corpus_manager.md)
- [Processing Configuration Reference](../development/processor_config_example.md)
- [Example Scripts](../../examples/)