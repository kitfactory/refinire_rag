# Part 2: QueryEngineæ¤œç´¢ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«

## Overview / æ¦‚è¦

This tutorial demonstrates how to use refinire-rag's QueryEngine for intelligent document search and answer generation. The QueryEngine integrates retrieval, reranking, and answer synthesis to provide accurate, contextual responses from your corpus.

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€refinire-ragã®QueryEngineã‚’ä½¿ç”¨ã—ãŸã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªæ–‡æ›¸æ¤œç´¢ã¨å›ç­”ç”Ÿæˆã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚QueryEngineã¯ã€æ¤œç´¢ã€å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€å›ç­”åˆæˆã‚’çµ±åˆã—ã¦ã€ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰æ­£ç¢ºã§æ–‡è„ˆã«æ²¿ã£ãŸå›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚

## Learning Objectives / å­¦ç¿’ç›®æ¨™

- Configure and initialize QueryEngine components / QueryEngineã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®è¨­å®šã¨åˆæœŸåŒ–
- Understand retrieval and reranking mechanisms / æ¤œç´¢ã¨å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®ç†è§£
- Master query normalization and processing / ã‚¯ã‚¨ãƒªæ­£è¦åŒ–ã¨å‡¦ç†ã®ãƒã‚¹ã‚¿ãƒ¼
- Implement custom answer synthesis / ã‚«ã‚¹ã‚¿ãƒ å›ç­”åˆæˆã®å®Ÿè£…
- Monitor and optimize query performance / ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ç›£è¦–ã¨æœ€é©åŒ–

## Prerequisites / å‰ææ¡ä»¶

```bash
# Complete Part 1: Corpus Creation first
# Part 1: ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆã‚’å…ˆã«å®Œäº†

# Set environment variables for LLM integration
export OPENAI_API_KEY="your-api-key"
export REFINIRE_RAG_LLM_MODEL="gpt-4o-mini"
export REFINIRE_RAG_EMBEDDING_MODEL="text-embedding-3-small"
```

## Quick Start Example / ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆä¾‹

```python
from refinire_rag.application.query_engine import QueryEngine
from refinire_rag.retrieval import SimpleRetriever, SimpleReranker, SimpleReader

# Initialize QueryEngine with existing corpus
query_engine = QueryEngine(
    document_store=doc_store,
    vector_store=vector_store,
    retriever=SimpleRetriever(vector_store),
    reranker=SimpleReranker(),
    reader=SimpleReader()
)

# Simple query
result = query_engine.answer("What is artificial intelligence?")
print(f"Answer: {result.answer}")
print(f"Sources: {len(result.sources)} documents")
print(f"Confidence: {result.confidence:.2f}")
```

## 1. QueryEngine Architecture / QueryEngine ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### 1.1 Core Components / ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

```python
from refinire_rag.application.query_engine import QueryEngine, QueryEngineConfig
from refinire_rag.retrieval import (
    SimpleRetriever, SimpleRetrieverConfig,
    SimpleReranker, SimpleRerankerConfig, 
    SimpleReader, SimpleReaderConfig
)

# Component configuration / ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè¨­å®š
retriever_config = SimpleRetrieverConfig(
    top_k=10,                        # Number of initial candidates
    similarity_threshold=0.1,        # Minimum similarity score
    embedding_model="text-embedding-3-small"
)

reranker_config = SimpleRerankerConfig(
    top_k=5,                         # Final number of results
    boost_exact_matches=True,        # Boost exact keyword matches
    length_penalty_factor=0.1        # Penalty for very long/short docs
)

reader_config = SimpleReaderConfig(
    llm_model="gpt-4o-mini",         # Language model for synthesis
    max_context_length=2000,         # Maximum context tokens
    temperature=0.2,                 # Generation temperature
    include_sources=True             # Include source citations
)

# Create components / ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆä½œæˆ
retriever = SimpleRetriever(vector_store, config=retriever_config)
reranker = SimpleReranker(config=reranker_config)
reader = SimpleReader(config=reader_config)
```

### 1.2 QueryEngine Configuration / QueryEngineè¨­å®š

```python
# Engine-level configuration / ã‚¨ãƒ³ã‚¸ãƒ³ãƒ¬ãƒ™ãƒ«è¨­å®š
engine_config = QueryEngineConfig(
    enable_query_normalization=True,    # Normalize queries using dictionary
    auto_detect_corpus_state=True,      # Auto-detect corpus capabilities
    retriever_top_k=10,                 # Override retriever top_k
    reranker_top_k=5,                   # Override reranker top_k
    include_sources=True,                # Include source documents
    include_confidence=True,             # Calculate confidence scores
    max_response_time=30.0,             # Maximum response time (seconds)
    enable_caching=True                  # Enable response caching
)

# Initialize QueryEngine / QueryEngineåˆæœŸåŒ–
query_engine = QueryEngine(
    document_store=doc_store,
    vector_store=vector_store,
    retriever=retriever,
    reranker=reranker,
    reader=reader,
    config=engine_config
)
```

## 2. Basic Query Operations / åŸºæœ¬ã‚¯ã‚¨ãƒªæ“ä½œ

### 2.1 Simple Queries / ã‚·ãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒª

```python
# Direct question answering / ç›´æ¥çš„ãªè³ªå•å¿œç­”
result = query_engine.answer("What is machine learning?")

print(f"Question / è³ªå•: What is machine learning?")
print(f"Answer / å›ç­”: {result.answer}")
print(f"Confidence / ä¿¡é ¼åº¦: {result.confidence:.3f}")
print(f"Processing time / å‡¦ç†æ™‚é–“: {result.metadata['processing_time']:.3f}s")

# Access source documents / ã‚½ãƒ¼ã‚¹æ–‡æ›¸ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹
for i, source in enumerate(result.sources):
    print(f"Source {i+1}: {source.metadata.get('title', 'Untitled')}")
    print(f"  Content preview: {source.content[:100]}...")
    print(f"  Relevance score: {source.metadata.get('relevance_score', 'N/A')}")
```

### 2.2 Queries with Context / æ–‡è„ˆä»˜ãã‚¯ã‚¨ãƒª

```python
# Multi-turn conversation / è¤‡æ•°ã‚¿ãƒ¼ãƒ³ä¼šè©±
context = {
    "previous_questions": [
        "What is artificial intelligence?",
        "What are the main types of AI?"
    ],
    "user_preferences": {
        "detail_level": "intermediate",
        "include_examples": True
    }
}

result = query_engine.answer(
    query="How does deep learning differ from traditional ML?",
    context=context
)

print(f"Contextual answer: {result.answer}")
```

### 2.3 Filtered Queries / ãƒ•ã‚£ãƒ«ã‚¿ä»˜ãã‚¯ã‚¨ãƒª

```python
# Search with metadata filters / ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ä»˜ãæ¤œç´¢
filters = {
    "category": "AI",
    "difficulty": ["beginner", "intermediate"],
    "source": "Technical Documentation"
}

result = query_engine.answer(
    query="Explain neural networks",
    filters=filters
)

print(f"Filtered result from {len(result.sources)} matching documents")
```

## 3. Advanced Query Features / é«˜åº¦ãªã‚¯ã‚¨ãƒªæ©Ÿèƒ½

### 3.1 Query Normalization / ã‚¯ã‚¨ãƒªæ­£è¦åŒ–

```python
# Enable automatic query normalization / è‡ªå‹•ã‚¯ã‚¨ãƒªæ­£è¦åŒ–ã‚’æœ‰åŠ¹åŒ–
# Requires dictionary generated in Part 1
# Part 1ã§ç”Ÿæˆã•ã‚ŒãŸè¾æ›¸ãŒå¿…è¦

# Query with technical terms / æŠ€è¡“ç”¨èªã‚’å«ã‚€ã‚¯ã‚¨ãƒª
original_query = "How does ML work?"
result = query_engine.answer(original_query)

# Check if query was normalized / ã‚¯ã‚¨ãƒªãŒæ­£è¦åŒ–ã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
if result.metadata.get('query_normalized'):
    normalized_query = result.metadata.get('normalized_query')
    print(f"Original: {original_query}")
    print(f"Normalized: {normalized_query}")
    print(f"Normalization improved search accuracy")
```

### 3.2 Multi-Retriever Integration / ãƒãƒ«ãƒæ¤œç´¢çµ±åˆ

```python
from refinire_rag.retrieval import HybridRetriever
from refinire_rag.keywordstore import TfidfKeywordStore

# Setup hybrid retrieval (vector + keyword)
# ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®è¨­å®šï¼ˆãƒ™ã‚¯ãƒˆãƒ« + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
keyword_store = TfidfKeywordStore()
hybrid_retriever = HybridRetriever(
    vector_store=vector_store,
    keyword_store=keyword_store,
    vector_weight=0.7,     # 70% weight for vector search
    keyword_weight=0.3     # 30% weight for keyword search
)

# QueryEngine with hybrid retrieval
hybrid_query_engine = QueryEngine(
    document_store=doc_store,
    vector_store=vector_store,
    retriever=hybrid_retriever,
    reranker=reranker,
    reader=reader
)

result = hybrid_query_engine.answer("machine learning algorithms")
print(f"Hybrid search found {len(result.sources)} relevant documents")
```

### 3.3 Custom Answer Instructions / ã‚«ã‚¹ã‚¿ãƒ å›ç­”æŒ‡ç¤º

```python
# Custom instructions for domain-specific responses
# ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–å›ç­”ã®ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ æŒ‡ç¤º
custom_reader_config = SimpleReaderConfig(
    llm_model="gpt-4o-mini",
    generation_instructions="""
    You are an AI technology expert. When answering questions:
    1. Provide clear, technical explanations
    2. Include practical examples and use cases
    3. Explain complex concepts in simple terms
    4. Always cite specific sources from the provided context
    5. If uncertain, clearly state limitations
    
    Format your response as:
    - Direct answer (2-3 sentences)
    - Technical details (if applicable)
    - Practical examples
    - Related concepts
    """,
    temperature=0.1,
    max_tokens=500
)

custom_reader = SimpleReader(config=custom_reader_config)
custom_engine = QueryEngine(
    document_store=doc_store,
    vector_store=vector_store,
    retriever=retriever,
    reranker=reranker,
    reader=custom_reader
)

result = custom_engine.answer("What are the applications of computer vision?")
```

## 4. Query Result Analysis / ã‚¯ã‚¨ãƒªçµæœåˆ†æ

### 4.1 Detailed Result Inspection / è©³ç´°çµæœæ¤œæŸ»

```python
def analyze_query_result(result):
    """
    Analyze and display detailed query results
    è©³ç´°ãªã‚¯ã‚¨ãƒªçµæœã‚’åˆ†æãƒ»è¡¨ç¤º
    """
    
    print("="*60)
    print("QUERY RESULT ANALYSIS / ã‚¯ã‚¨ãƒªçµæœåˆ†æ")
    print("="*60)
    
    # Basic metrics / åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    print(f"\nğŸ“Š Basic Metrics / åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
    print(f"   Answer length: {len(result.answer)} characters")
    print(f"   Confidence score: {result.confidence:.3f}")
    print(f"   Source count: {len(result.sources)}")
    print(f"   Processing time: {result.metadata.get('processing_time', 0):.3f}s")
    
    # Query processing details / ã‚¯ã‚¨ãƒªå‡¦ç†è©³ç´°
    print(f"\nğŸ” Query Processing / ã‚¯ã‚¨ãƒªå‡¦ç†:")
    print(f"   Original query: {result.metadata.get('original_query', 'N/A')}")
    print(f"   Normalized: {result.metadata.get('query_normalized', False)}")
    if result.metadata.get('normalized_query'):
        print(f"   Normalized query: {result.metadata['normalized_query']}")
    
    # Retrieval analysis / æ¤œç´¢åˆ†æ
    print(f"\nğŸ“š Retrieval Analysis / æ¤œç´¢åˆ†æ:")
    print(f"   Retrieved documents: {result.metadata.get('retrieved_count', 0)}")
    print(f"   Reranked documents: {result.metadata.get('reranked_count', 0)}")
    print(f"   Final sources used: {len(result.sources)}")
    
    # Source quality / ã‚½ãƒ¼ã‚¹å“è³ª
    print(f"\nğŸ¯ Source Quality / ã‚½ãƒ¼ã‚¹å“è³ª:")
    for i, source in enumerate(result.sources[:3]):  # Top 3 sources
        relevance = source.metadata.get('relevance_score', 0)
        title = source.metadata.get('title', f'Document {i+1}')
        print(f"   {i+1}. {title}: relevance={relevance:.3f}")
    
    # Answer quality indicators / å›ç­”å“è³ªæŒ‡æ¨™
    print(f"\nâœ… Quality Indicators / å“è³ªæŒ‡æ¨™:")
    answer_length = len(result.answer.split())
    print(f"   Answer word count: {answer_length}")
    print(f"   Has sources: {'Yes' if result.sources else 'No'}")
    print(f"   Confidence level: {'High' if result.confidence > 0.8 else 'Medium' if result.confidence > 0.5 else 'Low'}")

# Usage example / ä½¿ç”¨ä¾‹
result = query_engine.answer("Explain the difference between AI and ML")
analyze_query_result(result)
```

### 4.2 Performance Monitoring / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

```python
def monitor_query_performance(query_engine, test_queries):
    """
    Monitor QueryEngine performance across multiple queries
    è¤‡æ•°ã‚¯ã‚¨ãƒªã§QueryEngineã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç›£è¦–
    """
    
    print("ğŸš€ PERFORMANCE MONITORING / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–")
    print("="*60)
    
    results = []
    total_time = 0
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ“ Query {i}/{len(test_queries)}: {query}")
        
        import time
        start_time = time.time()
        result = query_engine.answer(query)
        end_time = time.time()
        
        query_time = end_time - start_time
        total_time += query_time
        
        results.append({
            'query': query,
            'time': query_time,
            'confidence': result.confidence,
            'source_count': len(result.sources),
            'answer_length': len(result.answer)
        })
        
        print(f"   â±ï¸  Time: {query_time:.3f}s")
        print(f"   ğŸ¯ Confidence: {result.confidence:.3f}")
        print(f"   ğŸ“š Sources: {len(result.sources)}")
    
    # Summary statistics / è¦ç´„çµ±è¨ˆ
    print(f"\nğŸ“Š Summary Statistics / è¦ç´„çµ±è¨ˆ:")
    avg_time = total_time / len(test_queries)
    avg_confidence = sum(r['confidence'] for r in results) / len(results)
    avg_sources = sum(r['source_count'] for r in results) / len(results)
    
    print(f"   Average response time: {avg_time:.3f}s")
    print(f"   Average confidence: {avg_confidence:.3f}")
    print(f"   Average sources per query: {avg_sources:.1f}")
    print(f"   Total queries processed: {len(test_queries)}")
    print(f"   Queries per second: {len(test_queries)/total_time:.2f}")

# Test queries for monitoring / ç›£è¦–ç”¨ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
test_queries = [
    "What is artificial intelligence?",
    "How does machine learning work?", 
    "What are the applications of deep learning?",
    "Explain neural networks",
    "What is the difference between supervised and unsupervised learning?"
]

monitor_query_performance(query_engine, test_queries)
```

## 5. QueryEngine Optimization / QueryEngineæœ€é©åŒ–

### 5.1 Component Configuration Tuning / ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè¨­å®šèª¿æ•´

```python
# Performance-optimized configuration / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–è¨­å®š
fast_config = {
    "retriever": SimpleRetrieverConfig(
        top_k=5,                    # Fewer candidates for speed
        similarity_threshold=0.2,   # Higher threshold
        embedding_model="text-embedding-3-small"
    ),
    "reranker": SimpleRerankerConfig(
        top_k=3,                    # Fewer final results
        boost_exact_matches=True
    ),
    "reader": SimpleReaderConfig(
        llm_model="gpt-4o-mini",    # Faster model
        max_context_length=1000,    # Shorter context
        temperature=0.1             # Lower temperature for consistency
    )
}

# Accuracy-optimized configuration / ç²¾åº¦æœ€é©åŒ–è¨­å®š
accurate_config = {
    "retriever": SimpleRetrieverConfig(
        top_k=20,                   # More candidates for accuracy
        similarity_threshold=0.05,  # Lower threshold for recall
        embedding_model="text-embedding-3-large"
    ),
    "reranker": SimpleRerankerConfig(
        top_k=8,                    # More final results
        boost_exact_matches=True,
        length_penalty_factor=0.05  # Less aggressive length penalty
    ),
    "reader": SimpleReaderConfig(
        llm_model="gpt-4",          # More capable model
        max_context_length=3000,    # Longer context
        temperature=0.2             # Balanced temperature
    )
}
```

### 5.2 Caching and Optimization / ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨æœ€é©åŒ–

```python
from refinire_rag.application.query_engine import QueryEngineConfig

# Enable advanced caching / é«˜åº¦ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹åŒ–
optimized_config = QueryEngineConfig(
    enable_caching=True,
    cache_ttl=3600,                # Cache for 1 hour
    cache_max_size=1000,           # Max cache entries
    enable_query_normalization=True,
    similarity_cache_threshold=0.95, # Cache similar queries
    max_response_time=10.0         # Response timeout
)

optimized_engine = QueryEngine(
    document_store=doc_store,
    vector_store=vector_store,
    retriever=retriever,
    reranker=reranker,
    reader=reader,
    config=optimized_config
)

# Warm up cache with common queries / ã‚ˆãã‚ã‚‹ã‚¯ã‚¨ãƒªã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
common_queries = [
    "What is AI?",
    "Machine learning basics",
    "Deep learning overview"
]

for query in common_queries:
    optimized_engine.answer(query)
    
print("Cache warmed up with common queries")
```

## 6. Error Handling and Debugging / ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ‡ãƒãƒƒã‚°

### 6.1 Robust Query Processing / å …ç‰¢ãªã‚¯ã‚¨ãƒªå‡¦ç†

```python
def robust_query(query_engine, query, max_retries=3):
    """
    Robust query processing with error handling and retries
    ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒªãƒˆãƒ©ã‚¤ã‚’å«ã‚€å …ç‰¢ãªã‚¯ã‚¨ãƒªå‡¦ç†
    """
    
    for attempt in range(max_retries):
        try:
            print(f"ğŸ”„ Attempt {attempt + 1}: {query}")
            
            result = query_engine.answer(query)
            
            # Validate result quality / çµæœå“è³ªã‚’æ¤œè¨¼
            if len(result.answer) < 10:
                raise ValueError("Answer too short")
            
            if result.confidence < 0.1:
                raise ValueError("Confidence too low")
            
            if not result.sources:
                raise ValueError("No sources found")
            
            print(f"âœ… Success on attempt {attempt + 1}")
            return result
            
        except Exception as e:
            print(f"âŒ Attempt {attempt + 1} failed: {e}")
            
            if attempt == max_retries - 1:
                print(f"ğŸš« All {max_retries} attempts failed")
                
                # Return fallback response / ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ã‚’è¿”ã™
                from refinire_rag.models.query import QueryResult
                fallback_result = QueryResult(
                    query=query,
                    answer="I apologize, but I couldn't process your query at this time. Please try rephrasing your question or contact support.",
                    confidence=0.0,
                    sources=[],
                    metadata={'error': str(e), 'fallback': True}
                )
                return fallback_result
    
    return None

# Usage / ä½¿ç”¨æ–¹æ³•
result = robust_query(query_engine, "What is quantum computing?")
```

### 6.2 Debugging Tools / ãƒ‡ãƒãƒƒã‚°ãƒ„ãƒ¼ãƒ«

```python
def debug_query_pipeline(query_engine, query):
    """
    Debug QueryEngine pipeline step by step
    QueryEngineãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ‡ãƒãƒƒã‚°
    """
    
    print("ğŸ”§ QUERY PIPELINE DEBUG / ã‚¯ã‚¨ãƒªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ‡ãƒãƒƒã‚°")
    print("="*60)
    
    # Step 1: Query preprocessing / ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¯ã‚¨ãƒªå‰å‡¦ç†
    print(f"\n1ï¸âƒ£ Original Query / å…ƒã‚¯ã‚¨ãƒª: {query}")
    
    # Check query normalization if enabled
    # æœ‰åŠ¹ãªå ´åˆã€ã‚¯ã‚¨ãƒªæ­£è¦åŒ–ã‚’ãƒã‚§ãƒƒã‚¯
    if hasattr(query_engine, '_normalize_query'):
        normalized = query_engine._normalize_query(query)
        print(f"   Normalized / æ­£è¦åŒ–æ¸ˆã¿: {normalized}")
    
    # Step 2: Retrieval / ã‚¹ãƒ†ãƒƒãƒ—2: æ¤œç´¢
    print(f"\n2ï¸âƒ£ Retrieval Phase / æ¤œç´¢ãƒ•ã‚§ãƒ¼ã‚º:")
    try:
        retrieved_docs = query_engine.retriever.retrieve(query)
        print(f"   Retrieved documents / æ¤œç´¢æ–‡æ›¸æ•°: {len(retrieved_docs)}")
        
        for i, doc in enumerate(retrieved_docs[:3]):
            score = doc.metadata.get('similarity_score', 'N/A')
            print(f"   {i+1}. Score: {score}, Length: {len(doc.content)}")
            
    except Exception as e:
        print(f"   âŒ Retrieval failed: {e}")
        return
    
    # Step 3: Reranking / ã‚¹ãƒ†ãƒƒãƒ—3: å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    print(f"\n3ï¸âƒ£ Reranking Phase / å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚º:")
    try:
        reranked_docs = query_engine.reranker.rerank(query, retrieved_docs)
        print(f"   Reranked documents / å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ–‡æ›¸æ•°: {len(reranked_docs)}")
        
        for i, doc in enumerate(reranked_docs[:3]):
            score = doc.metadata.get('rerank_score', 'N/A')
            print(f"   {i+1}. Rerank Score: {score}")
            
    except Exception as e:
        print(f"   âŒ Reranking failed: {e}")
        reranked_docs = retrieved_docs
    
    # Step 4: Answer synthesis / ã‚¹ãƒ†ãƒƒãƒ—4: å›ç­”åˆæˆ
    print(f"\n4ï¸âƒ£ Answer Synthesis / å›ç­”åˆæˆ:")
    try:
        answer = query_engine.reader.synthesize(query, reranked_docs)
        print(f"   Generated answer / ç”Ÿæˆå›ç­”: {len(answer)} characters")
        print(f"   Preview / ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {answer[:100]}...")
        
    except Exception as e:
        print(f"   âŒ Answer synthesis failed: {e}")
        return
    
    print(f"\nâœ… Pipeline completed successfully / ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ­£å¸¸å®Œäº†")

# Debug example / ãƒ‡ãƒãƒƒã‚°ä¾‹
debug_query_pipeline(query_engine, "How do neural networks learn?")
```

## 7. Complete Example / å®Œå…¨ãªä¾‹

```python
#!/usr/bin/env python3
"""
Complete QueryEngine tutorial example
å®Œå…¨ãªQueryEngineãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ä¾‹
"""

from pathlib import Path
from refinire_rag.application.corpus_manager_new import CorpusManager
from refinire_rag.application.query_engine import QueryEngine, QueryEngineConfig
from refinire_rag.storage.sqlite_store import SQLiteDocumentStore
from refinire_rag.storage.in_memory_vector_store import InMemoryVectorStore
from refinire_rag.retrieval import SimpleRetriever, SimpleReranker, SimpleReader

def main():
    # Setup corpus (assuming Part 1 completed)
    # ã‚³ãƒ¼ãƒ‘ã‚¹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆPart 1å®Œäº†å‰æï¼‰
    print("ğŸš€ QueryEngine Tutorial / QueryEngineãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«")
    print("="*60)
    
    # Initialize storage / ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åˆæœŸåŒ–
    doc_store = SQLiteDocumentStore(":memory:")
    vector_store = InMemoryVectorStore()
    
    # Create sample corpus / ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆ
    print("ğŸ“š Setting up sample corpus...")
    corpus_manager = CorpusManager.create_semantic_rag(doc_store, vector_store)
    
    # Create sample documents / ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ä½œæˆ
    sample_docs = [
        "AI is artificial intelligence technology.",
        "Machine learning is a subset of AI that learns from data.",
        "Deep learning uses neural networks with multiple layers."
    ]
    
    # Build corpus / ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰
    stats = corpus_manager.build_corpus(sample_docs)
    print(f"âœ… Corpus built: {stats.total_chunks_created} chunks")
    
    # Initialize QueryEngine / QueryEngineåˆæœŸåŒ–
    print("\nğŸ” Initializing QueryEngine...")
    
    query_engine = QueryEngine(
        document_store=doc_store,
        vector_store=vector_store,
        retriever=SimpleRetriever(vector_store),
        reranker=SimpleReranker(),
        reader=SimpleReader(),
        config=QueryEngineConfig(
            enable_query_normalization=True,
            include_sources=True,
            include_confidence=True
        )
    )
    
    # Demo queries / ãƒ‡ãƒ¢ã‚¯ã‚¨ãƒª
    queries = [
        "What is AI?",
        "How does machine learning work?",
        "Explain deep learning"
    ]
    
    print("\nğŸ’¬ Running demo queries...")
    for i, query in enumerate(queries, 1):
        print(f"\nğŸ“ Query {i}: {query}")
        result = query_engine.answer(query)
        
        print(f"ğŸ¤– Answer: {result.answer}")
        print(f"ğŸ¯ Confidence: {result.confidence:.3f}")
        print(f"ğŸ“š Sources: {len(result.sources)}")
    
    # Engine statistics / ã‚¨ãƒ³ã‚¸ãƒ³çµ±è¨ˆ
    print("\nğŸ“Š Engine Statistics:")
    stats = query_engine.get_engine_stats()
    print(f"   Queries processed: {stats.get('queries_processed', 0)}")
    print(f"   Average response time: {stats.get('average_response_time', 0):.3f}s")
    
    print("\nğŸ‰ Tutorial completed successfully!")
    return True

if __name__ == "__main__":
    success = main()
    print("âœ… All done!" if success else "âŒ Failed")
```

## Next Steps / æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

After mastering QueryEngine, proceed to:
QueryEngineã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ãŸå¾Œã€æ¬¡ã«é€²ã‚€ï¼š

1. **Part 3: Evaluation** - Learn to evaluate your RAG system performance
   **Part 3: Evaluation** - RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã‚’å­¦ç¿’
2. **Advanced Topics** - Custom retrievers, specialized readers, production deployment
   **é«˜åº¦ãªãƒˆãƒ”ãƒƒã‚¯** - ã‚«ã‚¹ã‚¿ãƒ æ¤œç´¢ã€ç‰¹æ®Šãƒªãƒ¼ãƒ€ãƒ¼ã€æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤

## Resources / ãƒªã‚½ãƒ¼ã‚¹

- [QueryEngine API Documentation](../api/query_engine.md)
- [Retrieval Components Reference](../api/retrieval.md)
- [Performance Tuning Guide](../development/performance_optimization.md)
- [Example Scripts](../../examples/)