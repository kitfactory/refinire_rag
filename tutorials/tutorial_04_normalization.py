#!/usr/bin/env python3
"""
ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«4: é«˜åº¦ãªæ­£è¦åŒ–ã¨ã‚¯ã‚¨ãƒªå‡¦ç†

ã“ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã¯ã€refinire-ragã®é«˜åº¦ãªæ­£è¦åŒ–æ©Ÿèƒ½ã¨ã€
ã‚¯ã‚¨ãƒªå‡¦ç†ã®æœ€é©åŒ–æŠ€è¡“ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
"""

import sys
import tempfile
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from refinire_rag.processing.normalizer import Normalizer, NormalizerConfig
from refinire_rag.application.query_engine import QueryEngine, QueryEngineConfig
from refinire_rag.storage.sqlite_store import SQLiteDocumentStore
from refinire_rag.storage.in_memory_vector_store import InMemoryVectorStore
from refinire_rag.retrieval import SimpleRetriever, SimpleReranker, SimpleReader
from refinire_rag.embedding import TFIDFEmbedder, TFIDFEmbeddingConfig
from refinire_rag.models.document import Document
from refinire_rag.storage.vector_store import VectorEntry


def create_comprehensive_dictionary(temp_dir: Path):
    """åŒ…æ‹¬çš„ãªæ­£è¦åŒ–è¾æ›¸ã‚’ä½œæˆ"""
    
    dict_file = temp_dir / "comprehensive_dictionary.md"
    dict_file.write_text("""# åŒ…æ‹¬çš„ãªæ­£è¦åŒ–è¾æ›¸

## AIãƒ»æ©Ÿæ¢°å­¦ç¿’ç”¨èª

### æ ¸å¿ƒæŠ€è¡“
- **RAG** (Retrieval-Augmented Generation): æ¤œç´¢æ‹¡å¼µç”Ÿæˆ
  - è¡¨ç¾æºã‚‰ã: æ¤œç´¢æ‹¡å¼µç”Ÿæˆ, æ¤œç´¢å¼·åŒ–ç”Ÿæˆ, æ¤œç´¢å¢—å¼·ç”Ÿæˆ, RAGã‚·ã‚¹ãƒ†ãƒ , RAGæŠ€è¡“, æ¤œç´¢æ‹¡å¼µæŠ€è¡“
  - ç•¥èª: RAG, R.A.G.

- **LLM** (Large Language Model): å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«
  - è¡¨ç¾æºã‚‰ã: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«, è¨€èªãƒ¢ãƒ‡ãƒ«, LLMãƒ¢ãƒ‡ãƒ«, å¤§è¦æ¨¡LM, å¤§å‹è¨€èªãƒ¢ãƒ‡ãƒ«
  - ç•¥èª: LLM, L.L.M.

- **NLP** (Natural Language Processing): è‡ªç„¶è¨€èªå‡¦ç†
  - è¡¨ç¾æºã‚‰ã: è‡ªç„¶è¨€èªå‡¦ç†, è¨€èªå‡¦ç†, NLPå‡¦ç†, è‡ªç„¶è¨€èªè§£æ
  - ç•¥èª: NLP, N.L.P.

### æ¤œç´¢æŠ€è¡“
- **ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢** (Vector Search): ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
  - è¡¨ç¾æºã‚‰ã: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢, æ„å‘³æ¤œç´¢, ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢, æ„å‘³çš„æ¤œç´¢, ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¤œç´¢
  - é–¢é€£æŠ€è¡“: åŸ‹ã‚è¾¼ã¿æ¤œç´¢, ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°æ¤œç´¢

- **é¡ä¼¼åº¦è¨ˆç®—** (Similarity Calculation): é¡ä¼¼åº¦è¨ˆç®—
  - è¡¨ç¾æºã‚‰ã: é¡ä¼¼åº¦è¨ˆç®—, é¡ä¼¼æ€§è¨ˆç®—, é¡ä¼¼åº¦æ¸¬å®š, é¡ä¼¼æ€§è©•ä¾¡
  - å…·ä½“çš„æ‰‹æ³•: ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦, ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢, å†…ç©è¨ˆç®—

### æ–‡æ›¸å‡¦ç†
- **ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°** (Chunking): ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
  - è¡¨ç¾æºã‚‰ã: ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°, æ–‡æ›¸åˆ†å‰², ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰², ãƒãƒ£ãƒ³ã‚¯åŒ–, æ–‡æ›¸ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
  - é–¢é€£æŠ€è¡“: æ–‡æ›¸åˆ†å‰², ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²

- **åŸ‹ã‚è¾¼ã¿** (Embedding): åŸ‹ã‚è¾¼ã¿
  - è¡¨ç¾æºã‚‰ã: åŸ‹ã‚è¾¼ã¿, ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°, ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾, ãƒ™ã‚¯ãƒˆãƒ«åŒ–, ç‰¹å¾´é‡æŠ½å‡º

## è©•ä¾¡ãƒ»å“è³ª
- **ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³** (Hallucination): ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³
  - è¡¨ç¾æºã‚‰ã: ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³, å¹»è¦š, è™šå½ç”Ÿæˆ, èª¤æƒ…å ±ç”Ÿæˆ, æé€ 

- **BLEU** (BLEU Score): BLEU
  - è¡¨ç¾æºã‚‰ã: BLEU, BLEUã‚¹ã‚³ã‚¢, BLEUè©•ä¾¡, ãƒ–ãƒ«ãƒ¼ã‚¹ã‚³ã‚¢

## æŠ€è¡“è¨­å®šãƒ‘ã‚¿ãƒ¼ãƒ³

### æ—¥æœ¬èªå›ºæœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
- **æ–‡å­—ç¨®å¤‰æ›**: ã²ã‚‰ãŒãªâ†”ã‚«ã‚¿ã‚«ãƒŠ
  - AI â†’ ã‚¨ãƒ¼ã‚¢ã‚¤, AI â†’ ã‚¨ã‚¤ã‚¢ã‚¤
  - ML â†’ ã‚¨ãƒ ã‚¨ãƒ«, ML â†’ ãƒã‚·ãƒ³ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°

### è‹±æ•°å­—è¡¨è¨˜æºã‚‰ã
- **è‹±å­—å¤§å°æ–‡å­—**: OpenAI â†” openai â†” OPENAI
- **æ•°å­—è¡¨è¨˜**: 1ã¤ â†” ä¸€ã¤ â†” ã²ã¨ã¤
- **å˜ä½è¡¨è¨˜**: 10MB â†” 10ãƒ¡ã‚¬ãƒã‚¤ãƒˆ â†” 10ãƒ¡ã‚¬B

### é€ã‚Šä»®åãƒ»èªå°¾å¤‰åŒ–
- **å‹•è©æ´»ç”¨**: è¡Œã† â†” è¡Œãªã†, è¡¨ã™ â†” è¡¨ã‚ã™
- **å½¢å®¹è©èªå°¾**: æ–°ã—ã„ â†” æ–°ã‚‰ã—ã„, æ­£ã—ã„ â†” æ­£ã‹ã—ã„
""", encoding='utf-8')
    
    return str(dict_file)


def create_domain_specific_dictionaries(temp_dir: Path):
    """ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–è¾æ›¸ã‚’ä½œæˆ"""
    
    # åŒ»ç™‚ãƒ‰ãƒ¡ã‚¤ãƒ³è¾æ›¸
    medical_dict = temp_dir / "medical_dictionary.md"
    medical_dict.write_text("""# åŒ»ç™‚ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–è¾æ›¸

## åŒ»ç™‚AIç”¨èª
- **è¨ºæ–­æ”¯æ´AI** (Diagnostic AI): è¨ºæ–­æ”¯æ´AI
  - è¡¨ç¾æºã‚‰ã: è¨ºæ–­æ”¯æ´AI, AIè¨ºæ–­, è¨ºæ–­AI, åŒ»ç™‚è¨ºæ–­AI, ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿è¨ºæ–­

- **ç”»åƒè¨ºæ–­** (Medical Imaging): ç”»åƒè¨ºæ–­
  - è¡¨ç¾æºã‚‰ã: ç”»åƒè¨ºæ–­, åŒ»ç”¨ç”»åƒ, æ”¾å°„ç·šè¨ºæ–­, ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ³ã‚°è¨ºæ–­

## ç–¾æ‚£å
- **COVID-19** (COVID-19): COVID-19
  - è¡¨ç¾æºã‚‰ã: COVID-19, Covid-19, ã‚³ãƒ­ãƒŠ, æ–°å‹ã‚³ãƒ­ãƒŠ, SARS-CoV-2
""", encoding='utf-8')
    
    # é‡‘èãƒ‰ãƒ¡ã‚¤ãƒ³è¾æ›¸
    finance_dict = temp_dir / "finance_dictionary.md"
    finance_dict.write_text("""# é‡‘èãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–è¾æ›¸

## é‡‘èAIç”¨èª
- **ãƒ­ãƒœã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼** (Robo-advisor): ãƒ­ãƒœã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼
  - è¡¨ç¾æºã‚‰ã: ãƒ­ãƒœã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼, ãƒ­ãƒœãƒ»ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼, AIæŠ•è³‡é¡§å•, è‡ªå‹•æŠ•è³‡

- **ãƒªã‚¹ã‚¯ç®¡ç†** (Risk Management): ãƒªã‚¹ã‚¯ç®¡ç†
  - è¡¨ç¾æºã‚‰ã: ãƒªã‚¹ã‚¯ç®¡ç†, ãƒªã‚¹ã‚¯ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ, å±é™ºç®¡ç†, ãƒªã‚¹ã‚¯åˆ¶å¾¡
""", encoding='utf-8')
    
    return str(medical_dict), str(finance_dict)


def demo_advanced_normalization_configs(temp_dir: Path):
    """é«˜åº¦ãªæ­£è¦åŒ–è¨­å®šã®ãƒ‡ãƒ¢"""
    
    print("\n" + "="*60)
    print("ğŸ”§ é«˜åº¦ãªæ­£è¦åŒ–è¨­å®šãƒ‡ãƒ¢")
    print("="*60)
    
    # åŒ…æ‹¬è¾æ›¸ä½œæˆ
    dict_path = create_comprehensive_dictionary(temp_dir)
    
    # ç•°ãªã‚‹è¨­å®šã§ã®æ­£è¦åŒ–æ¯”è¼ƒ
    configs = [
        {
            "name": "åŸºæœ¬è¨­å®š",
            "config": NormalizerConfig(
                dictionary_file_path=dict_path,
                normalize_variations=True,
                expand_abbreviations=True,
                whole_word_only=False
            )
        },
        {
            "name": "å³å¯†ãƒãƒƒãƒè¨­å®š",
            "config": NormalizerConfig(
                dictionary_file_path=dict_path,
                normalize_variations=True,
                expand_abbreviations=True,
                whole_word_only=True
            )
        },
        {
            "name": "éƒ¨åˆ†ãƒãƒƒãƒè¨­å®š",
            "config": NormalizerConfig(
                dictionary_file_path=dict_path,
                normalize_variations=True,
                expand_abbreviations=True,
                whole_word_only=False
            )
        }
    ]
    
    # ãƒ†ã‚¹ãƒˆæ–‡æ›¸
    test_texts = [
        "æ¤œç´¢å¼·åŒ–ç”Ÿæˆã¨RAGã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™",
        "LLMãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’BLEUã‚¹ã‚³ã‚¢ã§è©•ä¾¡",
        "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã¨ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦è¨ˆç®—",
        "AIè¨ºæ–­æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ã®é–‹ç™º",
        "ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ä½¿ã£ãŸæ„å‘³æ¤œç´¢ã®å®Ÿè£…"
    ]
    
    print("ğŸ“ è¨­å®šåˆ¥æ­£è¦åŒ–çµæœæ¯”è¼ƒ:")
    print("-" * 70)
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nğŸ“Œ ãƒ†ã‚¹ãƒˆæ–‡ {i}: ã€Œ{text}ã€")
        print("-" * 50)
        
        for config_info in configs:
            config_name = config_info["name"]
            normalizer = Normalizer(config_info["config"])
            
            doc = Document(id=f"test_{i}", content=text, metadata={})
            normalized_docs = normalizer.process(doc)
            
            if normalized_docs:
                normalized_text = normalized_docs[0].content
                changes = "âœ…å¤‰æ›´ã‚ã‚Š" if text != normalized_text else "ğŸ”„å¤‰æ›´ãªã—"
                
                print(f"  {config_name:12}: ã€Œ{normalized_text}ã€ {changes}")
            else:
                print(f"  {config_name:12}: âŒ å‡¦ç†å¤±æ•—")


def demo_query_normalization_optimization(temp_dir: Path):
    """ã‚¯ã‚¨ãƒªæ­£è¦åŒ–æœ€é©åŒ–ã®ãƒ‡ãƒ¢"""
    
    print("\n" + "="*60)
    print("ğŸ¯ ã‚¯ã‚¨ãƒªæ­£è¦åŒ–æœ€é©åŒ–ãƒ‡ãƒ¢")
    print("="*60)
    
    # è¾æ›¸ã¨ã‚³ãƒ¼ãƒ‘ã‚¹æº–å‚™
    dict_path = create_comprehensive_dictionary(temp_dir)
    
    # æ­£è¦åŒ–ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½œæˆ
    documents = [
        Document(
            id="doc1",
            content="""
            æ¤œç´¢æ‹¡å¼µç”Ÿæˆï¼ˆRAGï¼‰ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹
            é©æ–°çš„ãªæŠ€è¡“ã§ã™ã€‚å¤–éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®æƒ…å ±æ¤œç´¢ã¨
            è¨€èªç”Ÿæˆã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šæ­£ç¢ºã§
            æ ¹æ‹ ã®ã‚ã‚‹å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚
            """,
            metadata={"title": "RAGæŠ€è¡“è§£èª¬", "domain": "AI"}
        ),
        Document(
            id="doc2", 
            content="""
            ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯ã€æ–‡æ›¸ã®æ„å‘³çš„é¡ä¼¼æ€§ã‚’
            ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚„å†…ç©è¨ˆç®—ã§è©•ä¾¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚
            å¾“æ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã§ã¯ç™ºè¦‹ã§ããªã„
            é–¢é€£æƒ…å ±ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
            """,
            metadata={"title": "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æŠ€è¡“", "domain": "Search"}
        ),
        Document(
            id="doc3",
            content="""
            ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã¯ã€é•·ã„æ–‡æ›¸ã‚’æ¤œç´¢ã—ã‚„ã™ã„
            å°ã•ãªå˜ä½ã«åˆ†å‰²ã™ã‚‹å‰å‡¦ç†æŠ€è¡“ã§ã™ã€‚
            é©åˆ‡ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¨ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã«ã‚ˆã‚Šã€
            æ–‡è„ˆã‚’ä¿æŒã—ã¤ã¤åŠ¹ç‡çš„ãªæ¤œç´¢ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
            """,
            metadata={"title": "æ–‡æ›¸å‡¦ç†æŠ€è¡“", "domain": "NLP"}
        )
    ]
    
    # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åˆæœŸåŒ–
    doc_store = SQLiteDocumentStore(":memory:")
    vector_store = InMemoryVectorStore()
    
    # æ­£è¦åŒ–å™¨è¨­å®š
    normalizer_config = NormalizerConfig(
        dictionary_file_path=dict_path,
        normalize_variations=True,
        expand_abbreviations=True,
        whole_word_only=False
    )
    normalizer = Normalizer(normalizer_config)
    
    # ã‚³ãƒ¼ãƒ‘ã‚¹æ§‹ç¯‰ï¼ˆæ­£è¦åŒ–é©ç”¨ï¼‰
    normalized_docs = []
    for doc in documents:
        # åŸæ–‡ä¿å­˜
        doc_store.store_document(doc)
        
        # æ­£è¦åŒ–ç‰ˆä½œæˆ
        norm_results = normalizer.process(doc)
        if norm_results:
            norm_doc = norm_results[0]
            norm_doc.metadata["processing_stage"] = "normalized"
            doc_store.store_document(norm_doc)
            normalized_docs.append(norm_doc)
    
    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    embedder_config = TFIDFEmbeddingConfig(min_df=1, max_df=1.0)
    embedder = TFIDFEmbedder(config=embedder_config)
    
    corpus_texts = [doc.content for doc in normalized_docs]
    embedder.fit(corpus_texts)
    
    for doc in normalized_docs:
        embedding_result = embedder.embed_text(doc.content)
        vector_entry = VectorEntry(
            document_id=doc.id,
            content=doc.content[:200] + "..." if len(doc.content) > 200 else doc.content,
            embedding=embedding_result.vector.tolist(),
            metadata=doc.metadata
        )
        vector_store.add_vector(vector_entry)
    
    # QueryEngineè¨­å®šï¼ˆæ­£è¦åŒ–ã‚ã‚Š/ãªã—æ¯”è¼ƒï¼‰
    retriever = SimpleRetriever(vector_store, embedder=embedder)
    reranker = SimpleReranker()
    reader = SimpleReader()
    
    # æ­£è¦åŒ–ãªã—ã‚¨ãƒ³ã‚¸ãƒ³
    engine_no_norm = QueryEngine(
        document_store=doc_store,
        vector_store=vector_store,
        retriever=retriever,
        reader=reader,
        reranker=reranker,
        config=QueryEngineConfig(
            enable_query_normalization=False,
            retriever_top_k=5,
            include_sources=True
        )
    )
    
    # æ­£è¦åŒ–ã‚ã‚Šã‚¨ãƒ³ã‚¸ãƒ³
    engine_with_norm = QueryEngine(
        document_store=doc_store,
        vector_store=vector_store,
        retriever=retriever,
        reader=reader,
        reranker=reranker,
        config=QueryEngineConfig(
            enable_query_normalization=True,
            retriever_top_k=5,
            include_sources=True
        )
    )
    
    # æ‰‹å‹•ã§æ­£è¦åŒ–å™¨ã‚’è¨­å®š
    engine_with_norm.normalizer = normalizer
    engine_with_norm.corpus_state = {
        "has_normalization": True,
        "dictionary_path": dict_path
    }
    
    # è¡¨ç¾æºã‚‰ãã‚’å«ã‚€ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
    test_queries = [
        {
            "query": "æ¤œç´¢å¼·åŒ–ç”Ÿæˆã®ä»•çµ„ã¿ã‚’æ•™ãˆã¦",
            "expected_improvement": "æ¤œç´¢å¼·åŒ–ç”Ÿæˆ â†’ æ¤œç´¢æ‹¡å¼µç”Ÿæˆ"
        },
        {
            "query": "æ„å‘³æ¤œç´¢ã¨ã¯ã©ã®ã‚ˆã†ãªæŠ€è¡“ï¼Ÿ",
            "expected_improvement": "æ„å‘³æ¤œç´¢ â†’ ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"
        },
        {
            "query": "æ–‡æ›¸åˆ†å‰²ã®æœ€é©åŒ–æ–¹æ³•ã¯ï¼Ÿ",
            "expected_improvement": "æ–‡æ›¸åˆ†å‰² â†’ ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°"
        },
        {
            "query": "LLMãƒ¢ãƒ‡ãƒ«ã¨RAGã‚·ã‚¹ãƒ†ãƒ ã®é–¢ä¿‚",
            "expected_improvement": "LLMãƒ¢ãƒ‡ãƒ« â†’ å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«"
        }
    ]
    
    print("ğŸ“Š æ­£è¦åŒ–åŠ¹æœæ¯”è¼ƒ:")
    print("-" * 70)
    
    for i, test_case in enumerate(test_queries, 1):
        query = test_case["query"]
        expected = test_case["expected_improvement"]
        
        print(f"\nğŸ“Œ ã‚¯ã‚¨ãƒª {i}: ã€Œ{query}ã€")
        print(f"   æœŸå¾…ã™ã‚‹æ­£è¦åŒ–: {expected}")
        print("-" * 50)
        
        try:
            # æ­£è¦åŒ–ãªã—çµæœ
            result_no_norm = engine_no_norm.answer(query)
            
            # æ­£è¦åŒ–ã‚ã‚Šçµæœ
            result_with_norm = engine_with_norm.answer(query)
            
            print(f"ğŸ” æ­£è¦åŒ–ãªã—:")
            print(f"   - æ¤œç´¢çµæœæ•°: {result_no_norm.metadata.get('source_count', 0)}")
            print(f"   - ä¿¡é ¼åº¦: {result_no_norm.confidence:.3f}")
            if result_no_norm.sources:
                print(f"   - ä¸»è¦ã‚½ãƒ¼ã‚¹: {result_no_norm.sources[0].metadata.get('title', 'Unknown')}")
            
            print(f"âœ¨ æ­£è¦åŒ–ã‚ã‚Š:")
            normalized_query = result_with_norm.metadata.get('normalized_query', query)
            print(f"   - æ­£è¦åŒ–å¾Œã‚¯ã‚¨ãƒª: ã€Œ{normalized_query}ã€")
            print(f"   - æ¤œç´¢çµæœæ•°: {result_with_norm.metadata.get('source_count', 0)}")
            print(f"   - ä¿¡é ¼åº¦: {result_with_norm.confidence:.3f}")
            if result_with_norm.sources:
                print(f"   - ä¸»è¦ã‚½ãƒ¼ã‚¹: {result_with_norm.sources[0].metadata.get('title', 'Unknown')}")
            
            # æ”¹å–„åŠ¹æœè©•ä¾¡
            improvement = result_with_norm.confidence - result_no_norm.confidence
            if improvement > 0.05:
                print(f"   ğŸ“ˆ ä¿¡é ¼åº¦æ”¹å–„: +{improvement:.3f} (å¤§å¹…æ”¹å–„)")
            elif improvement > 0.01:
                print(f"   ğŸ“ˆ ä¿¡é ¼åº¦æ”¹å–„: +{improvement:.3f} (æ”¹å–„)")
            elif improvement > -0.01:
                print(f"   â¡ï¸ ä¿¡é ¼åº¦å¤‰åŒ–: {improvement:+.3f} (å¤‰åŒ–ãªã—)")
            else:
                print(f"   ğŸ“‰ ä¿¡é ¼åº¦ä½ä¸‹: {improvement:.3f} (è¦èª¿æ•´)")
                
        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    
    return engine_with_norm


def demo_normalization_performance_optimization(temp_dir: Path):
    """æ­£è¦åŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãƒ‡ãƒ¢"""
    
    print("\n" + "="*60)
    print("âš¡ æ­£è¦åŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–")
    print("="*60)
    
    # å¤§è¦æ¨¡è¾æ›¸ä½œæˆ
    dict_path = create_comprehensive_dictionary(temp_dir)
    
    # å¤§é‡ã®ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ
    test_documents = []
    for i in range(50):  # 100ã‹ã‚‰50ã«å‰Šæ¸›ã—ã¦ãƒ†ã‚¹ãƒˆæ™‚é–“ã‚’çŸ­ç¸®
        content = f"""
        ãƒ†ã‚¹ãƒˆæ–‡æ›¸ {i}: æ¤œç´¢å¼·åŒ–ç”Ÿæˆã¨LLMãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã€‚
        æ„å‘³æ¤œç´¢ã¨ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦è¨ˆç®—ã‚’ä½¿ç”¨ã€‚
        æ–‡æ›¸åˆ†å‰²ã¨ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã®æœ€é©åŒ–ã€‚
        AIè¨ºæ–­æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ã®é–‹ç™ºæ¡ˆä»¶ã€‚
        """
        test_documents.append(Document(
            id=f"perf_test_{i}",
            content=content,
            metadata={"test_id": i}
        ))
    
    # ç•°ãªã‚‹æœ€é©åŒ–è¨­å®šã‚’ãƒ†ã‚¹ãƒˆ
    optimization_configs = [
        {
            "name": "åŸºæœ¬è¨­å®š",
            "config": NormalizerConfig(
                dictionary_file_path=dict_path,
                normalize_variations=True,
                expand_abbreviations=True,
                whole_word_only=False
            ),
            "parallel": False
        },
        {
            "name": "é«˜é€Ÿè¨­å®š",
            "config": NormalizerConfig(
                dictionary_file_path=dict_path,
                normalize_variations=True,
                expand_abbreviations=True,
                whole_word_only=False
            ),
            "parallel": False
        },
        {
            "name": "ä¸¦åˆ—å‡¦ç†",
            "config": NormalizerConfig(
                dictionary_file_path=dict_path,
                normalize_variations=True,
                expand_abbreviations=True,
                whole_word_only=False
            ),
            "parallel": True
        }
    ]
    
    print("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒçµæœ:")
    print("-" * 60)
    
    results = {}
    
    for config_info in optimization_configs:
        config_name = config_info["name"]
        normalizer_config = config_info["config"]
        use_parallel = config_info["parallel"]
        
        print(f"\nğŸ”§ {config_name}:")
        
        normalizer = Normalizer(normalizer_config)
        
        start_time = time.time()
        
        if use_parallel:
            # ä¸¦åˆ—å‡¦ç†ã§æ­£è¦åŒ–
            with ThreadPoolExecutor(max_workers=4) as executor:
                futures = [executor.submit(normalizer.process, doc) for doc in test_documents]
                normalized_results = [future.result() for future in futures]
        else:
            # é€æ¬¡å‡¦ç†ã§æ­£è¦åŒ–
            normalized_results = [normalizer.process(doc) for doc in test_documents]
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        # çµæœçµ±è¨ˆ
        successful_docs = sum(1 for result in normalized_results if result)
        total_changes = 0
        
        for result in normalized_results:
            if result and len(result) > 0:
                original_doc = test_documents[normalized_results.index(result)]
                if result[0].content != original_doc.content:
                    total_changes += 1
        
        results[config_name] = {
            "time": processing_time,
            "successful": successful_docs,
            "changes": total_changes,
            "throughput": len(test_documents) / processing_time
        }
        
        print(f"   â±ï¸ å‡¦ç†æ™‚é–“: {processing_time:.3f}ç§’")
        print(f"   ğŸ“Š æˆåŠŸç‡: {successful_docs}/{len(test_documents)} ({successful_docs/len(test_documents):.1%})")
        print(f"   ğŸ”„ å¤‰æ›´æ–‡æ›¸æ•°: {total_changes}")
        print(f"   ğŸš€ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {len(test_documents)/processing_time:.1f} docs/sec")
    
    # æœ€é©åŒ–åŠ¹æœã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“ˆ æœ€é©åŒ–åŠ¹æœã‚µãƒãƒªãƒ¼:")
    baseline_time = results["åŸºæœ¬è¨­å®š"]["time"]
    
    for config_name, result in results.items():
        if config_name != "åŸºæœ¬è¨­å®š":
            speedup = baseline_time / result["time"]
            print(f"   {config_name}: {speedup:.2f}xé«˜é€ŸåŒ–")
    
    return results


def demo_domain_specific_normalization(temp_dir: Path):
    """ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–æ­£è¦åŒ–ã®ãƒ‡ãƒ¢"""
    
    print("\n" + "="*60)
    print("ğŸ¥ ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–æ­£è¦åŒ–ãƒ‡ãƒ¢")
    print("="*60)
    
    # ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–è¾æ›¸ä½œæˆ
    medical_dict, finance_dict = create_domain_specific_dictionaries(temp_dir)
    general_dict = create_comprehensive_dictionary(temp_dir)
    
    # å„ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    domain_tests = [
        {
            "domain": "åŒ»ç™‚",
            "dict_path": medical_dict,
            "texts": [
                "COVID-19ã®è¨ºæ–­æ”¯æ´AIã‚·ã‚¹ãƒ†ãƒ ",
                "ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã®ç”»åƒè¨ºæ–­æŠ€è¡“",
                "æ–°å‹ã‚³ãƒ­ãƒŠã®åŒ»ç”¨ç”»åƒè§£æ"
            ]
        },
        {
            "domain": "é‡‘è", 
            "dict_path": finance_dict,
            "texts": [
                "ãƒ­ãƒœãƒ»ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã«ã‚ˆã‚‹æŠ•è³‡æ”¯æ´",
                "AIæŠ•è³‡é¡§å•ã®ãƒªã‚¹ã‚¯ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ",
                "è‡ªå‹•æŠ•è³‡ã‚·ã‚¹ãƒ†ãƒ ã®å±é™ºç®¡ç†"
            ]
        },
        {
            "domain": "ä¸€èˆ¬AI",
            "dict_path": general_dict,
            "texts": [
                "æ¤œç´¢å¼·åŒ–ç”Ÿæˆã®å®Ÿè£…æ–¹æ³•",
                "LLMãƒ¢ãƒ‡ãƒ«ã¨æ„å‘³æ¤œç´¢ã®çµ„ã¿åˆã‚ã›",
                "æ–‡æ›¸åˆ†å‰²ã®æœ€é©åŒ–æŠ€è¡“"
            ]
        }
    ]
    
    print("ğŸ“ ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥æ­£è¦åŒ–çµæœ:")
    print("-" * 50)
    
    for domain_test in domain_tests:
        domain = domain_test["domain"]
        dict_path = domain_test["dict_path"]
        texts = domain_test["texts"]
        
        print(f"\nğŸ·ï¸ {domain}ãƒ‰ãƒ¡ã‚¤ãƒ³:")
        print("-" * 30)
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–æ­£è¦åŒ–å™¨
        normalizer_config = NormalizerConfig(
            dictionary_file_path=dict_path,
            normalize_variations=True,
            expand_abbreviations=True,
            whole_word_only=False
        )
        normalizer = Normalizer(normalizer_config)
        
        for i, text in enumerate(texts, 1):
            print(f"\nğŸ“Œ ãƒ†ã‚¹ãƒˆ {i}:")
            print(f"   å…ƒæ–‡: ã€Œ{text}ã€")
            
            try:
                doc = Document(id=f"{domain}_{i}", content=text, metadata={})
                normalized_docs = normalizer.process(doc)
                
                if normalized_docs:
                    normalized_text = normalized_docs[0].content
                    if text != normalized_text:
                        print(f"   æ­£è¦åŒ–å¾Œ: ã€Œ{normalized_text}ã€")
                        print(f"   ğŸ”„ å¤‰æ›´: ã‚ã‚Š")
                    else:
                        print(f"   ğŸ”„ å¤‰æ›´: ãªã—")
                else:
                    print(f"   âŒ æ­£è¦åŒ–å¤±æ•—")
            except Exception as e:
                print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    
    # è¤‡æ•°è¾æ›¸ã®çµ±åˆãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ”— è¤‡æ•°è¾æ›¸çµ±åˆãƒ†ã‚¹ãƒˆ:")
    print("-" * 30)
    
    # çµ±åˆè¾æ›¸ä½œæˆ
    combined_dict = temp_dir / "combined_dictionary.md"
    
    # å„è¾æ›¸ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚“ã§çµ±åˆ
    combined_content = "# çµ±åˆè¾æ›¸\n\n"
    
    for dict_path in [general_dict, medical_dict, finance_dict]:
        with open(dict_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’èª¿æ•´
            content = content.replace("# ", "## ")
            combined_content += content + "\n\n"
    
    combined_dict.write_text(combined_content, encoding='utf-8')
    
    # çµ±åˆè¾æ›¸ã§ã®æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ
    combined_normalizer = Normalizer(NormalizerConfig(
        dictionary_file_path=str(combined_dict),
        normalize_variations=True,
        expand_abbreviations=True,
        whole_word_only=False
    ))
    
    cross_domain_texts = [
        "COVID-19è¨ºæ–­ã®RAGã‚·ã‚¹ãƒ†ãƒ é–‹ç™º",
        "é‡‘èAIã¨LLMãƒ¢ãƒ‡ãƒ«ã®çµ±åˆ",
        "åŒ»ç™‚ç”»åƒã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢"
    ]
    
    for i, text in enumerate(cross_domain_texts, 1):
        print(f"\nğŸ“Œ ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ {i}:")
        print(f"   å…ƒæ–‡: ã€Œ{text}ã€")
        
        try:
            doc = Document(id=f"cross_{i}", content=text, metadata={})
            normalized_docs = combined_normalizer.process(doc)
            
            if normalized_docs:
                normalized_text = normalized_docs[0].content
                print(f"   çµ±åˆæ­£è¦åŒ–: ã€Œ{normalized_text}ã€")
                
                changes = []
                if "COVID-19" in text and "COVID-19" in normalized_text:
                    changes.append("åŒ»ç™‚ç”¨èªæ­£è¦åŒ–")
                if "RAG" in text:
                    changes.append("AIç”¨èªæ­£è¦åŒ–")
                
                if changes:
                    print(f"   ğŸ¯ é©ç”¨é ˜åŸŸ: {', '.join(changes)}")
                else:
                    print(f"   ğŸ”„ å¤‰æ›´ãªã—")
            else:
                print(f"   âŒ æ­£è¦åŒ–å¤±æ•—")
        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    print("ğŸš€ é«˜åº¦ãªæ­£è¦åŒ–ã¨ã‚¯ã‚¨ãƒªå‡¦ç† ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«")
    print("="*60)
    print("è¾æ›¸ãƒ™ãƒ¼ã‚¹æ­£è¦åŒ–ã®è©³ç´°è¨­å®šã¨ã‚¯ã‚¨ãƒªå‡¦ç†æœ€é©åŒ–ã‚’å­¦ç¿’ã—ã¾ã™")
    
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: é«˜åº¦ãªè¾æ›¸è¨­è¨ˆ
        print("\nğŸ“š ã‚¹ãƒ†ãƒƒãƒ—1: é«˜åº¦ãªè¾æ›¸è¨­è¨ˆ")
        create_comprehensive_dictionary(temp_dir)
        create_domain_specific_dictionaries(temp_dir)
        print("âœ… åŒ…æ‹¬çš„è¾æ›¸ã¨ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–è¾æ›¸ã‚’ä½œæˆ")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: æ­£è¦åŒ–è¨­å®šã®è©³ç´°ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
        demo_advanced_normalization_configs(temp_dir)
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¯ã‚¨ãƒªæ™‚æ­£è¦åŒ–ã®æœ€é©åŒ–
        optimized_engine = demo_query_normalization_optimization(temp_dir)
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
        perf_results = demo_normalization_performance_optimization(temp_dir)
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–æ­£è¦åŒ–
        demo_domain_specific_normalization(temp_dir)
        
        print("\nğŸ‰ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«4ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("\nğŸ“š å­¦ç¿’å†…å®¹:")
        print("   âœ… åŒ…æ‹¬çš„ãªæ­£è¦åŒ–è¾æ›¸ã®è¨­è¨ˆ")
        print("   âœ… æ­£è¦åŒ–è¨­å®šã®è©³ç´°ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º")
        print("   âœ… ã‚¯ã‚¨ãƒªæ™‚æ­£è¦åŒ–ã«ã‚ˆã‚‹æ¤œç´¢ç²¾åº¦å‘ä¸Š")
        print("   âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–æŠ€è¡“")
        print("   âœ… ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–æ­£è¦åŒ–ã®å®Ÿè£…")
        
        print(f"\nğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«: {temp_dir}")
        print("\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("   â€¢ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«5: ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰")
        print("   â€¢ ã‚ˆã‚ŠæŸ”è»Ÿãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆ")
        print("   â€¢ ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®é–‹ç™º")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)