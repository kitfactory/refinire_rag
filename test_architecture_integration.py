#!/usr/bin/env python3
"""
Integration test for the new unified architecture:
- VectorStore as DocumentProcessor with Indexer/Retriever functionality
- KeywordSearch as DocumentProcessor 
- CorpusManager working with new VectorStore
"""

import sys
from pathlib import Path
import tempfile
import numpy as np

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "src"))


def test_architecture_integration():
    """Test the complete integrated architecture"""
    print("=== Testing Architecture Integration ===\n")
    
    try:
        from refinire_rag.storage.in_memory_vector_store import InMemoryVectorStore
        from refinire_rag.storage.sqlite_store import SQLiteDocumentStore
        from refinire_rag.models.document import Document
        from refinire_rag.document_processor import DocumentPipeline
        # Import only core components for architecture test
        from refinire_rag.processing.chunker import Chunker, ChunkingConfig
        
        # Test 1: Set up stores
        print("1. Setting up stores:")
        
        # Create temporary database for testing
        temp_db = tempfile.NamedTemporaryFile(suffix='.db', delete=False)
        temp_db.close()
        
        document_store = SQLiteDocumentStore(temp_db.name)
        vector_store = InMemoryVectorStore()
        
        print("   ‚úì DocumentStore created")
        print("   ‚úì VectorStore created")
        
        # Set up mock embedder
        class MockEmbedder:
            def embed_text(self, text: str) -> np.ndarray:
                import hashlib
                hash_val = int(hashlib.md5(text.encode()).hexdigest()[:8], 16)
                np.random.seed(hash_val % 2**32)
                return np.random.randn(384).astype(np.float32)
            
            def embed_texts(self, texts: list) -> list:
                return [self.embed_text(text) for text in texts]
        
        vector_store.set_embedder(MockEmbedder())
        print("   ‚úì Mock embedder configured")
        
        # Test 2: Test VectorStore in DocumentPipeline
        print("\n2. Testing VectorStore in DocumentPipeline:")
        
        # Create test documents
        test_docs = [
            Document(id="test1", content="This is a test document about machine learning", metadata={"source": "test"}),
            Document(id="test2", content="This document discusses artificial intelligence concepts", metadata={"source": "test"}),
        ]
        
        # Create pipeline with VectorStore
        pipeline = DocumentPipeline([vector_store])
        
        # Process documents
        processed_docs = list(pipeline.process_documents(test_docs))
        print(f"   ‚úì Pipeline processed {len(processed_docs)} documents")
        print(f"   ‚úì Documents indexed in VectorStore: {vector_store.get_document_count()}")
        
        # Test retrieval
        results = vector_store.retrieve("machine learning", limit=1)
        print(f"   ‚úì Retrieval found {len(results)} relevant documents")
        if results:
            print(f"   ‚úì Top result score: {results[0].score:.3f}")
        
        # Test 3: Test combined processing pipeline
        print("\n3. Testing combined processing pipeline:")
        
        # Clear vector store for fresh test
        vector_store.clear_index()
        
        # Create pipeline: Chunker ‚Üí VectorStore
        chunker = Chunker(ChunkingConfig(chunk_size=100, overlap_size=20))
        combined_pipeline = DocumentPipeline([chunker, vector_store])
        
        # Create larger document for chunking
        large_doc = Document(
            id="large_doc", 
            content="This is a large document about artificial intelligence and machine learning. " * 10,
            metadata={"type": "large", "source": "test"}
        )
        
        # Process through combined pipeline
        processed_large = list(combined_pipeline.process_documents([large_doc]))
        print(f"   ‚úì Combined pipeline processed {len(processed_large)} documents")
        print(f"   ‚úì Chunks created and indexed: {vector_store.get_document_count()}")
        
        # Test retrieval on chunks
        chunk_results = vector_store.retrieve("artificial intelligence", limit=3)
        print(f"   ‚úì Chunk retrieval found {len(chunk_results)} results")
        
        # Test 4: Test advanced pipeline combinations 
        print("\n4. Testing advanced pipeline combinations:")
        
        # Clear vector store for fresh test
        vector_store.clear_index()
        
        # Test multiple document processing with different types
        mixed_docs = [
            Document(id="doc1", content="Machine learning algorithms are powerful tools", metadata={"type": "technical"}),
            Document(id="doc2", content="Data science involves statistical analysis", metadata={"type": "academic"}),
            Document(id="doc3", content="Artificial intelligence will transform society", metadata={"type": "opinion"}),
        ]
        
        # Create multi-stage pipeline: Chunker ‚Üí VectorStore
        multi_pipeline = DocumentPipeline([chunker, vector_store])
        
        # Process mixed documents
        processed_mixed = list(multi_pipeline.process_documents(mixed_docs))
        print(f"   ‚úì Multi-stage pipeline processed {len(processed_mixed)} documents")
        print(f"   ‚úì Total chunks indexed: {vector_store.get_document_count()}")
        
        # Test metadata-based retrieval
        results_tech = vector_store.search_by_metadata({"type": "technical"}, limit=5)
        print(f"   ‚úì Technical documents found: {len(results_tech)}")
        
        # Test content-based retrieval  
        results_ml = vector_store.search_with_text("machine learning", limit=3)
        print(f"   ‚úì ML-related documents found: {len(results_ml)}")
        
        print("   ‚úì Advanced pipeline combinations work correctly")
        
        # Test 5: Test statistics and monitoring
        print("\n5. Testing statistics and monitoring:")
        
        # Get vector store stats
        vs_stats = vector_store.get_stats()
        print(f"   ‚úì VectorStore stats - Total vectors: {vs_stats.total_vectors}")
        print(f"   ‚úì VectorStore stats - Vector dimension: {vs_stats.vector_dimension}")
        
        # Get processing stats
        processing_stats = vector_store.get_processing_stats()
        print(f"   ‚úì Processing stats - Vectors stored: {processing_stats.get('vectors_stored', 0)}")
        print(f"   ‚úì Processing stats - Searches performed: {processing_stats.get('searches_performed', 0)}")
        
        # Cleanup
        import os
        os.unlink(temp_db.name)
        
        return True
        
    except Exception as e:
        print(f"‚ùå Integration test failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Run architecture integration tests"""
    print("Architecture Integration Tests")
    print("=" * 60)
    
    success = test_architecture_integration()
    
    print("\n" + "=" * 60)
    
    if success:
        print("‚úÖ All architecture integration tests passed!")
        print("\nArchitecture Summary:")
        print("  üèóÔ∏è  Unified DocumentProcessor Architecture")
        print("     ‚Ä¢ VectorStore implements DocumentProcessor + Indexer + Retriever")
        print("     ‚Ä¢ KeywordSearch implements DocumentProcessor + Indexer + Retriever") 
        print("     ‚Ä¢ Direct integration with DocumentPipeline")
        print("")
        print("  üîÑ Pipeline Processing")
        print("     ‚Ä¢ Documents flow through processors sequentially")
        print("     ‚Ä¢ Each processor can transform, index, and pass through documents")
        print("     ‚Ä¢ Statistics tracked at each stage")
        print("")
        print("  üìä CorpusManager Integration")
        print("     ‚Ä¢ Uses VectorStore directly as DocumentProcessor")
        print("     ‚Ä¢ Supports stage-based pipeline construction") 
        print("     ‚Ä¢ Flexible embedder configuration")
        print("")
        print("  üéØ Benefits Achieved")
        print("     ‚Ä¢ Simplified class hierarchy")
        print("     ‚Ä¢ Eliminated redundant processor wrapper classes")
        print("     ‚Ä¢ Direct store usage in pipelines")
        print("     ‚Ä¢ Consistent interface across all components")
        
        return 0
    else:
        print("‚ùå Some architecture integration tests failed!")
        return 1


if __name__ == "__main__":
    exit(main())